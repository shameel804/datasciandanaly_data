{
    "id": "learn_probability",
    "topicId": "probability",
    "topicTitle": "Probability Theory",
    "description": "Master probability rules, Bayes theorem, random variables, probability distributions, and expected value",
    "baseKP": 90,
    "slides": [
        {
            "id": "probability_1",
            "type": "content",
            "title": "Welcome to Probability",
            "content": "# Probability Theory üé≤\n\nProbability is the mathematics of uncertainty‚Äîessential for data science and ML!\n\n## What You'll Learn\n- **Probability Rules** - Basic laws governing chance\n- **Bayes Theorem** - Updating beliefs with evidence\n- **Random Variables** - Modeling uncertain outcomes\n- **Probability Distributions** - Patterns of randomness\n- **Expected Value** - Average outcomes over time\n\n> üí° **Key Insight:** Machine learning is fundamentally about modeling probability distributions!\n\n## Why Probability Matters\n\n| Application | Probability Concept |\n|-------------|--------------------|\n| Spam detection | Conditional probability |\n| Medical diagnosis | Bayes theorem |\n| Risk assessment | Expected value |\n| A/B testing | Statistical inference |\n| Recommendation | Probabilistic models |\n\n## Probability Notation\n\n| Notation | Meaning |\n|----------|--------|\n| P(A) | Probability of event A |\n| P(A‚à£B) | Probability of A given B |\n| P(A ‚à© B) | Probability of A AND B |\n| P(A ‚à™ B) | Probability of A OR B |\n\nLet's master uncertainty! üéØ"
        },
        {
            "id": "probability_2",
            "type": "content",
            "title": "Basic Probability Rules",
            "content": "# Basic Probability Rules üìê\n\nThe fundamental laws of probability.\n\n## Probability Axioms\n\n### 1. Probability Range\n0 ‚â§ P(A) ‚â§ 1\n\n### 2. Certainty\nP(Sample Space) = 1\n\n### 3. Addition Rule (Mutually Exclusive)\nP(A ‚à™ B) = P(A) + P(B)\n\n## Key Rules\n\n### Complement Rule\nP(not A) = 1 - P(A)\n\n```python\nimport numpy as np\n\np_rain = 0.3\np_no_rain = 1 - p_rain\nprint(f\"P(no rain) = {p_no_rain}\")\n```\n\n### General Addition Rule\nP(A ‚à™ B) = P(A) + P(B) - P(A ‚à© B)\n\n### Multiplication Rule (Independent)\nP(A ‚à© B) = P(A) √ó P(B)\n\n```python\n# Probability of two independent coin flips = heads\np_heads = 0.5\np_two_heads = p_heads * p_heads\nprint(f\"P(two heads) = {p_two_heads}\")\n```\n\n<!-- FULL_CODE_START\nimport numpy as np\n\nprint(\"=== Basic Probability Rules ===\")\n\n# Complement Rule\nprint(\"\\n1. COMPLEMENT RULE\")\np_rain = 0.3\np_no_rain = 1 - p_rain\nprint(f\"   P(rain) = {p_rain}\")\nprint(f\"   P(no rain) = 1 - {p_rain} = {p_no_rain}\")\n\n# Addition Rule (Mutually Exclusive)\nprint(\"\\n2. ADDITION RULE (Mutually Exclusive)\")\np_a = 1/6  # Rolling a 1\np_b = 1/6  # Rolling a 6\np_a_or_b = p_a + p_b\nprint(f\"   P(roll 1) = {p_a:.4f}\")\nprint(f\"   P(roll 6) = {p_b:.4f}\")\nprint(f\"   P(roll 1 OR 6) = {p_a_or_b:.4f}\")\n\n# General Addition Rule\nprint(\"\\n3. GENERAL ADDITION RULE\")\np_red = 0.5  # Has red\np_circle = 0.4  # Is circle\np_red_and_circle = 0.2  # Red circle\np_red_or_circle = p_red + p_circle - p_red_and_circle\nprint(f\"   P(red) = {p_red}\")\nprint(f\"   P(circle) = {p_circle}\")\nprint(f\"   P(red AND circle) = {p_red_and_circle}\")\nprint(f\"   P(red OR circle) = {p_red} + {p_circle} - {p_red_and_circle} = {p_red_or_circle}\")\n\n# Multiplication Rule (Independent)\nprint(\"\\n4. MULTIPLICATION RULE (Independent)\")\np_heads = 0.5\np_two_heads = p_heads * p_heads\np_three_heads = p_heads ** 3\nprint(f\"   P(heads) = {p_heads}\")\nprint(f\"   P(two heads in a row) = {p_heads} √ó {p_heads} = {p_two_heads}\")\nprint(f\"   P(three heads in a row) = {p_heads}¬≥ = {p_three_heads}\")\nFULL_CODE_END -->\n\n> üéØ **Key Insight:** Always check if events are independent or mutually exclusive before applying rules!"
        },
        {
            "id": "probability_3",
            "type": "content",
            "title": "Conditional Probability",
            "content": "# Conditional Probability üîÑ\n\nThe probability of an event GIVEN that another event has occurred.\n\n## Definition\n\nP(A|B) = P(A ‚à© B) / P(B)\n\n\"The probability of A given B\"\n\n## Example: Medical Testing\n\n| | Disease | No Disease | Total |\n|---|---------|------------|-------|\n| **Positive Test** | 99 | 9 | 108 |\n| **Negative Test** | 1 | 891 | 892 |\n| **Total** | 100 | 900 | 1000 |\n\n### Q: If test is positive, what's the probability of disease?\n\nP(Disease | Positive) = 99 / 108 = 0.917\n\n```python\n# Conditional probability example\np_disease_and_positive = 99 / 1000\np_positive = 108 / 1000\n\np_disease_given_positive = p_disease_and_positive / p_positive\nprint(f\"P(Disease | Positive) = {p_disease_given_positive:.3f}\")\n```\n\n<!-- FULL_CODE_START\n# Conditional Probability - Medical Testing Example\n\nprint(\"=== Conditional Probability ===\")\nprint(\"\\nMedical Testing Scenario:\")\nprint(\"- 1000 patients tested\")\nprint(\"- 100 have the disease\")\nprint(\"- Test is 99% sensitive (catches 99% of disease)\")\nprint(\"- Test is 99% specific (correctly identifies 99% of healthy)\")\n\n# Given data\ntrue_positives = 99\nfalse_positives = 9\nfalse_negatives = 1\ntrue_negatives = 891\ntotal = 1000\n\nprint(\"\\n\" + \"=\"*40)\nprint(\"       | Disease | No Disease | Total\")\nprint(\"-\"*40)\nprint(f\"Pos    | {true_positives:7} | {false_positives:10} | {true_positives + false_positives}\")\nprint(f\"Neg    | {false_negatives:7} | {true_negatives:10} | {false_negatives + true_negatives}\")\nprint(f\"Total  | {true_positives + false_negatives:7} | {false_positives + true_negatives:10} | {total}\")\nprint(\"=\"*40)\n\n# Conditional probabilities\np_disease_given_positive = true_positives / (true_positives + false_positives)\np_healthy_given_negative = true_negatives / (false_negatives + true_negatives)\n\nprint(f\"\\nP(Disease | Positive Test) = {true_positives}/{true_positives + false_positives} = {p_disease_given_positive:.1%}\")\nprint(f\"P(Healthy | Negative Test) = {true_negatives}/{false_negatives + true_negatives} = {p_healthy_given_negative:.1%}\")\n\nprint(\"\\nüí° Insight: Even with 99% accuracy, 8.3% of positive tests are false alarms!\")\nFULL_CODE_END -->\n\n## Common Mistake ‚ö†Ô∏è\n\nP(A|B) ‚â† P(B|A)\n\nP(Disease|Positive) ‚â† P(Positive|Disease)\n\n> üéØ **Remember:** Always specify what you're conditioning on!"
        },
        {
            "id": "probability_quiz_1",
            "type": "quiz",
            "title": "Probability Rules Quiz",
            "content": "Test your understanding of probability rules!",
            "quizQuestion": "If P(A) = 0.6 and events A and B are independent with P(B) = 0.3, what is P(A AND B)?",
            "quizOptions": [
                "0.90",
                "0.18",
                "0.30",
                "0.60"
            ],
            "correctOptionIndex": 1
        },
        {
            "id": "probability_4",
            "type": "content",
            "title": "Bayes Theorem",
            "content": "# Bayes Theorem üß†\n\nThe foundation of modern probabilistic reasoning!\n\n## The Formula\n\nP(A|B) = [P(B|A) √ó P(A)] / P(B)\n\n## Components\n\n| Term | Name | Meaning |\n|------|------|--------|\n| P(A|B) | Posterior | Updated belief after evidence |\n| P(A) | Prior | Initial belief before evidence |\n| P(B|A) | Likelihood | Probability of evidence given hypothesis |\n| P(B) | Evidence | Total probability of evidence |\n\n## Intuition\n\n```\nPOMasterior = (Likelihood √ó Prior) / Evidence\n\n\"Update your beliefs based on new evidence\"\n```\n\n## Example: Email Spam Detection\n\n- P(Spam) = 0.3 (30% of emails are spam)\n- P(\"Free\"|Spam) = 0.8 (80% of spam has \"Free\")\n- P(\"Free\"|Not Spam) = 0.1 (10% of good emails have \"Free\")\n\n**Question:** Email contains \"Free\". P(Spam|\"Free\") = ?\n\n```python\np_spam = 0.3\np_free_given_spam = 0.8\np_free_given_not_spam = 0.1\n\n# P(Free) using total probability\np_free = (p_free_given_spam * p_spam) + (p_free_given_not_spam * (1 - p_spam))\n\n# Bayes theorem\np_spam_given_free = (p_free_given_spam * p_spam) / p_free\nprint(f\"P(Spam|'Free') = {p_spam_given_free:.2f}\")\n```\n\n<!-- FULL_CODE_START\n# Bayes Theorem - Spam Detection Example\n\nprint(\"=== Bayes Theorem: Spam Detection ===\")\n\n# Given probabilities\np_spam = 0.3\np_not_spam = 1 - p_spam\np_free_given_spam = 0.8\np_free_given_not_spam = 0.1\n\nprint(f\"\\nGiven:\")\nprint(f\"  P(Spam) = {p_spam}\")\nprint(f\"  P('Free'|Spam) = {p_free_given_spam}\")\nprint(f\"  P('Free'|Not Spam) = {p_free_given_not_spam}\")\n\n# Step 1: Calculate P(Free) using law of total probability\np_free = (p_free_given_spam * p_spam) + (p_free_given_not_spam * p_not_spam)\nprint(f\"\\nStep 1: P('Free') = {p_free_given_spam}√ó{p_spam} + {p_free_given_not_spam}√ó{p_not_spam}\")\nprint(f\"         P('Free') = {p_free}\")\n\n# Step 2: Apply Bayes theorem\np_spam_given_free = (p_free_given_spam * p_spam) / p_free\nprint(f\"\\nStep 2: Bayes Theorem\")\nprint(f\"  P(Spam|'Free') = [P('Free'|Spam) √ó P(Spam)] / P('Free')\")\nprint(f\"  P(Spam|'Free') = [{p_free_given_spam} √ó {p_spam}] / {p_free}\")\nprint(f\"  P(Spam|'Free') = {p_spam_given_free:.2%}\")\n\nprint(f\"\\nüí° Interpretation:\")\nprint(f\"  Before seeing 'Free': {p_spam:.0%} chance of spam\")\nprint(f\"  After seeing 'Free': {p_spam_given_free:.0%} chance of spam\")\nprint(f\"  The word 'Free' increased spam probability by {(p_spam_given_free/p_spam - 1):.0%}!\")\nFULL_CODE_END -->\n\n> üéØ **Key Insight:** Bayes lets us rationally update beliefs with new evidence!"
        },
        {
            "id": "probability_5",
            "type": "content",
            "title": "Random Variables",
            "content": "# Random Variables üé≤\n\nFormalizing uncertain outcomes mathematically.\n\n## Definition\n\nA **random variable** is a variable whose value is determined by a random phenomenon.\n\n## Types\n\n### Discrete Random Variables\nTake on countable values (integers)\n\n**Examples:**\n- Number of heads in 10 coin flips\n- Number of customers per hour\n- Dice roll outcome\n\n### Continuous Random Variables\nTake any value in a range (real numbers)\n\n**Examples:**\n- Height of a person\n- Time until next event\n- Temperature\n\n## Probability Functions\n\n| Type | Function | Meaning |\n|------|----------|--------|\n| Discrete | PMF (Probability Mass Function) | P(X = x) |\n| Continuous | PDF (Probability Density Function) | Density at x |\n| Both | CDF (Cumulative Distribution Function) | P(X ‚â§ x) |\n\n## Example: Dice Roll (PMF)\n\n```python\nimport numpy as np\n\n# PMF for a fair die\noutcomes = [1, 2, 3, 4, 5, 6]\nprobabilities = [1/6] * 6\n\nfor x, p in zip(outcomes, probabilities):\n    print(f\"P(X = {x}) = {p:.4f}\")\n```\n\n<!-- FULL_CODE_START\nimport numpy as np\n\nprint(\"=== Random Variables ===\")\n\n# Discrete: PMF for a fair die\nprint(\"\\n1. DISCRETE: Fair Die PMF\")\noutcomes = [1, 2, 3, 4, 5, 6]\nprobabilities = [1/6] * 6\n\nfor x, p in zip(outcomes, probabilities):\n    bar = \"‚ñà\" * int(p * 30)\n    print(f\"   P(X = {x}) = {p:.4f} {bar}\")\n\nprint(f\"   Sum of probabilities: {sum(probabilities)}\")\n\n# Simulate to verify\nnp.random.seed(42)\nrolls = np.random.randint(1, 7, 10000)\nprint(f\"\\n   Simulation (10000 rolls):\")\nfor x in outcomes:\n    freq = np.sum(rolls == x) / len(rolls)\n    print(f\"   Freq(X = {x}) = {freq:.4f}\")\n\n# Expected value\nexp_value = sum(x * p for x, p in zip(outcomes, probabilities))\nprint(f\"\\n   Expected Value: {exp_value}\")\nprint(f\"   Simulated Mean: {np.mean(rolls):.2f}\")\nFULL_CODE_END -->\n\n> üéØ **Key Insight:** Random variables let us use mathematics to reason about uncertain events!"
        },
        {
            "id": "probability_6",
            "type": "content",
            "title": "Probability Distributions",
            "content": "# Probability Distributions üìä\n\nPatterns that describe how probabilities are spread.\n\n## Discrete Distributions\n\n### Bernoulli Distribution\nSingle trial with two outcomes (success/failure)\n- **Parameter:** p (probability of success)\n- **Example:** Single coin flip\n\n### Binomial Distribution\nNumber of successes in n trials\n- **Parameters:** n (trials), p (probability)\n- **Example:** Heads in 10 coin flips\n\n### Poisson Distribution\nNumber of events in fixed time/space\n- **Parameter:** Œª (average rate)\n- **Example:** Emails per hour\n\n## Continuous Distributions\n\n### Normal (Gaussian)\nThe famous bell curve\n- **Parameters:** Œº (mean), œÉ (std dev)\n- **Example:** Heights, test scores\n\n### Uniform\nEqual probability across range\n- **Parameters:** a (min), b (max)\n- **Example:** Random number generator\n\n### Exponential\nTime between events\n- **Parameter:** Œª (rate)\n- **Example:** Time until next customer\n\n## Distribution Summary\n\n| Distribution | Type | When to Use |\n|--------------|------|-------------|\n| Bernoulli | Discrete | Yes/No outcome |\n| Binomial | Discrete | Count of successes |\n| Poisson | Discrete | Rare events count |\n| Normal | Continuous | Natural phenomena |\n| Exponential | Continuous | Wait times |\n\n> üí° **Tip:** Choosing the right distribution is crucial for accurate modeling!"
        },
        {
            "id": "probability_quiz_2",
            "type": "quiz",
            "title": "Distributions Quiz",
            "content": "Test your knowledge of probability distributions!",
            "quizQuestion": "Which distribution would you use to model the number of customers arriving at a store per hour?",
            "quizOptions": [
                "Normal Distribution",
                "Binomial Distribution",
                "Poisson Distribution",
                "Uniform Distribution"
            ],
            "correctOptionIndex": 2
        },
        {
            "id": "probability_7",
            "type": "content",
            "title": "Expected Value",
            "content": "# Expected Value üí∞\n\nThe average outcome over many repetitions.\n\n## Definition\n\n**Discrete:** E[X] = Œ£ x √ó P(X = x)\n\n**Continuous:** E[X] = ‚à´ x √ó f(x) dx\n\n## Example: Dice Roll\n\nE[X] = 1√ó(1/6) + 2√ó(1/6) + 3√ó(1/6) + 4√ó(1/6) + 5√ó(1/6) + 6√ó(1/6)\nE[X] = 21/6 = 3.5\n\n```python\nimport numpy as np\n\noutcomes = [1, 2, 3, 4, 5, 6]\nprob = 1/6\n\nexpected_value = sum(x * prob for x in outcomes)\nprint(f\"E[X] = {expected_value}\")\n```\n\n<!-- FULL_CODE_START\nimport numpy as np\n\nprint(\"=== Expected Value ===\")\n\n# Fair die\nprint(\"\\n1. FAIR DIE\")\noutcomes = [1, 2, 3, 4, 5, 6]\nprob = 1/6\n\nexpected_value = sum(x * prob for x in outcomes)\nprint(f\"   E[X] = Œ£ x √ó P(x)\")\nprint(f\"   E[X] = \" + \" + \".join([f\"{x}√ó(1/6)\" for x in outcomes]))\nprint(f\"   E[X] = {expected_value}\")\n\n# Verify with simulation\nnp.random.seed(42)\nrolls = np.random.randint(1, 7, 100000)\nprint(f\"\\n   Simulated mean (100k rolls): {np.mean(rolls):.4f}\")\n\n# Gambling example\nprint(\"\\n2. GAMBLING GAME\")\nprint(\"   Pay $5 to play. Win $20 with prob 0.2, else $0.\")\noutcomes = [-5, 15]  # -5 if lose, 15 if win (20 - 5 cost)\nprobs = [0.8, 0.2]\n\nev_game = sum(x * p for x, p in zip(outcomes, probs))\nprint(f\"   Expected outcome = {outcomes[0]}√ó{probs[0]} + {outcomes[1]}√ó{probs[1]}\")\nprint(f\"   Expected outcome = ${ev_game:.2f}\")\n\nif ev_game < 0:\n    print(f\"   ‚ö†Ô∏è Negative expected value: You'll lose money on average!\")\nelse:\n    print(f\"   ‚úÖ Positive expected value: Favorable game!\")\n\n# Insurance example\nprint(\"\\n3. INSURANCE COMPANY\")\nprint(\"   100,000 customers, premium $1000, payout $100,000\")\nprint(\"   P(claim) = 0.005\")\n\npremium = 1000\npayout = 100000\np_claim = 0.005\n\nev_per_customer = premium - (payout * p_claim)\nprint(f\"   E[profit per customer] = ${premium} - ${payout}√ó{p_claim}\")\nprint(f\"   E[profit per customer] = ${ev_per_customer}\")\nprint(f\"   E[total profit] = ${ev_per_customer * 100000:,.0f}\")\nFULL_CODE_END -->\n\n## Properties of Expected Value\n\n- E[a] = a (constant)\n- E[aX] = a √ó E[X]\n- E[X + Y] = E[X] + E[Y]\n\n> üéØ **Key Insight:** Expected value is the foundation of decision-making under uncertainty!"
        },
        {
            "id": "probability_8",
            "type": "content",
            "title": "Variance and Standard Deviation",
            "content": "# Variance of Random Variables üìê\n\nMeasuring spread in probability distributions.\n\n## Definition\n\n**Variance:** Var(X) = E[(X - Œº)¬≤] = E[X¬≤] - (E[X])¬≤\n\n**Standard Deviation:** œÉ = ‚àöVar(X)\n\n## Example: Dice Roll Variance\n\n```python\nimport numpy as np\n\noutcomes = np.array([1, 2, 3, 4, 5, 6])\nprob = 1/6\nmean = np.sum(outcomes * prob)\n\nvariance = np.sum((outcomes - mean)**2 * prob)\nstd_dev = np.sqrt(variance)\n\nprint(f\"Variance: {variance:.4f}\")\nprint(f\"Std Dev: {std_dev:.4f}\")\n```\n\n<!-- FULL_CODE_START\nimport numpy as np\n\nprint(\"=== Variance of Random Variables ===\")\n\n# Fair die\noutcomes = np.array([1, 2, 3, 4, 5, 6])\nprob = 1/6\n\n# Calculate mean (expected value)\nmean = np.sum(outcomes * prob)\nprint(f\"\\nFair Die:\")\nprint(f\"  Mean (E[X]): {mean}\")\n\n# Calculate E[X¬≤]\nexp_x_squared = np.sum(outcomes**2 * prob)\nprint(f\"  E[X¬≤]: {exp_x_squared:.4f}\")\n\n# Variance using formula: Var(X) = E[X¬≤] - (E[X])¬≤\nvariance = exp_x_squared - mean**2\nstd_dev = np.sqrt(variance)\n\nprint(f\"  Var(X) = E[X¬≤] - (E[X])¬≤ = {exp_x_squared:.4f} - {mean}¬≤ = {variance:.4f}\")\nprint(f\"  Std Dev: {std_dev:.4f}\")\n\n# Verify with simulation\nnp.random.seed(42)\nrolls = np.random.randint(1, 7, 100000)\nprint(f\"\\n  Simulated variance: {np.var(rolls):.4f}\")\nprint(f\"  Simulated std dev: {np.std(rolls):.4f}\")\n\n# Compare two games\nprint(\"\\n\" + \"=\"*40)\nprint(\"\\nComparing Two Games:\")\n\n# Game 1: Always win $10\ngame1_outcomes = [10]\ngame1_probs = [1.0]\ngame1_ev = 10\ngame1_var = 0\nprint(f\"\\nGame 1: Always win $10\")\nprint(f\"  E[X] = ${game1_ev}, Var = {game1_var}\")\n\n# Game 2: 50% win $20, 50% win $0\ngame2_outcomes = np.array([0, 20])\ngame2_probs = np.array([0.5, 0.5])\ngame2_ev = np.sum(game2_outcomes * game2_probs)\ngame2_var = np.sum((game2_outcomes - game2_ev)**2 * game2_probs)\nprint(f\"\\nGame 2: 50% chance $20, 50% chance $0\")\nprint(f\"  E[X] = ${game2_ev}, Var = {game2_var}\")\n\nprint(f\"\\nüí° Both games have same expected value, but Game 2 is riskier!\")\nFULL_CODE_END -->\n\n## Properties\n\n- Var(a) = 0 (constant has no variance)\n- Var(aX) = a¬≤ √ó Var(X)\n- Var(X + Y) = Var(X) + Var(Y) (if independent)\n\n> üí° **Insight:** Variance measures risk‚Äîsame expected value but different variances means different risk levels!"
        },
        {
            "id": "probability_9",
            "type": "content",
            "title": "Law of Large Numbers",
            "content": "# Law of Large Numbers üìà\n\nWhy probability works in practice.\n\n## The Concept\n\nAs sample size increases, sample mean approaches population mean.\n\n## Intuition\n\n- Flip a coin 10 times: Might get 70% heads\n- Flip a coin 1,000 times: Closer to 50%\n- Flip a coin 1,000,000 times: Very close to 50%\n\n## Mathematical Statement\n\nAs n ‚Üí ‚àû:\nSample Mean ‚Üí Population Mean (Œº)\n\n## Demonstration\n\n```python\nimport numpy as np\n\nnp.random.seed(42)\nfor n in [10, 100, 1000, 10000, 100000]:\n    flips = np.random.binomial(1, 0.5, n)\n    print(f\"n={n:6}: Mean = {np.mean(flips):.4f}\")\n```\n\n<!-- FULL_CODE_START\nimport numpy as np\n\nprint(\"=== Law of Large Numbers ===\")\nprint(\"\\nCoin Flip Experiment (True probability = 0.5)\")\nprint(\"-\" * 45)\n\nnp.random.seed(42)\nsample_sizes = [10, 50, 100, 500, 1000, 5000, 10000, 50000, 100000]\n\nprint(f\"{'Sample Size':>12} | {'Sample Mean':>12} | {'Error':>10}\")\nprint(\"-\" * 45)\n\nfor n in sample_sizes:\n    flips = np.random.binomial(1, 0.5, n)\n    sample_mean = np.mean(flips)\n    error = abs(sample_mean - 0.5)\n    print(f\"{n:>12,} | {sample_mean:>12.4f} | {error:>10.4f}\")\n\nprint(\"\\nüí° Notice how the sample mean converges to 0.5 as n increases!\")\nprint(\"   This is the Law of Large Numbers in action.\")\n\n# Running mean visualization\nprint(\"\\n\" + \"=\"*45)\nprint(\"\\nRunning Average Over 1000 Flips:\")\nflips = np.random.binomial(1, 0.5, 1000)\nrunning_mean = np.cumsum(flips) / np.arange(1, 1001)\n\ncheckpoints = [1, 10, 50, 100, 250, 500, 1000]\nfor i in checkpoints:\n    print(f\"  After {i:4} flips: mean = {running_mean[i-1]:.4f}\")\nFULL_CODE_END -->\n\n## Applications\n\n| Field | Application |\n|-------|-------------|\n| Insurance | Premium pricing |\n| Casinos | Guaranteed house edge |\n| Surveys | Sample size planning |\n| A/B Tests | Convergence to true effect |\n\n> üéØ **Key Insight:** This is why statistics works‚Äîwith enough data, randomness averages out!"
        },
        {
            "id": "probability_10",
            "type": "content",
            "title": "Central Limit Theorem",
            "content": "# Central Limit Theorem (CLT) üîî\n\nArguably the most important theorem in statistics!\n\n## The Theorem\n\nThe distribution of sample means approaches a normal distribution as sample size increases, regardless of the population distribution.\n\n## Key Points\n\n1. Works for ANY distribution (uniform, exponential, etc.)\n2. Sample size n ‚â• 30 is often \"large enough\"\n3. Mean of sample means = Population mean\n4. Standard error = œÉ / ‚àön\n\n## Why It Matters\n\n- Enables hypothesis testing\n- Allows confidence intervals\n- Justifies many statistical methods\n\n```python\nimport numpy as np\n\n# CLT demonstration with uniform distribution\nnp.random.seed(42)\nsample_means = []\n\nfor _ in range(1000):\n    sample = np.random.uniform(0, 10, size=30)\n    sample_means.append(np.mean(sample))\n\nprint(f\"Mean of sample means: {np.mean(sample_means):.2f}\")\nprint(f\"Std of sample means: {np.std(sample_means):.2f}\")\n```\n\n<!-- FULL_CODE_START\nimport numpy as np\n\nprint(\"=== Central Limit Theorem Demonstration ===\")\nprint(\"\\nUsing UNIFORM distribution (not normal!)\")\nprint(\"Population: Uniform(0, 10), True mean = 5\")\n\nnp.random.seed(42)\n\n# Population parameters\npop_mean = 5\npop_std = 10 / np.sqrt(12)  # Std of Uniform(0,10)\n\nprint(f\"Population std: {pop_std:.4f}\")\n\n# CLT for different sample sizes\nprint(\"\\n\" + \"=\"*60)\nprint(f\"{'Sample Size n':>15} | {'Mean of Means':>15} | {'Std of Means':>15} | {'œÉ/‚àön':>10}\")\nprint(\"=\"*60)\n\nfor n in [5, 10, 30, 50, 100]:\n    sample_means = []\n    for _ in range(10000):\n        sample = np.random.uniform(0, 10, size=n)\n        sample_means.append(np.mean(sample))\n    \n    mean_of_means = np.mean(sample_means)\n    std_of_means = np.std(sample_means)\n    theoretical_se = pop_std / np.sqrt(n)\n    \n    print(f\"{n:>15} | {mean_of_means:>15.4f} | {std_of_means:>15.4f} | {theoretical_se:>10.4f}\")\n\nprint(\"\\nüí° Key Observations:\")\nprint(\"  1. Mean of sample means ‚âà Population mean (5.0)\")\nprint(\"  2. Std of sample means ‚âà œÉ/‚àön (Standard Error)\")\nprint(\"  3. Distribution of means becomes normal (even from uniform!)\")\nFULL_CODE_END -->\n\n## Standard Error\n\nSE = œÉ / ‚àön\n\nAs n increases, SE decreases ‚Üí estimates become more precise!\n\n> üéØ **Key Insight:** CLT is why we can make confident statements about populations from samples!"
        },
        {
            "id": "probability_quiz_3",
            "type": "quiz",
            "title": "CLT Quiz",
            "content": "Test your understanding of the Central Limit Theorem!",
            "quizQuestion": "According to the Central Limit Theorem, what happens to the distribution of sample means as sample size increases?",
            "quizOptions": [
                "It becomes more uniform",
                "It approaches a normal distribution",
                "It becomes more skewed",
                "It stays the same as the population"
            ],
            "correctOptionIndex": 1
        },
        {
            "id": "probability_11",
            "type": "content",
            "title": "Summary",
            "content": "# Congratulations! üéâ\n\nYou've mastered Probability Theory!\n\n## Key Takeaways\n\n### Probability Rules\n- ‚úÖ Addition rule for OR events\n- ‚úÖ Multiplication rule for AND events\n- ‚úÖ Complement rule: P(not A) = 1 - P(A)\n\n### Bayes Theorem\n- ‚úÖ Update beliefs with evidence\n- ‚úÖ Posterior = (Likelihood √ó Prior) / Evidence\n- ‚úÖ Foundation of probabilistic reasoning\n\n### Random Variables\n- ‚úÖ Discrete: countable values (PMF)\n- ‚úÖ Continuous: any value in range (PDF)\n\n### Distributions\n- ‚úÖ Binomial: successes in n trials\n- ‚úÖ Poisson: events in fixed period\n- ‚úÖ Normal: the bell curve\n- ‚úÖ Exponential: wait times\n\n### Expected Value\n- ‚úÖ E[X] = average outcome over many trials\n- ‚úÖ Foundation for decision-making\n\n### Key Theorems\n- ‚úÖ **Law of Large Numbers:** Sample mean ‚Üí Population mean\n- ‚úÖ **Central Limit Theorem:** Sample means ‚Üí Normal distribution\n\n## Probability Connection to ML\n\n| ML Concept | Probability Foundation |\n|------------|----------------------|\n| Classification | Conditional probability |\n| Bayesian models | Bayes theorem |\n| Neural networks | Probabilistic outputs |\n| Uncertainty estimation | Distributions |\n\n> üöÄ **Next Steps:** Learn Inferential Statistics to make data-driven decisions!\n\nYou're a probability pro! üé≤"
        }
    ]
}
[
    {
        "q": "What type of target variable is required for Linear Regression?",
        "type": "mcq",
        "o": [
            "Continuous",
            "Categorical",
            "Binary",
            "Ordinal"
        ]
    },
    {
        "q": "In the linear equation below, what does 'beta-0' represent?",
        "c": "y = beta-0 + beta-1 * x + epsilon",
        "type": "mcq",
        "o": [
            "Y-intercept",
            "Slope",
            "Error term",
            "Dependent variable"
        ]
    },
    {
        "q": "The difference between the observed value (y) and the predicted value (y-hat) is known as the ______.",
        "type": "fill_blank",
        "answers": [
            "residual"
        ],
        "other_options": [
            "coefficient",
            "outlier",
            "variance"
        ]
    },
    {
        "q": "Logistic Regression is primarily used for classification tasks, not regression tasks.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the regression metric with its description:",
        "type": "match",
        "left": [
            "R-squared",
            "MSE",
            "Residual",
            "p-value"
        ],
        "right": [
            "Proportion of variance explained by the model",
            "Average of the squares of errors",
            "Difference between actual and predicted value",
            "Probability of observing results if H0 is true"
        ]
    },
    {
        "q": "Which function is used in Logistic Regression to map predicted values to probabilities between 0 and 1?",
        "type": "mcq",
        "o": [
            "Sigmoid",
            "ReLU",
            "Tangent",
            "Linear"
        ]
    },
    {
        "q": "Rearrange the steps of building a regression model:",
        "type": "rearrange",
        "words": [
            "Data Cleaning",
            "Split Train/Test",
            "Fit Model",
            "Evaluate Model",
            "Deploy"
        ]
    },
    {
        "q": "Homoscedasticity implies that the variance of the residuals is constant across all levels of the independent variable.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In a simple linear regression, if the coefficient 'beta-1' is zero, it implies there is ______ linear relationship between x and y.",
        "type": "fill_blank",
        "answers": [
            "no"
        ],
        "other_options": [
            "strong",
            "perfect",
            "negative"
        ]
    },
    {
        "q": "What is the output of this code snippet checking the shape of the coefficients?",
        "c": "from sklearn.linear_model import LinearRegression\nimport numpy as np\n\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n# X has 2 features\ny = np.dot(X, np.array([1, 2])) + 3\nreg = LinearRegression().fit(X, y)\nprint(reg.coef_.shape)",
        "type": "mcq",
        "o": [
            "(2,)",
            "(1,)",
            "(4,)",
            "(2, 2)"
        ]
    },
    {
        "q": "Match the Linear Regression assumption with its violation indicator:",
        "type": "match",
        "left": [
            "Linearity",
            "Homoscedasticity",
            "Independence",
            "Multicollinearity"
        ],
        "right": [
            "Curved pattern in residual plot",
            "Funnel shape in residual plot",
            "Pattern in residuals over time",
            "High Variance Inflation Factor"
        ]
    },
    {
        "q": "A p-value less than alpha (e.g., 0.05) indicates that the predictor variable is statistically significant.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which method is commonly used to estimate the coefficients in Linear Regression?",
        "type": "mcq",
        "o": [
            "Ordinary Least Squares",
            "Gradient Ascent",
            "K-Means Clustering",
            "Maximum Margin"
        ]
    },
    {
        "q": "Rearrange the logical flow of Ordinary Least Squares (OLS) estimation:",
        "type": "rearrange",
        "words": [
            "Calculate Residuals",
            "Square Residuals",
            "Sum Squared Errors",
            "Minimize Sum"
        ]
    },
    {
        "q": "Ideally, the residuals of a good regression model should follow a ______ distribution.",
        "type": "fill_blank",
        "answers": [
            "normal"
        ],
        "other_options": [
            "uniform",
            "binomial",
            "skewed"
        ]
    },
    {
        "q": "What is the likely issue if R-squared is high for training data but very low for testing data?",
        "type": "mcq",
        "o": [
            "Overfitting",
            "Underfitting",
            "Multicollinearity",
            "Heteroscedasticity"
        ]
    },
    {
        "q": "The sum of the residuals in an Ordinary Least Squares regression model is always equal to ______.",
        "type": "fill_blank",
        "answers": [
            "zero"
        ],
        "other_options": [
            "one",
            "infinity",
            "undefined"
        ]
    },
    {
        "q": "Adjusted R-squared will strictly increase when you add a new variable to the model, regardless of its relevance.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the Python libraries to their common regression functions:",
        "type": "match",
        "left": [
            "scikit-learn",
            "statsmodels",
            "seaborn",
            "numpy"
        ],
        "right": [
            "LinearRegression()",
            "OLS()",
            "regplot()",
            "polyfit()"
        ]
    },
    {
        "q": "Which method is typically used to estimate parameters in Logistic Regression?",
        "type": "mcq",
        "o": [
            "Maximum Likelihood Estimation",
            "Ordinary Least Squares",
            "Recursive Partitioning",
            "Principal Component Analysis"
        ]
    },
    {
        "q": "What is the output of this code snippet regarding the model score?",
        "c": "from sklearn.linear_model import LinearRegression\n# Assume X_test and y_test are valid datasets\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))",
        "type": "mcq",
        "o": [
            "R-squared value",
            "Mean Squared Error",
            "Coefficients list",
            "Intercept value"
        ]
    },
    {
        "q": "A Variance Inflation Factor (VIF) greater than 5 or 10 usually indicates the presence of ______.",
        "type": "fill_blank",
        "answers": [
            "multicollinearity"
        ],
        "other_options": [
            "heteroscedasticity",
            "nonlinearity",
            "autocorrelation"
        ]
    },
    {
        "q": "In Multiple Linear Regression, a coefficient represents the change in y for a one-unit change in x, holding all other variables constant.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the diagnostic plot to what it primarily checks:",
        "type": "match",
        "left": [
            "Q-Q Plot",
            "Scale-Location Plot",
            "Residuals vs Leverage",
            "Durbin-Watson Test"
        ],
        "right": [
            "Normality of residuals",
            "Homoscedasticity",
            "Influential points",
            "Autocorrelation of errors"
        ]
    },
    {
        "q": "The quantity 'log(p / (1 - p))' in Logistic Regression is known as the ______.",
        "type": "fill_blank",
        "answers": [
            "logit"
        ],
        "other_options": [
            "probit",
            "residue",
            "factor"
        ]
    },
    {
        "q": "Rearrange the sequence of calculating R-squared:",
        "type": "rearrange",
        "words": [
            "Find Mean of Y",
            "Calc Total Variance",
            "Calc Residual Variance",
            "Divide and Subtract"
        ]
    },
    {
        "q": "Which metric penalizes a linear model for adding unnecessary features (complexity)?",
        "type": "mcq",
        "o": [
            "Adjusted R-squared",
            "Standard R-squared",
            "Sum of Squared Errors",
            "Pearson Correlation"
        ]
    },
    {
        "q": "In a Residuals vs Fitted plot, a 'U' shape suggests that the linearity assumption is violated.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this code if the slope is 2 and intercept is 5?",
        "c": "def predict(x):\n    beta_1 = 2\n    beta_0 = 5\n    return beta_0 + beta_1 * x\n\nprint(predict(10))",
        "type": "mcq",
        "o": [
            "25",
            "20",
            "15",
            "52"
        ]
    },
    {
        "q": "Match the regression concept with its definition:",
        "type": "match",
        "left": [
            "Extrapolation",
            "Outlier",
            "Leverage",
            "Interaction Term"
        ],
        "right": [
            "Predicting outside data range",
            "Unusual y value",
            "Unusual x value",
            "Product of two features"
        ]
    },
    {
        "q": "Root Mean Squared Error (RMSE) measures the average magnitude of the ______.",
        "type": "fill_blank",
        "answers": [
            "error"
        ],
        "other_options": [
            "variance",
            "slope",
            "correlation"
        ]
    },
    {
        "q": "Rearrange the logical steps of a Logistic Regression prediction:",
        "type": "rearrange",
        "words": [
            "Linear Combination",
            "Apply Sigmoid",
            "Get Probability",
            "Check Threshold",
            "Assign Class"
        ]
    },
    {
        "q": "Cook's Distance is a metric used to identify observations that have a significant influence on the regression model.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which value implies perfect multicollinearity between two independent variables?",
        "type": "mcq",
        "o": [
            "Correlation = 1.0",
            "Correlation = 0.0",
            "p-value = 0.05",
            "Residual = 0"
        ]
    },
    {
        "q": "If the 95% confidence interval for a coefficient 'beta-1' includes zero, then the variable is likely ______.",
        "type": "fill_blank",
        "answers": [
            "insignificant"
        ],
        "other_options": [
            "significant",
            "optimal",
            "biased"
        ]
    },
    {
        "q": "A model with high bias and low variance is typically suffering from what?",
        "type": "mcq",
        "o": [
            "Underfitting",
            "Overfitting",
            "High Complexity",
            "Perfect Fit"
        ]
    },
    {
        "q": "In Logistic Regression, the exponent of a coefficient (exp(beta)) represents the change in the ______ for a one-unit increase in the predictor.",
        "type": "fill_blank",
        "answers": [
            "odds"
        ],
        "other_options": [
            "probability",
            "variance",
            "error"
        ]
    },
    {
        "q": "What happens to the intercept term if you set 'fit_intercept=False' in a Linear Regression model?",
        "c": "from sklearn.linear_model import LinearRegression\nmodel = LinearRegression(fit_intercept=False)\n# Model is fitted to data",
        "type": "mcq",
        "o": [
            "It is forced to be 0.0",
            "It is calculated normally",
            "It becomes 1.0",
            "It becomes infinite"
        ]
    },
    {
        "q": "Which statistical test is used to determine if a group of independent variables are collectively significant in explaining the dependent variable?",
        "type": "mcq",
        "o": [
            "F-test",
            "t-test",
            "Z-test",
            "Chi-square test"
        ]
    },
    {
        "q": "Logistic Regression assumes a linear relationship between the independent variables and the log-odds of the dependent variable.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the concept to the technique used to address it:",
        "type": "match",
        "left": [
            "Categorical Variables",
            "Different Scales",
            "Multicollinearity",
            "Non-linearity"
        ],
        "right": [
            "One-Hot Encoding",
            "Standardization",
            "Variance Inflation Factor",
            "Polynomial Features"
        ]
    },
    {
        "q": "In residual analysis, if the residuals are correlated with each other (e.g., in time series data), this violates the assumption of ______.",
        "type": "fill_blank",
        "answers": [
            "independence"
        ],
        "other_options": [
            "normality",
            "linearity",
            "homogeneity"
        ]
    },
    {
        "q": "Rearrange the steps for performing K-Fold Cross-Validation on a regression model:",
        "type": "rearrange",
        "words": [
            "Split Data K-Folds",
            "Train K-1 Folds",
            "Test 1 Fold",
            "Repeat K Times",
            "Average Scores"
        ]
    },
    {
        "q": "Which metric is best for comparing regression models with a different number of predictors, as it penalizes complexity?",
        "type": "mcq",
        "o": [
            "Akaike Information Criterion (AIC)",
            "Total Sum of Squares",
            "Raw R-squared",
            "Explained Variance"
        ]
    },
    {
        "q": "Standardizing features (scaling) changes the P-values and statistical significance of the predictors in Ordinary Least Squares.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What is the shape of the output array from this Logistic Regression probability prediction?",
        "c": "from sklearn.linear_model import LogisticRegression\nimport numpy as np\nX = np.random.rand(5, 2)\ny = [0, 1, 0, 1, 0]\nclf = LogisticRegression().fit(X, y)\n# Predicting probabilities for 3 samples\nprobs = clf.predict_proba(X[:3])\nprint(probs.shape)",
        "type": "mcq",
        "o": [
            "(3, 2)",
            "(3, 1)",
            "(3,)",
            "(2, 3)"
        ]
    },
    {
        "q": "Match the residual plot pattern to the regression problem it indicates:",
        "type": "match",
        "left": [
            "Fan or Cone shape",
            "Parabolic curve",
            "Random scatter",
            "Many points far from zero"
        ],
        "right": [
            "Heteroscedasticity",
            "Non-linearity",
            "Good Fit",
            "Outliers"
        ]
    },
    {
        "q": "In a Goodness of Fit test, a specific data point that has an extreme x-value and pulls the regression line towards itself is called a ______ point.",
        "type": "fill_blank",
        "answers": [
            "high leverage"
        ],
        "other_options": [
            "low variance",
            "nominal",
            "standard"
        ]
    },
    {
        "q": "If the correlation coefficient (r) between two variables is 0.8, what is the Coefficient of Determination (R-squared)?",
        "type": "mcq",
        "o": [
            "0.64",
            "0.80",
            "0.16",
            "0.08"
        ]
    },
    {
        "q": "Rearrange the components of the Total Sum of Squares (SST) formula concept:",
        "type": "rearrange",
        "words": [
            "Sum of",
            "Squared",
            "Differences",
            "From",
            "Mean y"
        ]
    },
    {
        "q": "Ridge Regression adds a penalty term equal to the square of the magnitude of coefficients to the loss function.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In Python statsmodels, which method provides a comprehensive table of regression statistics including R-squared, coefficients, and p-values?",
        "c": "import statsmodels.api as sm\nmodel = sm.OLS(y, X).fit()\n# What method is called here?\nprint(model.______())",
        "type": "mcq",
        "o": [
            "summary",
            "info",
            "describe",
            "report"
        ]
    },
    {
        "q": "A 'dummy variable' trap occurs when independent variables created from one-hot encoding are highly ______.",
        "type": "fill_blank",
        "answers": [
            "correlated"
        ],
        "other_options": [
            "random",
            "independent",
            "sparse"
        ]
    },
    {
        "q": "Which loss function is minimized during the training of a Logistic Regression model?",
        "type": "mcq",
        "o": [
            "Binary Cross-Entropy (Log Loss)",
            "Mean Squared Error",
            "Mean Absolute Error",
            "Hinge Loss"
        ]
    },
    {
        "q": "What is the primary purpose of adding a constant column (a column of 1s) to the feature matrix X in statsmodels?",
        "c": "import statsmodels.api as sm\nimport numpy as np\nX = np.random.rand(10, 2)\nX_new = sm.add_constant(X)",
        "type": "mcq",
        "o": [
            "To estimate the intercept (beta-0)",
            "To normalize the data",
            "To prevent overfitting",
            "To calculate the standard error"
        ]
    },
    {
        "q": "Lasso Regression (L1 regularization) has the unique ability to shrink coefficients exactly to ______, effectively performing feature selection.",
        "type": "fill_blank",
        "answers": [
            "zero"
        ],
        "other_options": [
            "one",
            "infinity",
            "mean"
        ]
    },
    {
        "q": "In Polynomial Regression, although the relationship between x and y is curved, the model is still considered 'linear' because it is linear in the parameters (coefficients).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the Regularization technique with its penalty term formula concept:",
        "type": "match",
        "left": [
            "Ridge Regression",
            "Lasso Regression",
            "Elastic Net",
            "OLS"
        ],
        "right": [
            "Sum of squared coefficients",
            "Sum of absolute coefficients",
            "Combination of L1 and L2",
            "No penalty term"
        ]
    },
    {
        "q": "The Durbin-Watson statistic is used to test for autocorrelation in residuals. A value closest to ______ suggests no autocorrelation.",
        "type": "fill_blank",
        "answers": [
            "2"
        ],
        "other_options": [
            "0",
            "4",
            "1"
        ]
    },
    {
        "q": "Rearrange the steps of the Gradient Descent algorithm for regression:",
        "type": "rearrange",
        "words": [
            "Initialize Parameters",
            "Calculate Gradient",
            "Update Parameters",
            "Check Convergence",
            "Stop"
        ]
    },
    {
        "q": "Which metric is less sensitive to outliers because it does not square the error differences?",
        "type": "mcq",
        "o": [
            "Mean Absolute Error (MAE)",
            "Mean Squared Error (MSE)",
            "Root Mean Squared Error",
            "R-squared"
        ]
    },
    {
        "q": "In Logistic Regression, the decision boundary that separates the two classes is linear in the feature space.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What error will this code likely raise given the dimensions?",
        "c": "from sklearn.linear_model import LinearRegression\nimport numpy as np\nX_train = np.random.rand(100, 5)\ny_train = np.random.rand(100)\nmodel = LinearRegression().fit(X_train, y_train)\n\n# Note the shape of the new data\nX_new = np.random.rand(10, 3)\npreds = model.predict(X_new)",
        "type": "mcq",
        "o": [
            "ValueError (Dimension mismatch)",
            "SyntaxError",
            "IndentationError",
            "TypeError"
        ]
    },
    {
        "q": "Match the goodness-of-fit concept with its context:",
        "type": "match",
        "left": [
            "Pseudo R-squared",
            "Standard Error of Estimate",
            "Log-Likelihood",
            "Confusion Matrix"
        ],
        "right": [
            "Used for Logistic Regression fit",
            "Standard deviation of residuals",
            "Maximized during fitting",
            "Table of True/False Positives/Negatives"
        ]
    },
    {
        "q": "When performing 'One-vs-Rest' (OvR) logistic regression for a problem with 3 classes (A, B, C), how many binary classifiers are trained?",
        "type": "fill_blank",
        "answers": [
            "3"
        ],
        "other_options": [
            "1",
            "2",
            "6"
        ]
    },
    {
        "q": "Rearrange the components of the standard confidence interval formula for a regression coefficient:",
        "type": "rearrange",
        "words": [
            "Coefficient Estimate",
            "Plus or Minus",
            "Critical Value (t)",
            "Times",
            "Standard Error"
        ]
    },
    {
        "q": "If the residuals of a regression model are not normally distributed, the confidence intervals and hypothesis tests (p-values) may be unreliable.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which Scikit-Learn parameter determines the regularization strength in LogisticRegression (inverse of lambda)?",
        "type": "mcq",
        "o": [
            "C",
            "alpha",
            "lambda",
            "gamma"
        ]
    },
    {
        "q": "What does this code calculate manually?",
        "c": "import numpy as np\n# y_true and y_pred are arrays\nerrors = y_true - y_pred\nsquared_errors = errors ** 2\nresult = np.sqrt(np.mean(squared_errors))",
        "type": "mcq",
        "o": [
            "RMSE (Root Mean Squared Error)",
            "MSE (Mean Squared Error)",
            "MAE (Mean Absolute Error)",
            "R-squared"
        ]
    },
    {
        "q": "To extend Linear Regression to capture non-linear patterns, we can add higher-order terms like x-squared. This is known as ______ regression.",
        "type": "fill_blank",
        "answers": [
            "polynomial"
        ],
        "other_options": [
            "logistic",
            "ridge",
            "quantile"
        ]
    },
    {
        "q": "In a Receiver Operating Characteristic (ROC) curve for a Logistic Regression model, what is plotted on the Y-axis?",
        "type": "mcq",
        "o": [
            "True Positive Rate (Sensitivity)",
            "False Positive Rate",
            "Precision",
            "Accuracy"
        ]
    },
    {
        "q": "What is the number of output features generated by this code?",
        "c": "from sklearn.preprocessing import PolynomialFeatures\nimport numpy as np\nX = np.array([[2, 3]]) # 1 sample, 2 features\n# degree=2, interaction_only=True includes x1, x2, x1*x2\npoly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\nout = poly.fit_transform(X)\nprint(out.shape[1])",
        "type": "mcq",
        "o": [
            "3",
            "2",
            "5",
            "4"
        ]
    },
    {
        "q": "A variable that influences both the dependent variable and independent variable, causing a spurious association, is called a ______ variable.",
        "type": "fill_blank",
        "answers": [
            "confounding"
        ],
        "other_options": [
            "target",
            "dummy",
            "binary"
        ]
    },
    {
        "q": "It is mathematically possible for the R-squared value to be negative if the model fits the data worse than a horizontal line (mean of y).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the Generalized Linear Model (GLM) component to its description:",
        "type": "match",
        "left": [
            "Link Function",
            "Random Component",
            "Systematic Component",
            "Identity Link"
        ],
        "right": [
            "Connects mean of response to predictors",
            "Probability distribution of response Y",
            "Linear combination of predictors (X*beta)",
            "Used in standard Linear Regression"
        ]
    },
    {
        "q": "Decreasing the decision threshold in Logistic Regression (e.g., from 0.5 to 0.3) generally increases Recall but decreases ______.",
        "type": "fill_blank",
        "answers": [
            "precision"
        ],
        "other_options": [
            "sensitivity",
            "bias",
            "variance"
        ]
    },
    {
        "q": "Rearrange the typical preprocessing pipeline for regression data:",
        "type": "rearrange",
        "words": [
            "Handle Missing Values",
            "Encode Categoricals",
            "Scale Features",
            "Train Model"
        ]
    },
    {
        "q": "Which optimization algorithm is preferred for Linear Regression on very large datasets (millions of rows) because it updates weights iteratively?",
        "type": "mcq",
        "o": [
            "Stochastic Gradient Descent (SGD)",
            "Normal Equation",
            "SVD Decomposition",
            "Recursive Feature Elimination"
        ]
    },
    {
        "q": "The assumption of 'Normality' in Linear Regression specifically applies to the distribution of the independent variables (predictors).",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What does the negative sign of the coefficient indicate in this output?",
        "c": "import numpy as np\n# Model: Price = 1000 - 50 * Age\ncoef = -50\nprint('Price decreases as Age increases')",
        "type": "mcq",
        "o": [
            "Inverse relationship",
            "Direct relationship",
            "No relationship",
            "Exponential relationship"
        ]
    },
    {
        "q": "Match the evaluation context to the correct metric:",
        "type": "match",
        "left": [
            "Linear Regression Fit",
            "Logistic Regression Fit",
            "Classification Accuracy",
            "Classification Separability"
        ],
        "right": [
            "R-squared",
            "Log Loss (Deviance)",
            "Confusion Matrix",
            "AUC Score"
        ]
    },
    {
        "q": "To model the effect where the impact of one variable depends on the value of another variable, you should include an ______ term.",
        "type": "fill_blank",
        "answers": [
            "interaction"
        ],
        "other_options": [
            "intercept",
            "error",
            "absolute"
        ]
    },
    {
        "q": "Which selection method starts with no variables and adds the most significant variable one by one?",
        "type": "mcq",
        "o": [
            "Forward Selection",
            "Backward Elimination",
            "Ridge Regression",
            "Recursive Elimination"
        ]
    },
    {
        "q": "Rearrange the steps to interpret a residual plot:",
        "type": "rearrange",
        "words": [
            "Plot Residuals vs Fitted",
            "Check for Patterns",
            "Identify Violations",
            "Refine Model"
        ]
    },
    {
        "q": "High multicollinearity between predictors significantly reduces the predictive power (accuracy) of the model on new data.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What is the output of this code snippet related to data splitting?",
        "c": "from sklearn.model_selection import train_test_split\nimport numpy as np\nX = np.arange(10).reshape(5, 2)\ny = range(5)\n# test_size=0.4 means 40% of 5 samples\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\nprint(len(X_test))",
        "type": "mcq",
        "o": [
            "2",
            "3",
            "1",
            "4"
        ]
    },
    {
        "q": "In Logistic Regression, the measure of goodness-of-fit that compares the likelihood of the fitted model to the likelihood of a saturated model is called ______.",
        "type": "fill_blank",
        "answers": [
            "deviance"
        ],
        "other_options": [
            "variance",
            "covariance",
            "residuals"
        ]
    },
    {
        "q": "Match the Sum of Squares component to its definition:",
        "type": "match",
        "left": [
            "SST (Total)",
            "SSR (Regression)",
            "SSE (Error)",
            "MST (Mean Total)"
        ],
        "right": [
            "Total variation in observed Y",
            "Variation explained by the model",
            "Unexplained variation",
            "SST divided by degrees of freedom"
        ]
    },
    {
        "q": "Which statistical test is specifically used to detect heteroscedasticity (non-constant variance) in a linear regression model?",
        "type": "mcq",
        "o": [
            "Breusch-Pagan test",
            "Shapiro-Wilk test",
            "T-test",
            "Pearson test"
        ]
    },
    {
        "q": "In a simple linear regression, the square of the Pearson correlation coefficient (r) is exactly equal to ______.",
        "type": "fill_blank",
        "answers": [
            "R-squared"
        ],
        "other_options": [
            "slope",
            "intercept",
            "error"
        ]
    },
    {
        "q": "A 'Null Model' (or intercept-only model) predicts the same value for every observation. What is that value?",
        "type": "mcq",
        "o": [
            "Mean of the target variable",
            "Median of the target variable",
            "Zero",
            "One"
        ]
    },
    {
        "q": "Rearrange the logic for calculating the F-statistic in regression:",
        "type": "rearrange",
        "words": [
            "Mean Square Regression",
            "Divided By",
            "Mean Square Error",
            "Yields F-value"
        ]
    },
    {
        "q": "Adding more independent variables to a model will always increase (or keep constant) the R-squared value, even if the variables are noise.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output shape of 'X_dummy' after dropping the first category to avoid multicollinearity?",
        "c": "import pandas as pd\ndf = pd.DataFrame({'Color': ['Red', 'Blue', 'Green']})\n# drop_first=True removes one column\nX_dummy = pd.get_dummies(df, drop_first=True)\nprint(X_dummy.shape)",
        "type": "mcq",
        "o": [
            "(3, 2)",
            "(3, 3)",
            "(3, 1)",
            "(2, 2)"
        ]
    },
    {
        "q": "Match the hypothesis test to the regression assumption it validates:",
        "type": "match",
        "left": [
            "Jarque-Bera",
            "Variance Inflation Factor",
            "Durbin-Watson",
            "Breusch-Pagan"
        ],
        "right": [
            "Normality of errors",
            "No Multicollinearity",
            "No Autocorrelation",
            "Homoscedasticity"
        ]
    },
    {
        "q": "For a categorical variable with 'k' distinct levels, you need ______ dummy variables to represent it in a regression model without collinearity.",
        "type": "fill_blank",
        "answers": [
            "k-1"
        ],
        "other_options": [
            "k",
            "k+1",
            "2k"
        ]
    },
    {
        "q": "In the context of the Bias-Variance Tradeoff, a very simple linear model often has ______ bias and low variance.",
        "type": "fill_blank",
        "answers": [
            "high"
        ],
        "other_options": [
            "low",
            "zero",
            "infinite"
        ]
    },
    {
        "q": "Rearrange the steps to interpret a Q-Q plot:",
        "type": "rearrange",
        "words": [
            "Sort Residuals",
            "Plot Against Theoretical Quantiles",
            "Check 45-degree Line",
            "Assess Normality"
        ]
    },
    {
        "q": "Which code snippet correctly instantiates a Ridge regression model with a regularization strength of 1.0?",
        "c": "from sklearn.linear_model import Ridge\n# Select the correct instantiation",
        "type": "mcq",
        "o": [
            "model = Ridge(alpha=1.0)",
            "model = Ridge(lambda=1.0)",
            "model = Ridge(C=1.0)",
            "model = Ridge(strength=1.0)"
        ]
    },
    {
        "q": "In Logistic Regression, the 'Likelihood Ratio Test' is used to compare a complex model against a simpler nested model.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the correlation value to its interpretation:",
        "type": "match",
        "left": [
            "+1.0",
            "-1.0",
            "0.0",
            "-0.8"
        ],
        "right": [
            "Perfect positive linear",
            "Perfect negative linear",
            "No linear relationship",
            "Strong negative linear"
        ]
    },
    {
        "q": "The standard error of a regression coefficient measures the variability of the estimated coefficient across different ______.",
        "type": "fill_blank",
        "answers": [
            "samples"
        ],
        "other_options": [
            "variables",
            "outliers",
            "intercepts"
        ]
    },
    {
        "q": "Which technique is used to check if the relationship between predictors and the log-odds is linear in Logistic Regression?",
        "type": "mcq",
        "o": [
            "Box-Tidwell Test",
            "T-test",
            "ANOVA",
            "Pearson Correlation"
        ]
    },
    {
        "q": "Rearrange the hierarchy of model evaluation metrics from simplest to most robust for classification:",
        "type": "rearrange",
        "words": [
            "Accuracy",
            "Precision/Recall",
            "F1-Score",
            "ROC-AUC"
        ]
    },
    {
        "q": "If a data point has a Studentized Residual greater than 3, it is typically considered an ______.",
        "type": "fill_blank",
        "answers": [
            "outlier"
        ],
        "other_options": [
            "inlier",
            "feature",
            "average"
        ]
    },
    {
        "q": "Ordinary Least Squares (OLS) is sensitive to outliers because squaring the errors magnifies large discrepancies.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In Simple Linear Regression, the slope coefficient is directly proportional to the Pearson correlation coefficient (r) multiplied by the ratio of the standard deviations of ______.",
        "type": "fill_blank",
        "answers": [
            "y and x"
        ],
        "other_options": [
            "x and y",
            "residuals",
            "errors"
        ]
    },
    {
        "q": "Which attribute of the Scikit-Learn LinearRegression object holds the estimated intercept term?",
        "c": "from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X, y)\n# How to access the intercept?",
        "type": "mcq",
        "o": [
            "model.intercept_",
            "model.bias_",
            "model.coef0_",
            "model.beta_0"
        ]
    },
    {
        "q": "The Akaike Information Criterion (AIC) estimates the relative quality of statistical models. A ______ AIC value indicates a better model.",
        "type": "fill_blank",
        "answers": [
            "lower"
        ],
        "other_options": [
            "higher",
            "zero",
            "positive"
        ]
    },
    {
        "q": "Multinomial Logistic Regression is an extension of binary logistic regression that allows for more than two categories in the dependent variable.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the type of Residual to its calculation method:",
        "type": "match",
        "left": [
            "Raw Residual",
            "Standardized Residual",
            "Studentized Residual",
            "Pearson Residual"
        ],
        "right": [
            "y_observed - y_predicted",
            "Raw residual divided by its standard deviation",
            "Residual divided by estimate of standard deviation",
            "Raw residual scaled by square root of variance"
        ]
    },
    {
        "q": "In a Linear Regression model, if the coefficient for a predictor 'X' is exactly zero, what is the Odds Ratio associated with 'X' in a Logistic Regression context?",
        "type": "mcq",
        "o": [
            "1.0",
            "0.0",
            "Infinity",
            "0.5"
        ]
    },
    {
        "q": "Rearrange the components of the Adjusted R-squared formula logic:",
        "type": "rearrange",
        "words": [
            "1 Minus",
            "Ratio of",
            "Residual Variance",
            "To",
            "Total Variance"
        ]
    },
    {
        "q": "Which assumption is violated if the error terms in a regression model follow a pattern over time (e.g., in stock market data)?",
        "type": "mcq",
        "o": [
            "No Autocorrelation",
            "Linearity",
            "Normality",
            "Multicollinearity"
        ]
    },
    {
        "q": "In Scikit-Learn, the 'predict()' method of a Logistic Regression model outputs class labels, whereas 'predict_proba()' outputs ______.",
        "type": "fill_blank",
        "answers": [
            "probabilities"
        ],
        "other_options": [
            "log-odds",
            "coefficients",
            "residuals"
        ]
    },
    {
        "q": "When calculating the F-statistic for the overall significance of the model, we compare the Mean Square Regression (MSR) to the ______.",
        "type": "mcq",
        "o": [
            "Mean Square Error (MSE)",
            "Total Sum of Squares",
            "R-squared",
            "Intercept"
        ]
    },
    {
        "q": "Match the Regression term to its role in the equation y = beta0 + beta1*x:",
        "type": "match",
        "left": [
            "y",
            "x",
            "beta0",
            "beta1"
        ],
        "right": [
            "Response Variable",
            "Predictor Variable",
            "Constant term",
            "Gradient term"
        ]
    },
    {
        "q": "Feature scaling (normalization/standardization) is strictly required for Ordinary Least Squares (OLS) to find the correct minimum.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange the steps to calculate the Mean Squared Error (MSE):",
        "type": "rearrange",
        "words": [
            "Subtract Pred from Actual",
            "Square the Difference",
            "Sum all Squares",
            "Divide by N"
        ]
    },
    {
        "q": "What is the primary function of the 'logit' link function in Generalized Linear Models?",
        "type": "mcq",
        "o": [
            "Maps (0,1) probabilities to (-inf, +inf)",
            "Maps (-inf, +inf) to (0,1)",
            "Squares the error term",
            "Calculates the residuals"
        ]
    },
    {
        "q": "In a residual plot, if the variance of the residuals increases as the fitted values increase, the data is said to exhibit ______.",
        "type": "fill_blank",
        "answers": [
            "heteroscedasticity"
        ],
        "other_options": [
            "homoscedasticity",
            "collinearity",
            "normality"
        ]
    },
    {
        "q": "Which code snippet correctly removes a column named 'ID' that is not useful for regression?",
        "c": "import pandas as pd\ndf = pd.read_csv('data.csv')\n# Which command drops the 'ID' column in place?",
        "type": "mcq",
        "o": [
            "df.drop('ID', axis=1, inplace=True)",
            "df.remove('ID')",
            "df.delete('ID')",
            "df.pop_column('ID')"
        ]
    },
    {
        "q": "The 'dummy variable trap' can be solved by dropping one of the categorical levels (e.g., if you have 3 days, use only 2 variables).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the primary consequence of omitting a relevant independent variable from a regression model (Omitted Variable Bias)?",
        "type": "mcq",
        "o": [
            "The estimated coefficients become biased",
            "The standard errors decrease",
            "The R-squared increases",
            "The residuals become normally distributed"
        ]
    },
    {
        "q": "What is the flaw in this preprocessing pipeline code?",
        "c": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\n# Look closely at the next line\nX_test_scaled = scaler.fit_transform(X_test)",
        "type": "mcq",
        "o": [
            "Data leakage: re-fitting on test data",
            "Syntax error in method name",
            "Cannot scale test data",
            "StandardScaler does not support fit_transform"
        ]
    },
    {
        "q": "In a 'Log-Log' regression model (log(y) vs log(x)), the coefficient beta-1 represents the ______ of y with respect to x.",
        "type": "fill_blank",
        "answers": [
            "elasticity"
        ],
        "other_options": [
            "slope",
            "intercept",
            "variance"
        ]
    },
    {
        "q": "Extrapolation involves making predictions for predictor values that are far outside the range of the data used to train the model.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the confusion matrix term to its definition in the context of Logistic Regression classification:",
        "type": "match",
        "left": [
            "True Positive",
            "False Positive",
            "True Negative",
            "False Negative"
        ],
        "right": [
            "Correctly predicted event",
            "Incorrectly predicted event (Type I Error)",
            "Correctly predicted non-event",
            "Incorrectly predicted non-event (Type II Error)"
        ]
    },
    {
        "q": "Which Scikit-Learn regressor allows for 'online learning' (updating the model with small batches of data)?",
        "type": "mcq",
        "o": [
            "SGDRegressor",
            "LinearRegression",
            "Ridge",
            "Lasso"
        ]
    },
    {
        "q": "Rearrange the steps to perform Stepwise Backward Elimination:",
        "type": "rearrange",
        "words": [
            "Start with all variables",
            "Fit Model",
            "Identify highest P-value",
            "Remove Variable",
            "Refit Model"
        ]
    },
    {
        "q": "If a regression model suffers from Heteroscedasticity, the Ordinary Least Squares (OLS) estimates are still unbiased, but the standard errors are ______.",
        "type": "fill_blank",
        "answers": [
            "wrong"
        ],
        "other_options": [
            "zero",
            "perfect",
            "minimized"
        ]
    },
    {
        "q": "In Generalized Linear Models (GLM), if the target variable represents count data (non-negative integers), which regression type is most appropriate?",
        "type": "mcq",
        "o": [
            "Poisson Regression",
            "Linear Regression",
            "Logistic Regression",
            "Probit Regression"
        ]
    },
    {
        "q": "The Principle of Parsimony states that among models with similar predictive power, the one with the fewest parameters is preferred.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the regression target variable range to the model type:",
        "type": "match",
        "left": [
            "(-inf, +inf)",
            "[0, 1]",
            "[0, +inf) Counts",
            "Categories {0, 1, 2}"
        ],
        "right": [
            "Linear Regression",
            "Logistic Regression",
            "Poisson Regression",
            "Multinomial Regression"
        ]
    },
    {
        "q": "What does the 'score()' method return for a LogisticRegression object in Scikit-Learn?",
        "c": "from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression().fit(X, y)\n# What metric is this?",
        "type": "mcq",
        "o": [
            "Mean Accuracy",
            "F1 Score",
            "Log Loss",
            "AUC-ROC"
        ]
    },
    {
        "q": "When calculating the t-statistic for a regression coefficient, the numerator is the coefficient estimate and the denominator is the ______.",
        "type": "fill_blank",
        "answers": [
            "standard error"
        ],
        "other_options": [
            "variance",
            "p-value",
            "mean"
        ]
    },
    {
        "q": "Rearrange the components to form the Linear Regression matrix equation for coefficients:",
        "type": "rearrange",
        "words": [
            "Inverse of",
            "X-transpose times X",
            "Times",
            "X-transpose times y"
        ]
    },
    {
        "q": "Scaling or Normalization is absolutely crucial when using Ridge or Lasso regression because the penalty term is sensitive to the magnitude of the coefficients.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In the context of Time Series Regression, using the previous value of the target variable as a predictor is known as a ______ variable.",
        "type": "fill_blank",
        "answers": [
            "lag"
        ],
        "other_options": [
            "lead",
            "dummy",
            "random"
        ]
    },
    {
        "q": "What statistical phenomenon is illustrated when adding a new variable reverses the direction of an association between two other variables?",
        "type": "mcq",
        "o": [
            "Simpson's Paradox",
            "Gambler's Fallacy",
            "Central Limit Theorem",
            "Law of Large Numbers"
        ]
    },
    {
        "q": "Which optimization algorithm is commonly used specifically for Lasso Regression because it handles the non-differentiable L1 penalty effectively?",
        "type": "mcq",
        "o": [
            "Coordinate Descent",
            "Normal Equation",
            "Newton-Raphson",
            "Gradient Ascent"
        ]
    },
    {
        "q": "What will be the output of this code snippet?",
        "c": "from sklearn.linear_model import LogisticRegression\nimport numpy as np\nmodel = LogisticRegression()\n# Model is NOT fitted yet\ntry:\n    print(model.coef_)\nexcept Exception as e:\n    print('Error')\nelse:\n    print('Success')",
        "type": "mcq",
        "o": [
            "Error",
            "Success",
            "None",
            "[]"
        ]
    },
    {
        "q": "In Simple Linear Regression with N observations and 2 parameters (intercept and slope), the Error Degrees of Freedom is equal to ______.",
        "type": "fill_blank",
        "answers": [
            "N-2"
        ],
        "other_options": [
            "N-1",
            "N",
            "N+1"
        ]
    },
    {
        "q": "Scaling the input features (normalization) affects the predicted values (y-hat) in Ordinary Least Squares regression.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the observation type to its definition:",
        "type": "match",
        "left": [
            "High Leverage Point",
            "Influential Point",
            "Outlier",
            "Centroid"
        ],
        "right": [
            "Extreme predictor (x) value",
            "Significantly changes slope if removed",
            "Large residual (y) value",
            "Mean of x and Mean of y"
        ]
    },
    {
        "q": "Which Scikit-Learn method allows you to predict the log of the probability estimates, which is often numerically more stable?",
        "type": "mcq",
        "o": [
            "predict_log_proba()",
            "predict_odds()",
            "predict_logit()",
            "score_log()"
        ]
    },
    {
        "q": "Rearrange the steps of Forward Selection for feature selection:",
        "type": "rearrange",
        "words": [
            "Start with null model",
            "Test all additions",
            "Select best p-value",
            "Add variable",
            "Repeat"
        ]
    },
    {
        "q": "Applying a Log transformation to the dependent variable (y) is a common technique to fix ______.",
        "type": "fill_blank",
        "answers": [
            "heteroscedasticity"
        ],
        "other_options": [
            "multicollinearity",
            "orthogonality",
            "underfitting"
        ]
    },
    {
        "q": "The assumption that residuals must be normally distributed is required for Logistic Regression hypothesis testing.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What is the primary purpose of setting the 'random_state' parameter in train_test_split?",
        "c": "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)",
        "type": "mcq",
        "o": [
            "To ensure reproducibility of the split",
            "To improve model accuracy",
            "To randomize the feature weights",
            "To speed up the computation"
        ]
    },
    {
        "q": "Match the Link Function to the Generalized Linear Model type:",
        "type": "match",
        "left": [
            "Identity",
            "Logit",
            "Log",
            "Inverse"
        ],
        "right": [
            "Linear Regression (Gaussian)",
            "Logistic Regression (Binomial)",
            "Poisson Regression (Count)",
            "Gamma Regression"
        ]
    },
    {
        "q": "If the slope coefficient (beta-1) in a simple linear regression is zero, the regression line is ______.",
        "type": "fill_blank",
        "answers": [
            "horizontal"
        ],
        "other_options": [
            "vertical",
            "diagonal",
            "undefined"
        ]
    },
    {
        "q": "Which plot is most effective for detecting a non-linear relationship between the predictors and the response?",
        "type": "mcq",
        "o": [
            "Residuals vs Fitted Values",
            "Histogram of Residuals",
            "Boxplot of Predictors",
            "Correlation Matrix"
        ]
    },
    {
        "q": "Rearrange the calculation steps for the T-statistic of a coefficient:",
        "type": "rearrange",
        "words": [
            "Estimated Coefficient",
            "Minus",
            "Hypothesized Value",
            "Divided By",
            "Standard Error"
        ]
    },
    {
        "q": "In a saturated model (perfect fit), the Residual Deviance is equal to ______.",
        "type": "fill_blank",
        "answers": [
            "zero"
        ],
        "other_options": [
            "one",
            "infinity",
            "negative"
        ]
    },
    {
        "q": "In Scikit-Learn, which parameter in LogisticRegression helps handle imbalanced datasets by automatically adjusting weights inversely proportional to class frequencies?",
        "type": "mcq",
        "o": [
            "class_weight='balanced'",
            "fit_intercept=True",
            "solver='liblinear'",
            "penalty='l2'"
        ]
    },
    {
        "q": "The 'Homoscedasticity' assumption implies that the error terms have the same variance for all observations.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the specific term for the property of the Ordinary Least Squares (OLS) estimator being the 'Best Linear Unbiased Estimator'?",
        "type": "mcq",
        "o": [
            "Gauss-Markov Theorem",
            "Central Limit Theorem",
            "Law of Large Numbers",
            "Bayes Theorem"
        ]
    },
    {
        "q": "In the context of regression with a binary independent variable (0 or 1), the coefficient represents the difference in the ______ between the two groups.",
        "type": "fill_blank",
        "answers": [
            "mean"
        ],
        "other_options": [
            "variance",
            "median",
            "count"
        ]
    },
    {
        "q": "What is the critical issue in this code snippet regarding data scaling?",
        "c": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n# Fit on training data\nX_train_s = scaler.fit_transform(X_train)\n# Fit on testing data\nX_test_s = scaler.fit_transform(X_test)",
        "type": "mcq",
        "o": [
            "Data leakage (re-fitting on test set)",
            "Syntax error",
            "Incorrect import",
            "StandardScaler cannot be used twice"
        ]
    },
    {
        "q": "For a Linear Regression model to be valid, the expected value of the error term given the independent variables must be zero (Zero Conditional Mean).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the Sum of Squares to its associated Degrees of Freedom (where n=samples, p=predictors):",
        "type": "match",
        "left": [
            "SST (Total)",
            "SSR (Regression)",
            "SSE (Error)",
            "Intercept only"
        ],
        "right": [
            "n - 1",
            "p",
            "n - p - 1",
            "1"
        ]
    },
    {
        "q": "Which plot is primarily used to check if the residuals follow a normal distribution?",
        "type": "mcq",
        "o": [
            "Q-Q Plot (Quantile-Quantile)",
            "Scatter Plot",
            "Box Plot",
            "Bar Chart"
        ]
    },
    {
        "q": "Rearrange the logical steps for performing a Likelihood Ratio Test between two models:",
        "type": "rearrange",
        "words": [
            "Fit Full Model",
            "Fit Reduced Model",
            "Calc Log-Likelihoods",
            "Compute Difference",
            "Compare to Chi-Square"
        ]
    },
    {
        "q": "In a 'Semi-Log' model where log(y) is regressed on x, the coefficient beta represents the ______ change in y for a unit change in x.",
        "type": "fill_blank",
        "answers": [
            "percentage"
        ],
        "other_options": [
            "absolute",
            "squared",
            "linear"
        ]
    },
    {
        "q": "When using Scikit-Learn's 'SGDRegressor', you must scale your data (e.g., using StandardScaler) because Gradient Descent is sensitive to feature magnitudes.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does the method 'predict()' return when applied to a Logistic Regression model in Scikit-Learn?",
        "c": "model = LogisticRegression().fit(X, y)\noutput = model.predict(X_new)",
        "type": "mcq",
        "o": [
            "Predicted class labels (0 or 1)",
            "Predicted probabilities",
            "Log-odds",
            "Decision function values"
        ]
    },
    {
        "q": "Match the Regression Evaluation Metric to its mathematical characteristic:",
        "type": "match",
        "left": [
            "MAE",
            "MSE",
            "RMSE",
            "R-squared"
        ],
        "right": [
            "Uses absolute differences",
            "Uses squared differences",
            "Same units as target variable",
            "Unitless (percentage)"
        ]
    },
    {
        "q": "The F-statistic in a regression summary is used to test the hypothesis that ______ regression coefficients are equal to zero.",
        "type": "fill_blank",
        "answers": [
            "all"
        ],
        "other_options": [
            "some",
            "intercept",
            "one"
        ]
    },
    {
        "q": "Rearrange the components of the Logistic function formula (sigmoid):",
        "type": "rearrange",
        "words": [
            "One",
            "Divided By",
            "One Plus",
            "e to power of",
            "Negative z"
        ]
    },
    {
        "q": "If the Confidence Interval for a regression coefficient ranges from -2.5 to +1.5, what can you infer about the variable's significance at that alpha level?",
        "type": "mcq",
        "o": [
            "It is not statistically significant",
            "It is significant",
            "It has a positive relationship",
            "It has a negative relationship"
        ]
    },
    {
        "q": "In Python's statsmodels, the 'endog' parameter refers to the independent variables (X), and 'exog' refers to the dependent variable (y).",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which value of R-squared indicates that the regression model explains none of the variability of the response data around its mean?",
        "type": "mcq",
        "o": [
            "0.0",
            "1.0",
            "0.5",
            "-1.0"
        ]
    },
    {
        "q": "The distance between a data point and the regression line, measured vertically (y-direction), is called the ______.",
        "type": "fill_blank",
        "answers": [
            "residual"
        ],
        "other_options": [
            "leverage",
            "distance",
            "slope"
        ]
    },
    {
        "q": "Which interval provides a range for an individual new observation rather than the mean response, making it typically wider?",
        "type": "mcq",
        "o": [
            "Prediction Interval",
            "Confidence Interval",
            "Credible Interval",
            "Tolerance Interval"
        ]
    },
    {
        "q": "What is the expected length of the 'coef_' attribute in this Scikit-Learn Linear Regression model?",
        "c": "from sklearn.linear_model import LinearRegression\nimport numpy as np\n# X has 100 samples and 4 features\nX = np.random.rand(100, 4)\ny = np.random.rand(100)\nmodel = LinearRegression().fit(X, y)\nprint(len(model.coef_))",
        "type": "mcq",
        "o": [
            "4",
            "100",
            "1",
            "5"
        ]
    },
    {
        "q": "In Logistic Regression, the curve that maps the input to the probability output (ranging from 0 to 1) has a characteristic ______ shape.",
        "type": "fill_blank",
        "answers": [
            "sigmoid"
        ],
        "other_options": [
            "linear",
            "bell",
            "parabolic"
        ]
    },
    {
        "q": "One of the OLS assumptions is that the independent variables (X) are fixed and not random (deterministic) in repeated sampling.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the diagnostic metric to what it specifically measures regarding specific data points:",
        "type": "match",
        "left": [
            "Cook's Distance",
            "Studentized Residuals",
            "Hat Matrix Values",
            "DFFITS"
        ],
        "right": [
            "Overall influence of a point",
            "Outlyingness in Y direction",
            "Leverage (Outlyingness in X)",
            "Change in predicted value if point removed"
        ]
    },
    {
        "q": "In a Log-Linear model where ln(y) = beta0 + beta1*x, a one-unit increase in x is associated with a ______ change in y.",
        "type": "mcq",
        "o": [
            "multiplicative (percentage)",
            "additive (constant)",
            "subtractive",
            "exponentially decreasing"
        ]
    },
    {
        "q": "Rearrange the standard workflow for model selection using a validation set:",
        "type": "rearrange",
        "words": [
            "Split Train/Validation/Test",
            "Train Models",
            "Select Best via Validation",
            "Evaluate on Test"
        ]
    },
    {
        "q": "The number of independent values or quantities which can be assigned to a statistical distribution is known as the Degrees of ______.",
        "type": "fill_blank",
        "answers": [
            "Freedom"
        ],
        "other_options": [
            "Variance",
            "Independence",
            "Rotation"
        ]
    },
    {
        "q": "Polynomial Regression with a very high degree (e.g., degree=15) is less likely to overfit the data than a simple linear regression.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which Scikit-Learn module typically contains the 'mean_squared_error' function?",
        "c": "from sklearn.______ import mean_squared_error",
        "type": "mcq",
        "o": [
            "metrics",
            "linear_model",
            "preprocessing",
            "utils"
        ]
    },
    {
        "q": "Match the variable transformation to its primary use case:",
        "type": "match",
        "left": [
            "Log Transformation",
            "Square Root Transformation",
            "Inverse Transformation",
            "Polynomial Feature"
        ],
        "right": [
            "Right-skewed positive data",
            "Count data (Poisson)",
            "Ratio data",
            "Curved relationships"
        ]
    },
    {
        "q": "Elastic Net regularization combines the penalties of both ______ and Lasso regression.",
        "type": "fill_blank",
        "answers": [
            "Ridge"
        ],
        "other_options": [
            "Linear",
            "Logistic",
            "Stepwise"
        ]
    },
    {
        "q": "Rearrange the components of the Multiple Linear Regression equation:",
        "type": "rearrange",
        "words": [
            "Intercept",
            "Plus",
            "Slope1 times X1",
            "Plus",
            "Slope2 times X2"
        ]
    },
    {
        "q": "A 'saturated model' is a model that has as many parameters as there are data points, resulting in a perfect fit but zero predictive power.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which argument in the 'fit()' method allows you to weight individual samples differently during training (e.g., to handle outliers or importance)?",
        "c": "model.fit(X, y, ______=weights)",
        "type": "mcq",
        "o": [
            "sample_weight",
            "class_weight",
            "importance",
            "bias"
        ]
    },
    {
        "q": "When two independent variables are perfectly correlated (correlation = 1), the design matrix X becomes singular and cannot be ______.",
        "type": "fill_blank",
        "answers": [
            "inverted"
        ],
        "other_options": [
            "multiplied",
            "scaled",
            "transposed"
        ]
    },
    {
        "q": "What does the 'C' parameter in Scikit-Learn's LogisticRegression control?",
        "type": "mcq",
        "o": [
            "Inverse of regularization strength",
            "Direct regularization strength",
            "Number of iterations",
            "Threshold value"
        ]
    },
    {
        "q": "In Gradient Descent optimization, if the 'Learning Rate' (alpha) is set too high, what is the most likely outcome?",
        "type": "mcq",
        "o": [
            "The algorithm may overshoot the minimum and fail to converge",
            "The algorithm will converge very slowly",
            "The model will overfit the data",
            "The intercept will become zero"
        ]
    },
    {
        "q": "Which Scikit-Learn solver is generally recommended for small datasets in Logistic Regression because it uses a coordinate descent algorithm?",
        "type": "mcq",
        "o": [
            "liblinear",
            "sag",
            "saga",
            "lbfgs"
        ]
    },
    {
        "q": "While probabilities range from 0 to 1, the 'Log-Odds' (logits) in Logistic Regression range from negative infinity to ______.",
        "type": "fill_blank",
        "answers": [
            "positive infinity"
        ],
        "other_options": [
            "one",
            "zero",
            "hundred"
        ]
    },
    {
        "q": "R-squared does not indicate the accuracy of the model; it measures the proportion of variance in the dependent variable explained by the independent variables.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the Scikit-Learn scaler to its transformation logic:",
        "type": "match",
        "left": [
            "StandardScaler",
            "MinMaxScaler",
            "RobustScaler",
            "Normalizer"
        ],
        "right": [
            "Mean 0, Std 1",
            "Range [0, 1]",
            "Uses Median and IQR",
            "Scales samples to unit norm"
        ]
    },
    {
        "q": "What is the output of this code regarding the number of features?",
        "c": "from sklearn.preprocessing import PolynomialFeatures\nimport numpy as np\n# X has 1 feature\nX = np.array([[5], [10], [15]])\n# degree=2, include_bias=True (default)\npoly = PolynomialFeatures(degree=2)\nout = poly.fit_transform(X)\nprint(out.shape[1])",
        "type": "mcq",
        "o": [
            "3",
            "2",
            "1",
            "4"
        ]
    },
    {
        "q": "Rearrange the typical cycle of a machine learning project specifically for regression:",
        "type": "rearrange",
        "words": [
            "Define Problem",
            "Exploratory Analysis",
            "Feature Engineering",
            "Model Selection",
            "Hyperparameter Tuning"
        ]
    },
    {
        "q": "In the Bias-Variance tradeoff, a model that is too simple (underfitting) is said to have high ______.",
        "type": "fill_blank",
        "answers": [
            "bias"
        ],
        "other_options": [
            "variance",
            "complexity",
            "precision"
        ]
    },
    {
        "q": "High multicollinearity among independent variables primarily affects the stability and interpretation of coefficients, not necessarily the model's predictive power.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which sampling technique is used in 'Stochastic Gradient Descent' to update the weights?",
        "type": "mcq",
        "o": [
            "One random training example at a time",
            "The entire dataset at once",
            "A small fixed batch of examples",
            "Only the outliers"
        ]
    },
    {
        "q": "Match the Regression Model to its typical use case based on data characteristics:",
        "type": "match",
        "left": [
            "Ridge Regression",
            "Lasso Regression",
            "Logistic Regression",
            "Polynomial Regression"
        ],
        "right": [
            "Multicollinearity present",
            "Feature Selection needed",
            "Binary Classification",
            "Curvilinear relationship"
        ]
    },
    {
        "q": "What happens when you increase the regularization parameter (alpha or lambda) in a regression model?",
        "type": "mcq",
        "o": [
            "Model variance decreases, bias increases",
            "Model variance increases, bias decreases",
            "Both variance and bias increase",
            "R-squared increases significantly"
        ]
    },
    {
        "q": "The measure '1 - Specificity' corresponds to the ______ Rate in an ROC curve analysis.",
        "type": "fill_blank",
        "answers": [
            "False Positive"
        ],
        "other_options": [
            "True Positive",
            "True Negative",
            "Precision"
        ]
    },
    {
        "q": "Rearrange the steps to calculate the Predicted Residual Error Sum of Squares (PRESS) statistic:",
        "type": "rearrange",
        "words": [
            "Remove one point",
            "Refit model",
            "Predict removed point",
            "Square error",
            "Sum all errors"
        ]
    },
    {
        "q": "Which function in Scikit-Learn allows you to create a pipeline that applies transformations and then fits a model?",
        "c": "from sklearn.pipeline import ______\npipe = ______(steps=[('scaler', StandardScaler()), ('reg', LinearRegression())])",
        "type": "mcq",
        "o": [
            "Pipeline",
            "make_pipeline",
            "Compose",
            "Sequence"
        ]
    },
    {
        "q": "In a boxplot of the residuals, if the median line is not in the center of the box, it suggests that the error distribution is ______.",
        "type": "fill_blank",
        "answers": [
            "skewed"
        ],
        "other_options": [
            "symmetric",
            "normal",
            "uniform"
        ]
    },
    {
        "q": "The 'Huber Regressor' is a linear model that is robust to outliers because it uses a different loss function than squared error for large deviations.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which criterion specifically imposes a harsher penalty for model complexity (number of features) compared to AIC, often favoring simpler models?",
        "type": "mcq",
        "o": [
            "Bayesian Information Criterion (BIC)",
            "Root Mean Squared Error",
            "F-statistic",
            "Log-Likelihood"
        ]
    },
    {
        "q": "What is the specific interpretation of the intercept (beta-0) in a simple linear regression model?",
        "type": "mcq",
        "o": [
            "The expected value of y when x is zero",
            "The change in y for a unit change in x",
            "The mean of all x values",
            "The error when x is infinity"
        ]
    },
    {
        "q": "In a linear regression model, the residuals (errors) are assumed to have a population mean of ______.",
        "type": "fill_blank",
        "answers": [
            "zero"
        ],
        "other_options": [
            "one",
            "constant",
            "infinity"
        ]
    },
    {
        "q": "Correlation does not imply causation: A high R-squared value proves that changes in the independent variable cause changes in the dependent variable.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the term to the correct mathematical relationship:",
        "type": "match",
        "left": [
            "Total Sum of Squares (SST)",
            "Explained Variation",
            "Unexplained Variation",
            "R-squared Formula"
        ],
        "right": [
            "SSR + SSE",
            "Sum of Squares Regression (SSR)",
            "Sum of Squares Error (SSE)",
            "SSR / SST"
        ]
    },
    {
        "q": "What does the second column (index 1) of the output array usually represent in this Scikit-Learn code?",
        "c": "from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression().fit(X, y)\nprobs = clf.predict_proba(X_new)\n# Specifically: probs[:, 1]",
        "type": "mcq",
        "o": [
            "Probability of the positive class (1)",
            "Probability of the negative class (0)",
            "The predicted label",
            "The log-odds value"
        ]
    },
    {
        "q": "Rearrange the calculation flow for the Adjusted R-squared:",
        "type": "rearrange",
        "words": [
            "Calculate 1 minus R-squared",
            "Multiply by n minus 1",
            "Divide by n minus p minus 1",
            "Subtract result from 1"
        ]
    },
    {
        "q": "If you perform a regression on the residuals of a model against the predictor variables and find a significant relationship, it indicates that the model has ______ information.",
        "type": "fill_blank",
        "answers": [
            "missed"
        ],
        "other_options": [
            "captured",
            "perfected",
            "maximized"
        ]
    },
    {
        "q": "In Python statsmodels, the 'Prob (F-statistic)' is essentially the p-value for the overall significance of the model.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which Scikit-Learn utility helps split data while maintaining the same percentage of samples for each class as the original dataset?",
        "type": "mcq",
        "o": [
            "StratifiedShuffleSplit",
            "KFold",
            "StandardScaler",
            "SimpleImputer"
        ]
    },
    {
        "q": "Match the violation to the likely remedy:",
        "type": "match",
        "left": [
            "Non-linearity",
            "Heteroscedasticity",
            "Outliers",
            "Collinearity"
        ],
        "right": [
            "Polynomial features or Log transform",
            "Weighted Least Squares (WLS)",
            "Robust Regression (e.g., RANSAC)",
            "Drop variables or PCA"
        ]
    },
    {
        "q": "In Logistic Regression, if the probability p = 0.5, the log-odds (logit) is exactly ______.",
        "type": "fill_blank",
        "answers": [
            "0"
        ],
        "other_options": [
            "1",
            "0.5",
            "infinity"
        ]
    },
    {
        "q": "What is the key difference between 'LinearRegression' and 'Ridge' in Scikit-Learn regarding the loss function?",
        "c": "LinearRegression: Minimize MSE\nRidge: Minimize MSE + ______ * Sum(coef^2)",
        "type": "mcq",
        "o": [
            "alpha",
            "beta",
            "gamma",
            "delta"
        ]
    },
    {
        "q": "Rearrange the steps to interpret a Logistic Regression coefficient for a continuous variable:",
        "type": "rearrange",
        "words": [
            "Take exponent of coefficient",
            "Subtract one",
            "Multiply by 100",
            "Read as percentage change in odds"
        ]
    },
    {
        "q": "In a regression model, if the variable 'Age' ranges from 0 to 100 and 'Salary' ranges from 10,000 to 1,000,000, the coefficient for 'Age' will likely be very large compared to 'Salary' without scaling.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which type of regression is used when the dependent variable represents time-to-event data (e.g., time until machine failure)?",
        "type": "mcq",
        "o": [
            "Cox Proportional Hazards (Survival)",
            "Logistic Regression",
            "Poisson Regression",
            "Linear Regression"
        ]
    },
    {
        "q": "In the equation y = beta-0 + beta-1 * x + epsilon, the term 'epsilon' represents the ______ error.",
        "type": "fill_blank",
        "answers": [
            "random"
        ],
        "other_options": [
            "systematic",
            "calculated",
            "parameter"
        ]
    },
    {
        "q": "What is the common Scikit-Learn error fix when fitting a model with a single feature (1D array)?",
        "c": "import numpy as np\nfrom sklearn.linear_model import LinearRegression\nX = np.array([1, 2, 3, 4])\ny = np.array([2, 4, 6, 8])\n# model.fit(X, y) # This throws an error\n# How must X be reshaped?",
        "type": "mcq",
        "o": [
            "X.reshape(-1, 1)",
            "X.reshape(1, -1)",
            "X.flatten()",
            "X.transpose()"
        ]
    },
    {
        "q": "In Logistic Regression, what phenomenon occurs when a predictor variable perfectly separates the two classes (0s and 1s), causing coefficients to explode to infinity?",
        "type": "mcq",
        "o": [
            "Complete Separation",
            "Multicollinearity",
            "Homoscedasticity",
            "Underfitting"
        ]
    },
    {
        "q": "The matrix in Linear Regression that maps the vector of observed values (y) to the vector of fitted values (y-hat) is commonly known as the ______ matrix.",
        "type": "fill_blank",
        "answers": [
            "hat"
        ],
        "other_options": [
            "identity",
            "correlation",
            "covariance"
        ]
    },
    {
        "q": "A Logistic Regression Odds Ratio (OR) of 0.5 implies that a one-unit increase in the predictor doubles the odds of the event occurring.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the R-squared variant to its specific purpose:",
        "type": "match",
        "left": [
            "Adjusted R-squared",
            "Predicted R-squared",
            "McFadden's R-squared",
            "Standard R-squared"
        ],
        "right": [
            "Penalizes extra variables",
            "Measures generalization to new data",
            "Used for Logistic Regression (Pseudo)",
            "Basic goodness of fit"
        ]
    },
    {
        "q": "Which assumption is primarily checked using a Partial Regression Plot (Added Variable Plot)?",
        "type": "mcq",
        "o": [
            "Linearity of a specific predictor given others",
            "Normality of residuals",
            "Independence of errors",
            "Homoscedasticity"
        ]
    },
    {
        "q": "Rearrange the logical steps for interpreting a significant interaction term between X1 and X2:",
        "type": "rearrange",
        "words": [
            "Identify Interaction P-value",
            "Confirm Significance",
            "Analyze Simple Slopes",
            "Plot Interaction Effect"
        ]
    },
    {
        "q": "If the residuals of a regression model form a distinct 'U' shape when plotted against the fitted values, the model is likely missing a ______ term.",
        "type": "fill_blank",
        "answers": [
            "squared"
        ],
        "other_options": [
            "linear",
            "constant",
            "interaction"
        ]
    },
    {
        "q": "In Scikit-Learn, the 'predict_proba' method returns an array where each row sums to exactly ______.",
        "type": "fill_blank",
        "answers": [
            "1"
        ],
        "other_options": [
            "0",
            "100",
            "infinity"
        ]
    },
    {
        "q": "What does the 'k' represent in the K-Nearest Neighbors regression algorithm (often used as a baseline comparison for linear regression)?",
        "type": "mcq",
        "o": [
            "Number of neighbors to average",
            "Number of features",
            "Number of iterations",
            "The error threshold"
        ]
    },
    {
        "q": "Match the 'Solver' in LogisticRegression to its compatibility:",
        "type": "match",
        "left": [
            "liblinear",
            "lbfgs",
            "sag",
            "newton-cg"
        ],
        "right": [
            "Good for small datasets/L1 penalty",
            "Default solver, robust",
            "Fast for large datasets",
            "Handles L2/None, expensive Hessian"
        ]
    },
    {
        "q": "Unlike Ordinary Least Squares, Quantile Regression minimizes the sum of absolute residuals to estimate the conditional ______ of the response variable.",
        "type": "fill_blank",
        "answers": [
            "median"
        ],
        "other_options": [
            "mean",
            "mode",
            "variance"
        ]
    },
    {
        "q": "If you train a Linear Regression model on data where X is measured in meters, and then retrain it with X measured in kilometers, the R-squared value will change.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange the components of the 'Odds' formula:",
        "type": "rearrange",
        "words": [
            "Probability of Success",
            "Divided By",
            "One Minus",
            "Probability of Success"
        ]
    },
    {
        "q": "What is the output of checking the coefficients after using StandardScalar?",
        "c": "from sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n# X is scaled to mean=0, std=1\nmodel = LinearRegression().fit(X_scaled, y)\n# The intercept usually represents:",
        "type": "mcq",
        "o": [
            "The mean of y",
            "Zero",
            "The mean of X",
            "The standard deviation of y"
        ]
    },
    {
        "q": "The 'Hosmer-Lemeshow' test is a statistical test for goodness of fit specifically designed for ______ regression models.",
        "type": "fill_blank",
        "answers": [
            "logistic"
        ],
        "other_options": [
            "linear",
            "ridge",
            "polynomial"
        ]
    },
    {
        "q": "Which value measures how far an observation's x-value is from the mean of the x-values, indicating its potential leverage?",
        "type": "mcq",
        "o": [
            "Hat value (h_ii)",
            "p-value",
            "t-statistic",
            "F-value"
        ]
    },
    {
        "q": "In Simple Linear Regression, the F-statistic is mathematically equal to the square of the ______ statistic.",
        "type": "fill_blank",
        "answers": [
            "t"
        ],
        "other_options": [
            "z",
            "chi-square",
            "p"
        ]
    },
    {
        "q": "Which test is specifically used in the statsmodels library (Omnibus test) to check if the residuals are normally distributed?",
        "type": "mcq",
        "o": [
            "D'Agostino's K-squared test",
            "T-test",
            "F-test",
            "Levene's test"
        ]
    },
    {
        "q": "When interpreting a dummy variable coefficient (e.g., 'Is_Male'), the value represents the difference in the predicted mean compared to the ______ category.",
        "type": "fill_blank",
        "answers": [
            "reference"
        ],
        "other_options": [
            "missing",
            "largest",
            "average"
        ]
    },
    {
        "q": "What does the 'rank_' attribute of a fitted LinearRegression model in Scikit-Learn represent?",
        "c": "from sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nprint(reg.rank_)",
        "type": "mcq",
        "o": [
            "Matrix rank of X (number of independent features)",
            "The R-squared value",
            "The number of samples",
            "The number of iterations"
        ]
    },
    {
        "q": "Match the Regularization parameter to its behavior:",
        "type": "match",
        "left": [
            "Large Alpha (Lasso)",
            "Alpha = 0 (Ridge)",
            "Alpha -> Infinity",
            "Elastic Net Ratio = 1"
        ],
        "right": [
            "Sets many coefficients to zero",
            "Becomes OLS Regression",
            "Coefficients approach zero",
            "Becomes Lasso Regression"
        ]
    },
    {
        "q": "If the dependent variable is a count of events (e.g., number of clicks), using Ordinary Least Squares Linear Regression can result in predicting negative counts, which is impossible.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the components of the Akaike Information Criterion (AIC) formula:",
        "type": "rearrange",
        "words": [
            "2 times k",
            "Minus",
            "2 times",
            "Natural Log of Likelihood"
        ]
    },
    {
        "q": "Which plot is best suited to check the assumption of 'Linearity' between a specific predictor and the outcome, while controlling for other predictors?",
        "type": "mcq",
        "o": [
            "Partial Regression Plot (CCPR)",
            "Histogram",
            "Boxplot",
            "Time Series Plot"
        ]
    },
    {
        "q": "In the context of Goodness of Fit, the 'Log-Likelihood' value for a valid model is typically negative, and a value closer to ______ indicates a better fit.",
        "type": "fill_blank",
        "answers": [
            "zero"
        ],
        "other_options": [
            "negative infinity",
            "one",
            "minus one"
        ]
    },
    {
        "q": "What is the primary reason for using 'StratifiedKFold' instead of standard 'KFold' when evaluating a Logistic Regression model?",
        "type": "mcq",
        "o": [
            "To preserve the percentage of samples for each class",
            "To shuffle the data more thoroughly",
            "To speed up the training process",
            "To handle missing values automatically"
        ]
    },
    {
        "q": "Match the specific residual analysis finding to its implication:",
        "type": "match",
        "left": [
            "Residuals vs Time shows waves",
            "Residuals vs Fitted shows curve",
            "Residuals vs Fitted shows fanning",
            "Histogram of Residuals is flat"
        ],
        "right": [
            "Positive Autocorrelation",
            "Missing non-linear term",
            "Heteroscedasticity",
            "Non-normal errors"
        ]
    },
    {
        "q": "For large datasets, computing the closed-form Normal Equation solution (inverse of X-transpose X) is computationally cheaper than Gradient Descent.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which Scikit-Learn function transforms a categorical feature with 5 levels into 5 binary columns (or 4 if dropping one)?",
        "c": "from sklearn.preprocessing import ______\nenc = ______(drop='first')",
        "type": "mcq",
        "o": [
            "OneHotEncoder",
            "LabelEncoder",
            "OrdinalEncoder",
            "Binarizer"
        ]
    },
    {
        "q": "Rearrange the logic of the 'F-test' for overall significance:",
        "type": "rearrange",
        "words": [
            "Compare Model SSR",
            "To",
            "Residual SSE",
            "Adjusted by",
            "Degrees of Freedom"
        ]
    },
    {
        "q": "In a Logistic Regression summary, the 'z-score' associated with a coefficient is calculated by dividing the coefficient estimate by its ______.",
        "type": "fill_blank",
        "answers": [
            "standard error"
        ],
        "other_options": [
            "p-value",
            "variance",
            "mean"
        ]
    },
    {
        "q": "If you perform Linear Regression on a dataset where the Target (y) has been standardized (Z-scored), the intercept of the model will be approximately ______.",
        "type": "mcq",
        "o": [
            "0",
            "1",
            "The mean of y",
            "The standard deviation of y"
        ]
    },
    {
        "q": "A 'Confusion Matrix' is a tool specifically used for evaluating the Goodness of Fit for Linear Regression models.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "In a 'Level-Log' regression model where y = beta0 + beta1 * log(x), a 1% increase in x is associated with a change in y of ______.",
        "type": "mcq",
        "o": [
            "beta1 divided by 100",
            "beta1 multiplied by 100",
            "beta1 exactly",
            "log of beta1"
        ]
    },
    {
        "q": "Which method in Scikit-Learn's LogisticRegression returns the signed distance of each sample to the decision boundary (hyperplane)?",
        "c": "clf = LogisticRegression().fit(X, y)\n# Returns distances, not probabilities",
        "type": "mcq",
        "o": [
            "decision_function()",
            "predict_proba()",
            "predict_log_proba()",
            "score()"
        ]
    },
    {
        "q": "In the context of Ridge Regression, as the regularization parameter alpha approaches infinity, the regression coefficients converge towards ______.",
        "type": "fill_blank",
        "answers": [
            "zero"
        ],
        "other_options": [
            "infinity",
            "one",
            "mean"
        ]
    },
    {
        "q": "Anscombe's Quartet demonstrates that four datasets can have nearly identical descriptive statistics (mean, variance, correlation) but very different distributions and relationships.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the specific test to the regression assumption it evaluates:",
        "type": "match",
        "left": [
            "Anderson-Darling",
            "Goldfeld-Quandt",
            "Box-Cox",
            "Ljung-Box"
        ],
        "right": [
            "Normality of Residuals",
            "Homoscedasticity",
            "Linearity (Transformation)",
            "Absence of Serial Correlation"
        ]
    },
    {
        "q": "When handling categorical variables with 'One-Hot Encoding', if you include all generated columns plus an intercept term, you create a problem known as the ______.",
        "type": "fill_blank",
        "answers": [
            "dummy variable trap"
        ],
        "other_options": [
            "vanishing gradient",
            "bias variance tradeoff",
            "cold start problem"
        ]
    },
    {
        "q": "Rearrange the logical steps to compute the 't-value' for a specific coefficient:",
        "type": "rearrange",
        "words": [
            "Calculate Coefficient",
            "Calculate Standard Error",
            "Divide Coef by Error",
            "Compare to Distribution"
        ]
    },
    {
        "q": "Which metric is the harmonic mean of Precision and Recall, providing a single score for Logistic Regression classification performance?",
        "type": "mcq",
        "o": [
            "F1 Score",
            "Accuracy",
            "Jaccard Index",
            "Brier Score"
        ]
    },
    {
        "q": "In a Multiple Linear Regression model, the 'Degrees of Freedom for Regression' (numerator in F-test) is equal to the number of ______.",
        "type": "fill_blank",
        "answers": [
            "predictors"
        ],
        "other_options": [
            "samples",
            "errors",
            "classes"
        ]
    },
    {
        "q": "What is the purpose of the 'multi_class' parameter in Scikit-Learn's LogisticRegression?",
        "c": "model = LogisticRegression(multi_class='multinomial')",
        "type": "mcq",
        "o": [
            "To handle problems with more than 2 target categories",
            "To use multiple CPU cores",
            "To combine multiple models",
            "To classify multiple features"
        ]
    },
    {
        "q": "Match the term to its description in the context of Model Selection:",
        "type": "match",
        "left": [
            "Stepwise Regression",
            "Best Subset Selection",
            "Cross-Validation",
            "Holdout Method"
        ],
        "right": [
            "Iteratively adding/removing variables",
            "Testing every combination of variables",
            "Rotating train/test splits",
            "Single static train/test split"
        ]
    },
    {
        "q": "For a valid Linear Regression, the residuals must be independent of the predictor variable X. If they are correlated, it implies there is information in X that the model failed to capture.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the parts of the formula for Mean Absolute Error (MAE):",
        "type": "rearrange",
        "words": [
            "Sum of",
            "Absolute",
            "Values of",
            "Errors",
            "Divide by n"
        ]
    },
    {
        "q": "Which Scikit-Learn function is used to calculate the coefficient of determination (R-squared) between the true and predicted values?",
        "c": "from sklearn.metrics import ______\nscore = ______(y_true, y_pred)",
        "type": "mcq",
        "o": [
            "r2_score",
            "explained_variance_score",
            "accuracy_score",
            "f1_score"
        ]
    },
    {
        "q": "The 'Standard Error of the Estimate' (SEE) is essentially the standard deviation of the ______.",
        "type": "fill_blank",
        "answers": [
            "residuals"
        ],
        "other_options": [
            "predictions",
            "coefficients",
            "inputs"
        ]
    },
    {
        "q": "What happens if you try to fit a Linear Regression model with fewer data samples (rows) than features (columns)?",
        "type": "mcq",
        "o": [
            "The system is underdetermined (infinite solutions)",
            "It works perfectly",
            "It causes a stack overflow",
            "It automatically performs PCA"
        ]
    },
    {
        "q": "In Logistic Regression, the 'C' parameter is inversely proportional to lambda. Therefore, a smaller 'C' value implies ______ regularization.",
        "type": "fill_blank",
        "answers": [
            "stronger"
        ],
        "other_options": [
            "weaker",
            "zero",
            "negative"
        ]
    },
    {
        "q": "In a standardized Linear Regression (where both X and y are centered to have mean 0), the intercept term (beta-0) will always be equal to ______.",
        "type": "fill_blank",
        "answers": [
            "zero"
        ],
        "other_options": [
            "one",
            "mean of y",
            "standard deviation"
        ]
    },
    {
        "q": "Which Scikit-Learn parameter allows Logistic Regression to handle imbalanced classes by assigning higher penalties to errors on the minority class?",
        "c": "clf = LogisticRegression(______='balanced')",
        "type": "mcq",
        "o": [
            "class_weight",
            "sample_weight",
            "priors",
            "stratify"
        ]
    },
    {
        "q": "The Condition Number of the matrix (X-transpose * X) is a diagnostic measure used to detect ______.",
        "type": "mcq",
        "o": [
            "Multicollinearity",
            "Heteroscedasticity",
            "Non-normality",
            "Autocorrelation"
        ]
    },
    {
        "q": "Using 'Stepwise Regression' (automatically adding/removing variables based on p-values) is often criticized because it increases the risk of overfitting and p-hacking.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the Cross-Validation technique to its behavior:",
        "type": "match",
        "left": [
            "Leave-One-Out (LOOCV)",
            "Stratified K-Fold",
            "TimeSeriesSplit",
            "GroupKFold"
        ],
        "right": [
            "Uses N-1 samples for training",
            "Preserves class proportions",
            "Respects temporal order (no future peeking)",
            "Prevents leakage from same subject/group"
        ]
    },
    {
        "q": "In a Quantile-Quantile (Q-Q) plot, if the points curve upward at both ends (U-shape relative to the line), it suggests the residual distribution has ______ tails.",
        "type": "fill_blank",
        "answers": [
            "heavy"
        ],
        "other_options": [
            "light",
            "no",
            "skewed"
        ]
    },
    {
        "q": "Rearrange the steps to calculate the Variance Inflation Factor (VIF) for variable X1:",
        "type": "rearrange",
        "words": [
            "Regress X1 against other X's",
            "Calculate R-squared",
            "Subtract R-squared from 1",
            "Invert the result"
        ]
    },
    {
        "q": "What is the result of this code snippet involving feature selection?",
        "c": "from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n# X has 10 features\nselector = RFE(estimator=LinearRegression(), n_features_to_select=3)\nselector.fit(X, y)\nprint(sum(selector.support_))",
        "type": "mcq",
        "o": [
            "3",
            "10",
            "7",
            "1"
        ]
    },
    {
        "q": "The 'Mean Absolute Percentage Error' (MAPE) is undefined if any of the actual values (y) are equal to ______.",
        "type": "fill_blank",
        "answers": [
            "zero"
        ],
        "other_options": [
            "one",
            "negative",
            "outliers"
        ]
    },
    {
        "q": "Logistic Regression belongs to a broader class of models known as Generalized Linear Models (GLM), where the error distribution is assumed to be ______.",
        "type": "mcq",
        "o": [
            "Binomial",
            "Gaussian",
            "Poisson",
            "Gamma"
        ]
    },
    {
        "q": "Match the scaling technique to its formula concept:",
        "type": "match",
        "left": [
            "Min-Max Scaling",
            "Z-Score Standardization",
            "Max Abs Scaling",
            "Log Transformation"
        ],
        "right": [
            "(x - min) / (max - min)",
            "(x - mean) / std_dev",
            "x / abs(max(x))",
            "ln(x) or log10(x)"
        ]
    },
    {
        "q": "A high R-squared value (e.g., 0.95) guarantees that the model has low bias and captures the true underlying physical relationship.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which algorithm is typically used to solve the Linear Regression 'Normal Equation' directly?",
        "type": "mcq",
        "o": [
            "Matrix Inversion (or Pseudo-inverse)",
            "Gradient Descent",
            "Backpropagation",
            "Genetic Algorithms"
        ]
    },
    {
        "q": "Rearrange the hierarchy of model fitting from underfitting to overfitting:",
        "type": "rearrange",
        "words": [
            "High Bias",
            "Optimal Balance",
            "High Variance",
            "Memorizing Noise"
        ]
    },
    {
        "q": "In a 'Multinomial' Logistic Regression with 3 classes (0, 1, 2) and 4 features, what is the shape of the 'coef_' matrix in Scikit-Learn?",
        "type": "mcq",
        "o": [
            "(3, 4)",
            "(1, 4)",
            "(3, 1)",
            "(4, 3)"
        ]
    },
    {
        "q": "When a regression model captures the relationship well, the plot of Residuals vs. Fitted Values should look like ______.",
        "type": "fill_blank",
        "answers": [
            "random noise"
        ],
        "other_options": [
            "a parabola",
            "a straight line",
            "a funnel"
        ]
    },
    {
        "q": "Regularization (Ridge/Lasso) allows you to fit a regression model even when the number of features (p) is greater than the number of samples (n).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which assumption states that the variance of the error terms determines the spread of the observed data points around the regression line?",
        "type": "mcq",
        "o": [
            "Homoscedasticity",
            "Linearity",
            "Independence",
            "Normality"
        ]
    },
    {
        "q": "What is the return type of the 'predict()' method in Scikit-Learn's LinearRegression model?",
        "c": "model = LinearRegression().fit(X, y)\npreds = model.predict(X_test)\nprint(type(preds))",
        "type": "mcq",
        "o": [
            "numpy.ndarray",
            "pandas.DataFrame",
            "list",
            "float"
        ]
    },
    {
        "q": "When comparing two nested models, the one with the significantly lower ______ is generally preferred.",
        "type": "fill_blank",
        "answers": [
            "deviance"
        ],
        "other_options": [
            "R-squared",
            "likelihood",
            "precision"
        ]
    },
    {
        "q": "In a Linear Regression, if you multiply all values of the predictor variable X by 10, the slope coefficient (beta-1) will be divided by 10.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the Regularization Type to its geometric interpretation of the constraint region:",
        "type": "match",
        "left": [
            "Lasso (L1)",
            "Ridge (L2)",
            "Elastic Net",
            "No Regularization"
        ],
        "right": [
            "Diamond (Square rotated)",
            "Circle (Disk)",
            "Combination of Diamond and Circle",
            "Entire Plane (Unconstrained)"
        ]
    },
    {
        "q": "Which Scikit-Learn function calculates the fraction of correctly classified samples?",
        "type": "mcq",
        "o": [
            "accuracy_score",
            "r2_score",
            "mean_squared_error",
            "confusion_matrix"
        ]
    },
    {
        "q": "Rearrange the logical flow of handling missing data before regression:",
        "type": "rearrange",
        "words": [
            "Identify Nulls",
            "Check Pattern",
            "Impute or Drop",
            "Fit Model"
        ]
    },
    {
        "q": "A 'dummy variable' is a binary variable that takes the value of 0 or 1 to indicate the absence or presence of some ______ effect.",
        "type": "fill_blank",
        "answers": [
            "categorical"
        ],
        "other_options": [
            "continuous",
            "random",
            "linear"
        ]
    },
    {
        "q": "If the 'Tolerance' (1 / VIF) of a variable is less than 0.1, it suggests serious multicollinearity issues.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the specific purpose of the 'solver=\"liblinear\"' in Logistic Regression?",
        "c": "LogisticRegression(solver='liblinear', penalty='l1')",
        "type": "mcq",
        "o": [
            "Optimization for small datasets",
            "Parallel processing",
            "Multiclass classification",
            "Gradient boosting"
        ]
    },
    {
        "q": "Match the coefficient sign to the interpretation (assuming significant p-value):",
        "type": "match",
        "left": [
            "Positive Beta",
            "Negative Beta",
            "Zero Beta",
            "Large Beta"
        ],
        "right": [
            "Direct Correlation",
            "Inverse Correlation",
            "No Linear Effect",
            "Strong Effect Magnitude"
        ]
    },
    {
        "q": "The sum of the squared differences between the observed values and the mean of the observed values is known as the ______ Sum of Squares.",
        "type": "fill_blank",
        "answers": [
            "Total"
        ],
        "other_options": [
            "Residual",
            "Regression",
            "Error"
        ]
    },
    {
        "q": "Rearrange the steps to calculate Sensitivity (Recall):",
        "type": "rearrange",
        "words": [
            "True Positives",
            "Divided By",
            "Sum of",
            "True Positives",
            "And False Negatives"
        ]
    },
    {
        "q": "Which technique involves fitting a regression model on subsets of the data and averaging the predictions to reduce variance?",
        "type": "mcq",
        "o": [
            "Bagging",
            "Boosting",
            "Scaling",
            "Pruning"
        ]
    },
    {
        "q": "In Scikit-Learn, calling 'fit()' multiple times on the same model object (without re-initializing) will generally ______.",
        "type": "mcq",
        "o": [
            "overwrite the previous training",
            "add to the previous training",
            "average the weights",
            "raise an error"
        ]
    },
    {
        "q": "The 'One-Standard-Error' rule is a heuristic used in cross-validation to select the most parsimonious model (fewest variables) whose error is within one standard error of the minimum.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Standardized residuals that fall outside the range of ______ are typically flagged as potential outliers.",
        "type": "fill_blank",
        "answers": [
            "-3 to +3"
        ],
        "other_options": [
            "-1 to +1",
            "0 to 1",
            "-10 to +10"
        ]
    },
    {
        "q": "What is the primary purpose of the 'learning_rate' hyperparameter in Gradient Descent optimization for regression?",
        "type": "mcq",
        "o": [
            "Controls the size of the step taken towards the minimum",
            "Determines the number of features to select",
            "Sets the strength of regularization",
            "Initializes the intercept to zero"
        ]
    },
    {
        "q": "Which attribute in Scikit-Learn's LinearRegression stores the number of features seen during the fit?",
        "c": "model = LinearRegression().fit(X, y)\n# Check feature count\nprint(model.______)",
        "type": "mcq",
        "o": [
            "n_features_in_",
            "n_columns_",
            "features_",
            "rank_"
        ]
    },
    {
        "q": "In a 'Log-Level' regression (log(y) vs x), the coefficient beta-1 is multiplied by ______ to interpret the percentage change in y for a one-unit increase in x.",
        "type": "fill_blank",
        "answers": [
            "100"
        ],
        "other_options": [
            "10",
            "1",
            "log(10)"
        ]
    },
    {
        "q": "Weighted Least Squares (WLS) is a modification of Ordinary Least Squares specifically designed to handle data that violates the ______ assumption.",
        "type": "fill_blank",
        "answers": [
            "homoscedasticity"
        ],
        "other_options": [
            "linearity",
            "normality",
            "independence"
        ]
    },
    {
        "q": "Match the Regularization path characteristic to the model:",
        "type": "match",
        "left": [
            "Lasso Path",
            "Ridge Path",
            "Elastic Net Path",
            "Stepwise Path"
        ],
        "right": [
            "Variables enter/leave one by one",
            "Coefficients shrink smoothly",
            "Hybrid shrinking and selection",
            "Discrete inclusions based on p-value"
        ]
    },
    {
        "q": "When computing the R-squared on the *training* set, it is mathematically impossible for the value to be negative if the model includes an intercept.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the components of the t-statistic formula for a regression coefficient:",
        "type": "rearrange",
        "words": [
            "Coefficient Estimate",
            "Minus",
            "Null Hypothesis Value",
            "Divided By",
            "Standard Error"
        ]
    },
    {
        "q": "Which Scikit-Learn metric function should be used to calculate the 'Mean Squared Logarithmic Error' (MSLE), often used when targets are exponentially growing?",
        "type": "mcq",
        "o": [
            "mean_squared_log_error",
            "log_loss",
            "mean_absolute_error",
            "r2_score"
        ]
    },
    {
        "q": "In the presence of perfect multicollinearity, the determinant of the matrix (X-transpose * X) becomes ______.",
        "type": "fill_blank",
        "answers": [
            "zero"
        ],
        "other_options": [
            "one",
            "infinite",
            "negative"
        ]
    },
    {
        "q": "What is the shape of the 'intercept_' attribute for a Logistic Regression model trained on 3 classes (One-vs-Rest) and 5 features?",
        "c": "clf = LogisticRegression(multi_class='ovr').fit(X, y)\n# y has 3 classes\nprint(clf.intercept_.shape)",
        "type": "mcq",
        "o": [
            "(3,)",
            "(5,)",
            "(1,)",
            "(3, 5)"
        ]
    },
    {
        "q": "Match the evaluation metric to its unit of measurement:",
        "type": "match",
        "left": [
            "MSE",
            "RMSE",
            "MAE",
            "R-squared"
        ],
        "right": [
            "Squared units of target",
            "Same units as target",
            "Same units as target (absolute)",
            "Unitless (Ratio)"
        ]
    },
    {
        "q": "The 'Deviance' in Logistic Regression is analogous to the ______ Sum of Squares in Linear Regression.",
        "type": "fill_blank",
        "answers": [
            "Residual"
        ],
        "other_options": [
            "Total",
            "Regression",
            "Explained"
        ]
    },
    {
        "q": "If you perform a linear transformation on the target variable y (e.g., y_new = y * 10), the R-squared value of the new model will increase significantly.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange the steps to calculate the 'Hat Matrix' (Projection Matrix):",
        "type": "rearrange",
        "words": [
            "X",
            "Times",
            "Inverse of X-transpose X",
            "Times",
            "X-transpose"
        ]
    },
    {
        "q": "Which feature selection technique recursively removes features and builds a model on those that remain to rank feature importance?",
        "type": "mcq",
        "o": [
            "Recursive Feature Elimination (RFE)",
            "Lasso",
            "VarianceThreshold",
            "SelectKBest"
        ]
    },
    {
        "q": "In a 'Reciprocal' model where y = beta0 + beta1 * (1/x), as x approaches infinity, y approaches ______.",
        "type": "fill_blank",
        "answers": [
            "beta0"
        ],
        "other_options": [
            "beta1",
            "zero",
            "infinity"
        ]
    },
    {
        "q": "A 'Partial Residual Plot' helps visualize the relationship between a predictor and the response while accounting for the effects of all other predictors.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In the context of Ridge Regression, the regularization parameter 'alpha' controls the trade-off between fitting the training data well and keeping the coefficients ______.",
        "type": "fill_blank",
        "answers": [
            "small"
        ],
        "other_options": [
            "large",
            "positive",
            "negative"
        ]
    },
    {
        "q": "Which Scikit-Learn parameter in 'LogisticRegression' allows you to switch between 'L1' (Lasso) and 'L2' (Ridge) penalties?",
        "c": "from sklearn.linear_model import LogisticRegression\n# Which parameter is changed?\nmodel = LogisticRegression(______='l1')",
        "type": "mcq",
        "o": [
            "penalty",
            "solver",
            "dual",
            "C"
        ]
    },
    {
        "q": "The 'dummy variable trap' typically results in perfect multicollinearity because one variable can be predicted exactly from the others (e.g., sum equals 1).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the Regression diagnostic plot to the specific assumption it validates:",
        "type": "match",
        "left": [
            "Residuals vs Fitted",
            "Normal Q-Q",
            "Scale-Location",
            "Residuals vs Leverage"
        ],
        "right": [
            "Linearity and Homoscedasticity",
            "Normality of error terms",
            "Homoscedasticity (spread)",
            "Influential points"
        ]
    },
    {
        "q": "What is the result of applying 'fit_transform' on the test data instead of just 'transform'?",
        "type": "mcq",
        "o": [
            "Data Leakage",
            "Better Accuracy",
            "Underfitting",
            "Improved Generalization"
        ]
    },
    {
        "q": "In a logistic regression model, an Odds Ratio of 1.0 indicates that there is ______ relationship between the predictor and the event.",
        "type": "fill_blank",
        "answers": [
            "no"
        ],
        "other_options": [
            "strong",
            "positive",
            "inverse"
        ]
    },
    {
        "q": "Rearrange the steps to interpret a Box-Cox transformation:",
        "type": "rearrange",
        "words": [
            "Check Lambda value",
            "If zero use Log",
            "If one use No Change",
            "Apply Transform"
        ]
    },
    {
        "q": "Which statistic is used to compare nested regression models to see if adding variables significantly improves the fit?",
        "type": "mcq",
        "o": [
            "F-statistic (Partial F-test)",
            "Z-score",
            "Pearson Correlation",
            "Spearman Rank"
        ]
    },
    {
        "q": "If the residuals of a regression model are not independent (e.g., autocorrelation), the estimated standard errors of the coefficients will be biased, often leading to falsely low p-values.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does the 'coef_' attribute contain for a Scikit-Learn LinearRegression model trained on a single feature?",
        "c": "from sklearn.linear_model import LinearRegression\n# X shape is (100, 1)\nmodel = LinearRegression().fit(X, y)\nprint(model.coef_)",
        "type": "mcq",
        "o": [
            "An array with 1 value",
            "A single float",
            "An array with 100 values",
            "A matrix of (1, 1)"
        ]
    },
    {
        "q": "Match the Information Criterion to its penalty on model complexity (k = number of parameters, n = number of samples):",
        "type": "match",
        "left": [
            "AIC Penalty",
            "BIC Penalty",
            "R-squared Penalty",
            "Adjusted R-squared Penalty"
        ],
        "right": [
            "2 * k",
            "ln(n) * k",
            "None",
            "Based on degrees of freedom"
        ]
    },
    {
        "q": "In a 'Log-Log' regression model (ln(y) vs ln(x)), the slope coefficient represents the ______ of y with respect to x.",
        "type": "fill_blank",
        "answers": [
            "elasticity"
        ],
        "other_options": [
            "slope",
            "intercept",
            "odds"
        ]
    },
    {
        "q": "Which algorithm is used by Scikit-Learn's 'SGDRegressor' to minimize the cost function?",
        "type": "mcq",
        "o": [
            "Stochastic Gradient Descent",
            "Least Angle Regression",
            "Singular Value Decomposition",
            "Coordinate Descent"
        ]
    },
    {
        "q": "Rearrange the components of the standard error of the slope formula:",
        "type": "rearrange",
        "words": [
            "Standard Error of Estimate",
            "Divided By",
            "Square Root of",
            "Sum of Squared X deviations"
        ]
    },
    {
        "q": "Sensitivity (True Positive Rate) and Specificity (True Negative Rate) are inversely related; as you lower the decision threshold, Sensitivity increases while Specificity decreases.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In Python's statsmodels, the method 'get_influence()' allow you to extract which specific metric for identifying outliers?",
        "c": "import statsmodels.api as sm\nmodel = sm.OLS(y, X).fit()\ninfl = model.get_influence()\n# Accessing a specific metric",
        "type": "mcq",
        "o": [
            "cooks_distance",
            "f_value",
            "r_squared",
            "aic"
        ]
    },
    {
        "q": "If the relationship between X and y is 'U-shaped', a simple linear regression model will likely have a slope close to ______.",
        "type": "fill_blank",
        "answers": [
            "zero"
        ],
        "other_options": [
            "one",
            "infinity",
            "negative"
        ]
    },
    {
        "q": "In a 'Regression through the Origin' (no intercept) model, which standard metric loses its traditional interpretation and can become negative or meaningless?",
        "type": "mcq",
        "o": [
            "R-squared",
            "Mean Squared Error",
            "Slope Coefficient",
            "Standard Error"
        ]
    },
    {
        "q": "What is the shape of the 'coef_' attribute in a Scikit-Learn LogisticRegression model trained with 'multi_class=\"multinomial\"' on 3 classes and 4 features?",
        "c": "model = LogisticRegression(multi_class='multinomial')\nmodel.fit(X, y) # X.shape=(100, 4), y has 3 unique classes",
        "type": "mcq",
        "o": [
            "(3, 4)",
            "(4, 3)",
            "(1, 4)",
            "(12,)"
        ]
    },
    {
        "q": "In a 'Log-Level' regression model where ln(y) = beta0 + beta1 * x, the coefficient beta1 represents the ______ change in y for a one-unit increase in x.",
        "type": "fill_blank",
        "answers": [
            "percentage"
        ],
        "other_options": [
            "absolute",
            "squared",
            "linear"
        ]
    },
    {
        "q": "The 'One-Standard-Error Rule' in Cross-Validation suggests selecting the most parsimonious (simplest) model whose error is within one standard error of the best model.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the Regression variant to its specific objective function modification:",
        "type": "match",
        "left": [
            "Lasso",
            "Ridge",
            "Elastic Net",
            "Quantile Regression"
        ],
        "right": [
            "Adds Absolute value of Coefs",
            "Adds Squared value of Coefs",
            "Adds both Absolute and Squared",
            "Minimizes Absolute Deviations (MAD)"
        ]
    },
    {
        "q": "Which test statistic is commonly used in Logistic Regression to test the significance of individual coefficients (similar to the t-test in Linear Regression)?",
        "type": "mcq",
        "o": [
            "Wald Chi-Squared Test",
            "F-test",
            "Pearson Correlation",
            "Anova"
        ]
    },
    {
        "q": "Rearrange the steps to interpret a 'Dummy Variable' coefficient:",
        "type": "rearrange",
        "words": [
            "Identify Reference Category",
            "Compare Group Mean",
            "To Reference Mean",
            "Difference is Coefficient"
        ]
    },
    {
        "q": "If the Durbin-Watson statistic is exactly 2, it indicates that there is ______ autocorrelation in the residuals.",
        "type": "fill_blank",
        "answers": [
            "no"
        ],
        "other_options": [
            "strong",
            "positive",
            "negative"
        ]
    },
    {
        "q": "In Scikit-Learn, if you use `cross_val_score` with a scoring parameter `neg_mean_squared_error`, the output values will be negative.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does the `fit_transform` method do in the context of a `PolynomialFeatures` preprocessor?",
        "c": "from sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X)",
        "type": "mcq",
        "o": [
            "Generates squared and interaction terms",
            "Scales the data between 0 and 1",
            "Selects the best features",
            "Fits a linear regression model"
        ]
    },
    {
        "q": "Match the Goodness-of-Fit metric to the model type it is most commonly associated with:",
        "type": "match",
        "left": [
            "Adjusted R-squared",
            "Pseudo R-squared (McFadden)",
            "Concordance / Discordance",
            "MAPE (Mean Abs % Error)"
        ],
        "right": [
            "Multiple Linear Regression",
            "Logistic Regression",
            "Binary Classification Ranking",
            "Forecasting / Time Series"
        ]
    },
    {
        "q": "A model that captures the underlying trend of the data but ignores the noise is said to have low bias and low ______.",
        "type": "fill_blank",
        "answers": [
            "variance"
        ],
        "other_options": [
            "accuracy",
            "precision",
            "recall"
        ]
    },
    {
        "q": "Rearrange the components of the 'Total Sum of Squares' (SST) decomposition:",
        "type": "rearrange",
        "words": [
            "Total Variation (SST)",
            "Equals",
            "Explained Variation (SSR)",
            "Plus",
            "Unexplained Variation (SSE)"
        ]
    },
    {
        "q": "If you add an interaction term (x1 * x2) to a model and it is statistically significant, it means the effect of x1 on y depends on the value of x2.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which plot allows you to assess whether the variance of the residuals changes with the fitted values (Heteroscedasticity)?",
        "type": "mcq",
        "o": [
            "Scale-Location Plot (Spread-Location)",
            "Q-Q Plot",
            "Histogram of Residuals",
            "Cook's Distance Plot"
        ]
    },
    {
        "q": "When using 'Stochastic Gradient Descent' (SGD), the data must be ______ to ensure the algorithm converges efficiently.",
        "type": "fill_blank",
        "answers": [
            "scaled"
        ],
        "other_options": [
            "sorted",
            "imputed",
            "categorical"
        ]
    },
    {
        "q": "What is the primary reason for using 'Elastic Net' over 'Lasso'?",
        "type": "mcq",
        "o": [
            "Lasso can behave erratically when features are highly correlated",
            "Elastic Net is computationally faster",
            "Lasso cannot select features",
            "Elastic Net always forces coefficients to zero"
        ]
    },
    {
        "q": "What is the primary function of the 'sm.add_constant(X)' method in the statsmodels library?",
        "c": "import statsmodels.api as sm\nimport numpy as np\nX = np.random.rand(100, 2)\nX_new = sm.add_constant(X)",
        "type": "mcq",
        "o": [
            "Adds a column of ones to the feature matrix",
            "Adds a constant noise term to the data",
            "Normalizes the data to have unit variance",
            "Calculates the intercept automatically"
        ]
    },
    {
        "q": "In a Residual vs. Predictor plot, if the residuals are randomly scattered around the zero line with no discernible pattern, it supports the assumption of ______.",
        "type": "fill_blank",
        "answers": [
            "linearity"
        ],
        "other_options": [
            "normality",
            "collinearity",
            "causality"
        ]
    },
    {
        "q": "When interpreting a regression coefficient for a standardized variable (Z-score), the coefficient represents the change in y (in standard deviations) for a one ______ deviation increase in x.",
        "type": "fill_blank",
        "answers": [
            "standard"
        ],
        "other_options": [
            "unit",
            "average",
            "percent"
        ]
    },
    {
        "q": "In Scikit-Learn, the 'score' method for a LinearRegression estimator returns the Mean Squared Error (MSE).",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the Regression technique to its method of handling outliers:",
        "type": "match",
        "left": [
            "Ordinary Least Squares",
            "Robust Regression (Huber)",
            "Quantile Regression",
            "RANSAC"
        ],
        "right": [
            "Highly sensitive to outliers",
            "Reduces weight of outliers",
            "Estimates median (resistant)",
            "Ignores outliers completely (inliers only)"
        ]
    },
    {
        "q": "Which component of the Generalized Linear Model (GLM) defines the relationship between the linear predictor and the mean of the distribution function?",
        "type": "mcq",
        "o": [
            "Link Function",
            "Loss Function",
            "Basis Function",
            "Activation Function"
        ]
    },
    {
        "q": "Rearrange the steps to interpret the 'Global F-test' in a regression summary:",
        "type": "rearrange",
        "words": [
            "Locate F-statistic",
            "Check Prob(F-stat)",
            "Compare to Alpha",
            "Reject Null Hypothesis"
        ]
    },
    {
        "q": "If a Linear Regression model produces a 'positive' residual for a specific data point, it means the model has ______ the actual value.",
        "type": "fill_blank",
        "answers": [
            "underestimated"
        ],
        "other_options": [
            "overestimated",
            "perfectly predicted",
            "normalized"
        ]
    },
    {
        "q": "Regularization helps prevent overfitting by increasing the bias of the model slightly to significantly reduce the ______.",
        "type": "fill_blank",
        "answers": [
            "variance"
        ],
        "other_options": [
            "error",
            "intercept",
            "correlation"
        ]
    },
    {
        "q": "What does the 'tol' (tolerance) parameter typically control in Scikit-Learn's LogisticRegression solver?",
        "c": "model = LogisticRegression(tol=1e-4, solver='lbfgs')",
        "type": "mcq",
        "o": [
            "Stopping criteria for convergence",
            "The regularization strength",
            "The threshold for classification",
            "The size of the test set"
        ]
    },
    {
        "q": "Match the statistical distance measure to its application in regression diagnosis:",
        "type": "match",
        "left": [
            "Mahalanobis Distance",
            "Euclidean Distance",
            "Cook's Distance",
            "DFFITS"
        ],
        "right": [
            "Multivariate outlier detection in X",
            "Simple geometric distance",
            "Combined leverage and residual influence",
            "Influence on fitted value"
        ]
    },
    {
        "q": "If the 95% Confidence Interval for a regression coefficient is [0.5, 2.5], we can reject the null hypothesis that the coefficient is zero at the 5% significance level.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the calculation flow for the Standard Error of a coefficient:",
        "type": "rearrange",
        "words": [
            "MSE (Mean Square Error)",
            "Divided by",
            "Sum of Squared X deviations",
            "Take Square Root"
        ]
    },
    {
        "q": "Which Scikit-Learn tool allows you to transform a target variable (y) to be normally distributed before fitting a regression model, and then transforms predictions back?",
        "type": "mcq",
        "o": [
            "TransformedTargetRegressor",
            "StandardScaler",
            "Normalizer",
            "RobustScaler"
        ]
    },
    {
        "q": "In a 'Reciprocal' transformation model where y = beta-0 + beta-1 * (1/x), as x increases towards infinity, y approaches the asymptote defined by ______.",
        "type": "fill_blank",
        "answers": [
            "beta-0"
        ],
        "other_options": [
            "beta-1",
            "zero",
            "one"
        ]
    },
    {
        "q": "If two features X1 and X2 are perfectly correlated (r=1), Scikit-Learn's LinearRegression will fail and throw a 'Singular Matrix' error.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What specific error does 'Coordinate Descent' (used in Lasso) address that Gradient Descent might struggle with?",
        "type": "mcq",
        "o": [
            "Non-differentiability of the L1 penalty at zero",
            "Local minima in the cost function",
            "Vanishing gradients in deep networks",
            "High memory usage"
        ]
    },
    {
        "q": "In Generalized Linear Models (GLM), which link function is typically used when the target variable follows a Poisson distribution (count data)?",
        "type": "mcq",
        "o": [
            "Log Link",
            "Logit Link",
            "Identity Link",
            "Inverse Link"
        ]
    },
    {
        "q": "What is the expected output of the 'score()' method on a trained LinearRegression object in Scikit-Learn?",
        "c": "from sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\nprint(reg.score(X, y))",
        "type": "mcq",
        "o": [
            "R-squared (Coefficient of Determination)",
            "Mean Squared Error",
            "Root Mean Squared Error",
            "Adjusted R-squared"
        ]
    },
    {
        "q": "When a model is too simple to capture the underlying structure of the data (e.g., fitting a straight line to a curve), it is said to have high ______.",
        "type": "fill_blank",
        "answers": [
            "bias"
        ],
        "other_options": [
            "variance",
            "complexity",
            "precision"
        ]
    },
    {
        "q": "The 'Homoscedasticity' assumption requires that the residuals have constant variance, regardless of the value of the predictor variable.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the Regression outlier type to its description:",
        "type": "match",
        "left": [
            "High Leverage Point",
            "Influential Point",
            "Outlier in Y",
            "Centroid"
        ],
        "right": [
            "Extreme value in X direction",
            "Significantly changes slope if removed",
            "Large residual (vertical distance)",
            "Point defined by (mean X, mean Y)"
        ]
    },
    {
        "q": "Which Scikit-Learn class allows you to perform linear regression on a dataset that fits into memory but is too large to fit the closed-form solution (Normal Equation)?",
        "type": "mcq",
        "o": [
            "SGDRegressor",
            "KernelRidge",
            "GaussianProcessRegressor",
            "LassoLars"
        ]
    },
    {
        "q": "Rearrange the steps for performing 'K-Fold Cross-Validation':",
        "type": "rearrange",
        "words": [
            "Shuffle Dataset",
            "Split into K Groups",
            "Train on K-1 Groups",
            "Test on 1 Group",
            "Average the Scores"
        ]
    },
    {
        "q": "In the context of model selection, the principle of ______ suggests preferring the simpler model when two models have similar predictive performance.",
        "type": "fill_blank",
        "answers": [
            "parsimony"
        ],
        "other_options": [
            "complexity",
            "duality",
            "variance"
        ]
    },
    {
        "q": "Multicollinearity (high correlation between features) significantly degrades the predictive accuracy of a model on new data.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What issue is indicated if the 'Condition Number' of the design matrix X is extremely large?",
        "type": "mcq",
        "o": [
            "Severe Multicollinearity",
            "Heteroscedasticity",
            "Autocorrelation",
            "Non-normality of errors"
        ]
    },
    {
        "q": "Match the pattern in a 'Residuals vs Fitted' plot to the violation:",
        "type": "match",
        "left": [
            "Fan Shape (Cone)",
            "U-Shape (Parabola)",
            "Sine Wave pattern",
            "Random Cloud"
        ],
        "right": [
            "Heteroscedasticity",
            "Non-linearity (Missing quadratic)",
            "Autocorrelation / Seasonality",
            "No Violation (Good Fit)"
        ]
    },
    {
        "q": "To capture the effect where the impact of Variable A on the target depends on the value of Variable B, you must include an ______ term.",
        "type": "fill_blank",
        "answers": [
            "interaction"
        ],
        "other_options": [
            "intercept",
            "error",
            "identity"
        ]
    },
    {
        "q": "Rearrange the components of the F-statistic formula logic:",
        "type": "rearrange",
        "words": [
            "Mean Square Regression",
            "Divided By",
            "Mean Square Error",
            "Equals",
            "F-value"
        ]
    },
    {
        "q": "In Scikit-Learn, which encoding technique is safest for Linear Regression to avoid introducing an arbitrary ordinal relationship (e.g., Red=1, Blue=2)?",
        "type": "mcq",
        "o": [
            "OneHotEncoder",
            "OrdinalEncoder",
            "LabelEncoder",
            "MinMaxScaler"
        ]
    },
    {
        "q": "The 'Intercept' (beta-0) in a regression model represents the predicted value of y when all independent variables (x) are equal to ______.",
        "type": "fill_blank",
        "answers": [
            "zero"
        ],
        "other_options": [
            "one",
            "mean",
            "median"
        ]
    },
    {
        "q": "Comparing Akaike Information Criterion (AIC) values: A model with an AIC of 500 is considered better than a model with an AIC of 600.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does the 'predict_proba' method return for a binary classification problem (0 vs 1) in Scikit-Learn?",
        "c": "probs = model.predict_proba(X_test)\nprint(probs.shape) # Assuming 10 test samples",
        "type": "mcq",
        "o": [
            "(10, 2)",
            "(10, 1)",
            "(10,)",
            "(2, 10)"
        ]
    },
    {
        "q": "Which statistical interval estimates the range in which a single future observation will fall, making it wider than the confidence interval for the mean?",
        "type": "mcq",
        "o": [
            "Prediction Interval",
            "Confidence Interval",
            "Credible Interval",
            "Tolerance Interval"
        ]
    },
    {
        "q": "What is the specific value of the Durbin-Watson statistic that indicates perfect positive autocorrelation in the residuals?",
        "type": "mcq",
        "o": [
            "0",
            "2",
            "4",
            "1"
        ]
    },
    {
        "q": "In a 'Log-Level' regression model (log(y) = beta0 + beta1 * x), the coefficient beta1 is often multiplied by 100 to be interpreted as a ______ change in y.",
        "type": "fill_blank",
        "answers": [
            "percentage"
        ],
        "other_options": [
            "unit",
            "standard",
            "logarithmic"
        ]
    },
    {
        "q": "The 'Zero Conditional Mean' assumption states that the expected value of the error term is zero for any given value of the independent variable.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the Regression Test to the specific violation it detects:",
        "type": "match",
        "left": [
            "White Test",
            "Jarque-Bera Test",
            "Box-Cox Test",
            "Ramsey RESET Test"
        ],
        "right": [
            "Heteroscedasticity",
            "Non-Normality of Errors",
            "Need for Power Transformation",
            "Misspecification / Missing Variables"
        ]
    },
    {
        "q": "What happens to the R-squared value if you force the regression line to pass through the origin (intercept = 0) when the data actually has a non-zero intercept?",
        "type": "mcq",
        "o": [
            "It loses its standard meaning and can be negative",
            "It becomes exactly 1.0",
            "It remains unchanged",
            "It doubles"
        ]
    },
    {
        "q": "Rearrange the logical steps for 'Backward Elimination' in feature selection:",
        "type": "rearrange",
        "words": [
            "Fit model with all features",
            "Check P-values",
            "Remove feature with highest P-value > alpha",
            "Refit Model",
            "Repeat until all significant"
        ]
    },
    {
        "q": "The difference between the 'Error Term' and the 'Residual' is that the Error Term is theoretical and unobservable, while the Residual is ______.",
        "type": "fill_blank",
        "answers": [
            "observable"
        ],
        "other_options": [
            "random",
            "constant",
            "infinite"
        ]
    },
    {
        "q": "In Logistic Regression, the 'Log-Likelihood' value is maximized to find the best fitting parameters.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output shape of 'coef_' for a multi-output regression problem with 3 target variables and 5 features?",
        "c": "from sklearn.linear_model import LinearRegression\n# y has shape (n_samples, 3)\n# X has shape (n_samples, 5)\nmodel = LinearRegression().fit(X, y)\nprint(model.coef_.shape)",
        "type": "mcq",
        "o": [
            "(3, 5)",
            "(5, 3)",
            "(3,)",
            "(5,)"
        ]
    },
    {
        "q": "Match the concept to its mathematical representation in Simple Linear Regression:",
        "type": "match",
        "left": [
            "Slope (beta-1)",
            "Intercept (beta-0)",
            "Fitted Value (y-hat)",
            "Residual (e)"
        ],
        "right": [
            "Rise over Run",
            "Value of y when x=0",
            "Point on the regression line",
            "Vertical distance from point to line"
        ]
    },
    {
        "q": "To compare coefficients of variables measured in different units (e.g., Age vs Income), one should look at the ______ coefficients.",
        "type": "fill_blank",
        "answers": [
            "standardized"
        ],
        "other_options": [
            "unstandardized",
            "raw",
            "intercept"
        ]
    },
    {
        "q": "Which Scikit-Learn parameter in 'SGDRegressor' determines the type of regularization (L1, L2, or Elastic Net)?",
        "type": "mcq",
        "o": [
            "penalty",
            "alpha",
            "loss",
            "learning_rate"
        ]
    },
    {
        "q": "Rearrange the components of the 'Standardized Residual' calculation:",
        "type": "rearrange",
        "words": [
            "Residual",
            "Divided By",
            "Standard Deviation of Residuals",
            "Times",
            "Square Root of 1 minus Leverage"
        ]
    },
    {
        "q": "Adding a polynomial term (e.g., x-squared) to a linear regression model makes the model 'Non-Linear' in terms of parameters.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What does the 'predict_log_proba' method return, which is often numerically more stable than 'predict_proba'?",
        "c": "model = LogisticRegression()\nmodel.fit(X, y)\n# Returns natural logarithm of:",
        "type": "mcq",
        "o": [
            "The probability estimates",
            "The odds ratios",
            "The coefficients",
            "The feature matrix"
        ]
    },
    {
        "q": "If the 'Variance Inflation Factor' (VIF) for a variable is 1, it implies that the variable has ______ correlation with the other predictors.",
        "type": "fill_blank",
        "answers": [
            "zero"
        ],
        "other_options": [
            "perfect",
            "high",
            "negative"
        ]
    },
    {
        "q": "Which statistical test is designed to detect Heteroscedasticity (non-constant variance) by regressing the squared residuals on the original regressors and their squares/cross-products?",
        "type": "mcq",
        "o": [
            "White's Test",
            "Durbin-Watson Test",
            "Shapiro-Wilk Test",
            "T-test"
        ]
    },
    {
        "q": "In a 'Log-Log' regression model where ln(y) = beta0 + beta1 * ln(x), the coefficient beta1 represents the ______ of y with respect to x.",
        "type": "fill_blank",
        "answers": [
            "elasticity"
        ],
        "other_options": [
            "slope",
            "intercept",
            "variance"
        ]
    },
    {
        "q": "What is the primary function of the 'Hat Matrix' (Projection Matrix) in linear algebra terms for regression?",
        "type": "mcq",
        "o": [
            "Projects observed y values onto the subspace spanned by X",
            "Inverts the feature matrix",
            "Calculates the correlation coefficients",
            "Standardizes the residuals"
        ]
    },
    {
        "q": "When fitting a regression model without an intercept (through the origin), the standard R-squared formula remains valid and interpretable.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the Residual type to its definition:",
        "type": "match",
        "left": [
            "Standardized Residual",
            "Studentized Residual",
            "Press Residual",
            "Partial Residual"
        ],
        "right": [
            "Residual divided by its standard deviation",
            "Residual divided by estimated standard deviation (deleted)",
            "Residual from model fitted without that observation",
            "Residual adding back component of one predictor"
        ]
    },
    {
        "q": "The F-statistic in the regression output tests the Global Null Hypothesis that ______.",
        "type": "mcq",
        "o": [
            "all slope coefficients are equal to zero",
            "the intercept is zero",
            "the residuals are normally distributed",
            "the model has no multicollinearity"
        ]
    },
    {
        "q": "Rearrange the components of the standard confidence interval for a mean response:",
        "type": "rearrange",
        "words": [
            "Predicted Mean",
            "Plus or Minus",
            "Critical T-value",
            "Times",
            "Standard Error of Mean"
        ]
    },
    {
        "q": "Which Scikit-Learn attribute allows you to inspect the 'intercept' (bias term) of a fitted SGDRegressor model?",
        "c": "from sklearn.linear_model import SGDRegressor\nmodel = SGDRegressor().fit(X, y)\n# Attribute name?",
        "type": "mcq",
        "o": [
            "intercept_",
            "bias_",
            "coef0_",
            "offset_"
        ]
    },
    {
        "q": "In Logistic Regression, the assumption of Homoscedasticity (constant variance of errors) is not required because the variance depends on the ______.",
        "type": "fill_blank",
        "answers": [
            "mean"
        ],
        "other_options": [
            "median",
            "mode",
            "intercept"
        ]
    },
    {
        "q": "Match the Information Criterion to its specific property:",
        "type": "match",
        "left": [
            "AIC",
            "BIC",
            "Adjusted R-squared",
            "Mallows Cp"
        ],
        "right": [
            "Approximates KL-divergence",
            "Stronger penalty for model complexity (log n)",
            "Corrects R-squared for number of predictors",
            "Estimates prediction error (bias-variance)"
        ]
    },
    {
        "q": "The variance of the Ordinary Least Squares (OLS) estimator for the slope decreases as the variance of the independent variable (X) ______.",
        "type": "fill_blank",
        "answers": [
            "increases"
        ],
        "other_options": [
            "decreases",
            "stays constant",
            "approaches zero"
        ]
    },
    {
        "q": "If a data point has high leverage but a small residual, it may not necessarily be an influential point (having high Cook's distance).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does the 'decision_function' method return in a Logistic Regression classifier?",
        "c": "clf = LogisticRegression().fit(X, y)\nscores = clf.decision_function(X)",
        "type": "mcq",
        "o": [
            "The distance to the separating hyperplane",
            "The probability of class 1",
            "The predicted class labels",
            "The accuracy score"
        ]
    },
    {
        "q": "Rearrange the steps to interpret a Partial Regression Plot (Added Variable Plot):",
        "type": "rearrange",
        "words": [
            "Regress Y on other Xs",
            "Regress target X on other Xs",
            "Plot Residuals vs Residuals",
            "Slope is Coefficient"
        ]
    },
    {
        "q": "Which term describes the situation where the independent variables are correlated with the error term, violating the OLS assumption?",
        "type": "mcq",
        "o": [
            "Endogeneity",
            "Exogeneity",
            "Homogeneity",
            "Stationarity"
        ]
    },
    {
        "q": "In a 'Polynomial Regression' of degree 2, if the coefficient of the squared term is negative, the regression curve is ______.",
        "type": "fill_blank",
        "answers": [
            "concave down"
        ],
        "other_options": [
            "concave up",
            "linear",
            "exponential"
        ]
    },
    {
        "q": "Using 'Mean Absolute Error' (MAE) instead of 'Mean Squared Error' (MSE) as a loss function makes the regression model more robust to outliers.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which specific test is often used to determine if a linear regression model requires non-linear terms (checking for specification error)?",
        "type": "mcq",
        "o": [
            "Ramsey RESET Test",
            "Shapiro-Wilk Test",
            "Levene Test",
            "Bartlett Test"
        ]
    },
    {
        "q": "In Python's statsmodels formula API, how do you specify an interaction term between two variables 'a' and 'b' along with their individual main effects?",
        "c": "import statsmodels.formula.api as smf\n# Formula string syntax\nformula = 'y ~ ______'",
        "type": "mcq",
        "o": [
            "a * b",
            "a : b",
            "a & b",
            "a + b + a:b:c"
        ]
    },
    {
        "q": "The Box-Cox transformation, used to stabilize variance and make data more normal, strictly requires that all input data values must be ______.",
        "type": "fill_blank",
        "answers": [
            "positive"
        ],
        "other_options": [
            "negative",
            "integers",
            "zero"
        ]
    },
    {
        "q": "In a 'Log-Linear' model (log(y) = beta0 + beta1 * x), the exponentiated coefficient (exp(beta1)) represents the multiplicative factor change in y for a unit increase in x.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the Regression term to its alternative name or description:",
        "type": "match",
        "left": [
            "Independent Variable",
            "Dependent Variable",
            "Residual Sum of Squares",
            "Regression Sum of Squares"
        ],
        "right": [
            "Exogenous / Regressor",
            "Endogenous / Response",
            "Unexplained Variation",
            "Explained Variation"
        ]
    },
    {
        "q": "When using 'Lasso' regression, as the penalty parameter alpha increases, the variance of the model ______.",
        "type": "fill_blank",
        "answers": [
            "decreases"
        ],
        "other_options": [
            "increases",
            "fluctuates",
            "stays constant"
        ]
    },
    {
        "q": "Rearrange the calculation steps for McFadden's Pseudo R-squared (Logistic Regression):",
        "type": "rearrange",
        "words": [
            "Log-Likelihood of Fitted Model",
            "Divided by",
            "Log-Likelihood of Null Model",
            "Subtract result from 1"
        ]
    },
    {
        "q": "Which plot specifically displays the coefficients of a model as a function of the regularization parameter (alpha/lambda)?",
        "type": "mcq",
        "o": [
            "Regularization Path",
            "Learning Curve",
            "ROC Curve",
            "Confusion Matrix"
        ]
    },
    {
        "q": "In Ordinary Least Squares, if the errors are Heteroscedastic, the standard errors of the coefficients will be incorrect, but the coefficient estimates themselves remain unbiased.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the result of the following code regarding interaction terms?",
        "c": "from sklearn.preprocessing import PolynomialFeatures\nimport numpy as np\nX = np.ones((1, 3)) # 3 features: a, b, c\n# interaction_only=True excludes a^2, b^2, c^2\npoly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\nprint(poly.fit_transform(X).shape[1])",
        "type": "mcq",
        "o": [
            "6",
            "3",
            "9",
            "4"
        ]
    },
    {
        "q": "Match the Scikit-Learn Linear Model to its characteristic:",
        "type": "match",
        "left": [
            "HuberRegressor",
            "RANSACRegressor",
            "TheilSenRegressor",
            "LinearRegression"
        ],
        "right": [
            "Linear loss for outliers, squared for inliers",
            "Fits model on random inlier subsets",
            "Uses median of slopes (robust)",
            "Minimizes Mean Squared Error"
        ]
    },
    {
        "q": "The 'Yeo-Johnson' transformation is often preferred over 'Box-Cox' because it can handle ______ values.",
        "type": "fill_blank",
        "answers": [
            "negative"
        ],
        "other_options": [
            "large",
            "missing",
            "infinite"
        ]
    },
    {
        "q": "Which statistic in the statsmodels summary is related to the skewness and kurtosis of the residuals (testing for Normality)?",
        "type": "mcq",
        "o": [
            "Jarque-Bera (JB)",
            "Durbin-Watson",
            "Condition Number",
            "F-statistic"
        ]
    },
    {
        "q": "Rearrange the typical order of model selection using Information Criteria (finding the best fit):",
        "type": "rearrange",
        "words": [
            "Fit Candidate Models",
            "Calculate AIC for each",
            "Identify Lowest AIC",
            "Select Associated Model"
        ]
    },
    {
        "q": "Centering the independent variables (subtracting the mean) in a regression model affects the slope coefficients (beta-1, etc.).",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "The 'Hinge Loss' function is primarily associated with Support Vector Machines, whereas Logistic Regression uses ______ Loss.",
        "type": "fill_blank",
        "answers": [
            "Log"
        ],
        "other_options": [
            "Linear",
            "Absolute",
            "Huber"
        ]
    },
    {
        "q": "Which parameter in 'SGDRegressor' determines the schedule for decreasing the learning rate over time?",
        "type": "mcq",
        "o": [
            "learning_rate='invscaling'",
            "penalty='l2'",
            "loss='squared_error'",
            "shuffle=True"
        ]
    },
    {
        "q": "If you multiply the dependent variable (y) by a constant factor of 10, what happens to the R-squared value of the linear regression model?",
        "type": "mcq",
        "o": [
            "It remains exactly the same",
            "It is multiplied by 10",
            "It is divided by 10",
            "It increases slightly"
        ]
    },
    {
        "q": "Which Scikit-Learn function allows you to apply different preprocessing transformations to different columns (e.g., scaling numericals and encoding categoricals) before regression?",
        "c": "from sklearn.compose import ______",
        "type": "mcq",
        "o": [
            "ColumnTransformer",
            "FeatureUnion",
            "Pipeline",
            "FunctionTransformer"
        ]
    },
    {
        "q": "In a diagnostic plot of 'Residuals vs Leverage', the contour lines typically represent specific values of ______ distance to identify influential points.",
        "type": "fill_blank",
        "answers": [
            "Cook's"
        ],
        "other_options": [
            "Euclidean",
            "Mahalanobis",
            "Manhattan"
        ]
    },
    {
        "q": "When the number of features (p) equals the number of samples (n) in Ordinary Least Squares, the model will typically achieve an R-squared of 1.0 but will generalize poorly.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the term to its role in the Bias-Variance Decomposition of error:",
        "type": "match",
        "left": [
            "Bias",
            "Variance",
            "Irreducible Error",
            "Total Error"
        ],
        "right": [
            "Error from erroneous assumptions (underfitting)",
            "Sensitivity to fluctuations in training set (overfitting)",
            "Noise in the system that cannot be modeled",
            "Sum of Bias^2 + Variance + Irreducible Error"
        ]
    },
    {
        "q": "Which type of regression is specifically designed to minimize the effect of outliers by using the Median Absolute Deviation rather than Mean Squared Error?",
        "type": "mcq",
        "o": [
            "Least Absolute Deviations (LAD)",
            "Ordinary Least Squares",
            "Ridge Regression",
            "Stepwise Regression"
        ]
    },
    {
        "q": "Rearrange the logical steps for interpreting a 'p-value' of a regression coefficient:",
        "type": "rearrange",
        "words": [
            "Assume Null Hypothesis",
            "Calculate Test Statistic",
            "Determine probability",
            "Compare to Alpha",
            "Decide Significance"
        ]
    },
    {
        "q": "In Logistic Regression, the 'Decision Boundary' is the set of points where the estimated probability is exactly ______.",
        "type": "fill_blank",
        "answers": [
            "0.5"
        ],
        "other_options": [
            "0.0",
            "1.0",
            "0.05"
        ]
    },
    {
        "q": "The 'explained_variance_score' in Scikit-Learn is identical to R-squared only if the mean of the residuals is zero.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this interaction checking code?",
        "c": "from sklearn.preprocessing import PolynomialFeatures\nimport numpy as np\nX = np.array([[2, 3]])\n# degree=2, interaction_only=True\n# Features: [x1, x2, x1*x2]\npoly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\nprint(poly.fit_transform(X)[0][2])",
        "type": "mcq",
        "o": [
            "6.0",
            "5.0",
            "2.0",
            "3.0"
        ]
    },
    {
        "q": "Match the Scikit-Learn Estimator to its handling of regularization:",
        "type": "match",
        "left": [
            "LinearRegression",
            "Ridge",
            "Lasso",
            "ElasticNet"
        ],
        "right": [
            "No Regularization",
            "L2 Regularization",
            "L1 Regularization",
            "L1 and L2 Combined"
        ]
    },
    {
        "q": "If you perform a regression of y on x, and then a regression of x on y, the t-statistic for the slope will be the ______ in both cases.",
        "type": "fill_blank",
        "answers": [
            "same"
        ],
        "other_options": [
            "inverse",
            "negative",
            "squared"
        ]
    },
    {
        "q": "Which specific assumption states that the independent variables are not correlated with each other?",
        "type": "mcq",
        "o": [
            "No Multicollinearity",
            "No Autocorrelation",
            "Homoscedasticity",
            "Linearity"
        ]
    },
    {
        "q": "Rearrange the components of the 'Total Degrees of Freedom' decomposition:",
        "type": "rearrange",
        "words": [
            "Total DF (n-1)",
            "Equals",
            "Regression DF (p)",
            "Plus",
            "Error DF (n-p-1)"
        ]
    },
    {
        "q": "For a 'Binary' predictor variable in regression (e.g., 0 for Placebo, 1 for Drug), the intercept (beta-0) represents the mean response for the ______ group.",
        "type": "fill_blank",
        "answers": [
            "Placebo"
        ],
        "other_options": [
            "Drug",
            "Average",
            "Total"
        ]
    },
    {
        "q": "In Scikit-Learn's `SGDRegressor`, which parameter setting allows the model to perform 'Early Stopping' to prevent overfitting?",
        "type": "mcq",
        "o": [
            "early_stopping=True",
            "penalty='early'",
            "learning_rate='stop'",
            "validation_fraction=0.0"
        ]
    },
    {
        "q": "A 'Confounding Variable' is an extraneous variable that correlates with both the dependent variable and the independent variable, potentially creating a false relationship.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In Simple Linear Regression, the relationship between the t-statistic of the slope and the F-statistic of the model is mathematically described as ______.",
        "type": "fill_blank",
        "answers": [
            "t-squared equals F"
        ],
        "other_options": [
            "t equals F",
            "t equals square root of F",
            "t plus F equals 1"
        ]
    },
    {
        "q": "What is the primary purpose of using a 'Dummy Regressor' (or Null Model) in Scikit-Learn before training a complex regression model?",
        "type": "mcq",
        "o": [
            "To establish a baseline performance metric",
            "To handle missing values",
            "To encode categorical variables",
            "To perform feature selection"
        ]
    },
    {
        "q": "Which Scikit-Learn function is used to generate predictions via Cross-Validation, often used to visualize the spread of predictions on the training set?",
        "c": "from sklearn.model_selection import ______\npreds = ______(model, X, y, cv=5)",
        "type": "mcq",
        "o": [
            "cross_val_predict",
            "cross_val_score",
            "GridSearchCV",
            "validation_curve"
        ]
    },
    {
        "q": "The 'Standard Error of the Regression' (S) represents the average distance that the observed values fall from the regression line and is measured in the ______ units as the dependent variable.",
        "type": "fill_blank",
        "answers": [
            "same"
        ],
        "other_options": [
            "squared",
            "percentage",
            "inverse"
        ]
    },
    {
        "q": "Match the feature engineering technique to its Scikit-Learn implementation:",
        "type": "match",
        "left": [
            "Polynomial Features",
            "Log Transformation",
            "Discretization (Binning)",
            "Custom Function"
        ],
        "right": [
            "PolynomialFeatures()",
            "FunctionTransformer(np.log)",
            "KBinsDiscretizer()",
            "FunctionTransformer()"
        ]
    },
    {
        "q": "In a 'Level-Level' regression (y vs x), the intercept has no physical meaning if the predictor x cannot realistically take the value of zero (e.g., Height of a person).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the logical steps to calculate the 'Odds Ratio' from a Logistic Regression coefficient:",
        "type": "rearrange",
        "words": [
            "Get Coefficient (beta)",
            "Calculate Exponential of beta",
            "Result is Odds Ratio",
            "Interpret Magnitude"
        ]
    },
    {
        "q": "Which assumption implies that the error terms (epsilon) are purely random and contain no predictive information?",
        "type": "mcq",
        "o": [
            "Independence (No Autocorrelation)",
            "Normality",
            "Linearity",
            "Multicollinearity"
        ]
    },
    {
        "q": "What is the specific behavior of Lasso Regression when two independent variables are highly correlated?",
        "type": "mcq",
        "o": [
            "It arbitrarily selects one and shrinks the other to zero",
            "It keeps both and averages their coefficients",
            "It shrinks both coefficients equally",
            "It increases the coefficients of both"
        ]
    },
    {
        "q": "In Scikit-Learn, the 'SGDRegressor' is sensitive to feature scaling. If you fit it without scaling X, the algorithm will likely ______.",
        "type": "fill_blank",
        "answers": [
            "fail to converge"
        ],
        "other_options": [
            "converge faster",
            "overfit",
            "underfit"
        ]
    },
    {
        "q": "Match the residual plot shape to the recommended transformation:",
        "type": "match",
        "left": [
            "Funnel Shape (Heteroscedasticity)",
            "Curved/U-Shape",
            "Right Skewed Distribution",
            "Outliers present"
        ],
        "right": [
            "Log(y) or Box-Cox",
            "Polynomial Terms (x squared)",
            "Log(y) or Sqrt(y)",
            "Robust Regression"
        ]
    },
    {
        "q": "When using `statsmodels`, if the 'Kurtosis' of the residuals is significantly higher than 3, the residual distribution is said to be ______.",
        "type": "fill_blank",
        "answers": [
            "leptokurtic"
        ],
        "other_options": [
            "platykurtic",
            "normal",
            "symmetric"
        ]
    },
    {
        "q": "What does the 'warm_start=True' parameter allow in Scikit-Learn iterative models like SGDRegressor?",
        "type": "mcq",
        "o": [
            "Reuse solution of previous call to fit as initialization",
            "Randomly initialize weights every time",
            "Heat up the CPU before training",
            "Start training with a high learning rate"
        ]
    },
    {
        "q": "Rearrange the components of the 'General Linear Test' (Partial F-Test) formula structure:",
        "type": "rearrange",
        "words": [
            "SSE(Reduced) minus SSE(Full)",
            "Divided by number of extra params",
            "Divided by MSE(Full)",
            "Equals F-statistic"
        ]
    },
    {
        "q": "If the dependent variable is binary (0/1), Linear Regression (OLS) is technically usable but can predict probabilities greater than 1 or less than 0.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the resulting shape of the array after this operation?",
        "c": "import numpy as np\ny_pred = np.array([0, 1, 0, 1])\ny_true = np.array([0, 1, 0, 0])\n# Calculating residuals manually\nresid = y_true - y_pred\nprint(resid.shape)",
        "type": "mcq",
        "o": [
            "(4,)",
            "(4, 1)",
            "(1, 4)",
            "(2, 2)"
        ]
    },
    {
        "q": "In the context of Stepwise Regression, 'Forward Selection' is computationally ______ than 'Best Subset Selection' because it explores fewer models.",
        "type": "fill_blank",
        "answers": [
            "cheaper"
        ],
        "other_options": [
            "expensive",
            "slower",
            "complex"
        ]
    },
    {
        "q": "Which assumption of Linear Regression states that the residuals (errors) should not show any distinct patterns when plotted against the predicted values?",
        "type": "mcq",
        "o": [
            "Linearity",
            "Normality",
            "Multicollinearity",
            "Independence"
        ]
    },
    {
        "q": "What is the expected output of this code snippet?",
        "c": "from sklearn.metrics import r2_score\ny_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\nprint(r2_score(y_true, y_pred) <= 1.0)",
        "type": "mcq",
        "o": [
            "True",
            "False",
            "Error",
            "None"
        ]
    },
    {
        "q": "In a 'Level-Log' regression model where y = beta0 + beta1 * ln(x), if beta1 is 5.0, a 1% increase in x is associated with an approximate increase in y of ______.",
        "type": "fill_blank",
        "answers": [
            "0.05"
        ],
        "other_options": [
            "5.0",
            "0.5",
            "500"
        ]
    },
    {
        "q": "The 'Mean Squared Error' (MSE) is strictly a convex function for Linear Regression, ensuring that Gradient Descent will converge to the global minimum.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the Regression Selection Criterion to its formula structure (where L is Likelihood, k is parameters, n is samples):",
        "type": "match",
        "left": [
            "AIC",
            "BIC",
            "R-squared",
            "Adjusted R-squared"
        ],
        "right": [
            "2k - 2ln(L)",
            "k*ln(n) - 2ln(L)",
            "1 - (SSE/SST)",
            "1 - (1-R2)(n-1)/(n-k-1)"
        ]
    },
    {
        "q": "Which technique is specifically used to remedy the violation of 'Homoscedasticity' (non-constant variance) by giving less influence to observations with higher variance?",
        "type": "mcq",
        "o": [
            "Weighted Least Squares (WLS)",
            "Ordinary Least Squares (OLS)",
            "Ridge Regression",
            "Principal Component Regression"
        ]
    },
    {
        "q": "Rearrange the logical steps of the 'Backward Elimination' feature selection process:",
        "type": "rearrange",
        "words": [
            "Fit Full Model",
            "Check P-values",
            "Remove Max P-value",
            "Refit Model",
            "Stop when all significant"
        ]
    },
    {
        "q": "In Logistic Regression, the 'Pseudo R-squared' (like McFadden's) compares the log-likelihood of the fitted model to the log-likelihood of the ______ model.",
        "type": "fill_blank",
        "answers": [
            "null"
        ],
        "other_options": [
            "full",
            "saturated",
            "linear"
        ]
    },
    {
        "q": "If the correlation coefficient between X and Y is -0.5, then the coefficient of determination (R-squared) is -0.25.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What attribute of the Scikit-Learn 'LinearRegression' model allows you to inspect the independent term (bias) of the equation?",
        "c": "from sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\n# Which attribute?",
        "type": "mcq",
        "o": [
            "intercept_",
            "bias_",
            "coef0_",
            "constant_"
        ]
    },
    {
        "q": "Match the residual plot pattern to the regression issue:",
        "type": "match",
        "left": [
            "Points following a curve",
            "Points fanning out (cone)",
            "Points far from the rest (y-axis)",
            "Points far from the rest (x-axis)"
        ],
        "right": [
            "Non-linearity",
            "Heteroscedasticity",
            "Outlier",
            "High Leverage Point"
        ]
    },
    {
        "q": "Using 'Lasso' regression (L1 regularization) typically results in a ______ model because it forces some coefficients to exactly zero.",
        "type": "fill_blank",
        "answers": [
            "sparse"
        ],
        "other_options": [
            "dense",
            "complex",
            "heavy"
        ]
    },
    {
        "q": "Which Scikit-Learn cross-validation strategy is best for time-series regression to prevent 'looking into the future'?",
        "type": "mcq",
        "o": [
            "TimeSeriesSplit",
            "KFold",
            "StratifiedKFold",
            "ShuffleSplit"
        ]
    },
    {
        "q": "Rearrange the components of the Logistic Regression 'Logit' formula:",
        "type": "rearrange",
        "words": [
            "Natural Log of",
            "Probability of Success",
            "Divided By",
            "Probability of Failure"
        ]
    },
    {
        "q": "The 'F-test' in a regression summary is used to determine if the relationship between the dependent variable and the *entire* set of independent variables is statistically significant.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the shape of the 'coef_' attribute in Scikit-Learn for a binary Logistic Regression problem with 10 features?",
        "c": "model = LogisticRegression()\nmodel.fit(X_train, y_train) # X has 10 cols\nprint(model.coef_.shape)",
        "type": "mcq",
        "o": [
            "(1, 10)",
            "(10, 1)",
            "(10,)",
            "(2, 10)"
        ]
    },
    {
        "q": "When adding a new predictor to a model, the 'Residual Sum of Squares' (SSE) will never ______.",
        "type": "fill_blank",
        "answers": [
            "increase"
        ],
        "other_options": [
            "decrease",
            "stabilize",
            "vanish"
        ]
    },
    {
        "q": "In a 'Log-Level' regression model where ln(y) = beta0 + beta1 * x, the coefficient beta1 represents the fractional change in y for a ______ change in x.",
        "type": "fill_blank",
        "answers": [
            "unit"
        ],
        "other_options": [
            "percentage",
            "logarithmic",
            "standard"
        ]
    },
    {
        "q": "Which Scikit-Learn class is specifically designed to handle regression tasks where the target variable is a count (non-negative integers)?",
        "type": "mcq",
        "o": [
            "PoissonRegressor",
            "LogisticRegression",
            "LinearRegression",
            "SGDRegressor"
        ]
    },
    {
        "q": "If the residuals of a regression model are normally distributed, the plot of 'Theoretical Quantiles' vs 'Sample Quantiles' (Q-Q Plot) should follow a ______ line.",
        "type": "fill_blank",
        "answers": [
            "straight"
        ],
        "other_options": [
            "curved",
            "horizontal",
            "vertical"
        ]
    },
    {
        "q": "The 'Mean Squared Error' (MSE) is more sensitive to outliers than the 'Mean Absolute Error' (MAE) because the errors are squared.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the Regression assumption to the diagnostic test used to check it:",
        "type": "match",
        "left": [
            "Normality of Residuals",
            "Homoscedasticity",
            "No Autocorrelation",
            "Linear Specification"
        ],
        "right": [
            "Jarque-Bera Test",
            "Breusch-Pagan Test",
            "Ljung-Box Test",
            "Ramsey RESET Test"
        ]
    },
    {
        "q": "What is the correct interpretation of the intercept in a standardized linear regression (where X and y are Z-scored)?",
        "type": "mcq",
        "o": [
            "It is exactly zero",
            "It is the mean of y",
            "It is the standard deviation of y",
            "It is the correlation coefficient"
        ]
    },
    {
        "q": "Rearrange the steps to perform a 'Leave-One-Out Cross-Validation' (LOOCV):",
        "type": "rearrange",
        "words": [
            "Select one sample as test",
            "Train on n-1 samples",
            "Calculate error",
            "Repeat n times",
            "Average the errors"
        ]
    },
    {
        "q": "In Logistic Regression, if the coefficient for a variable is negative, increasing that variable ______ the probability of the positive class (assuming other variables are held constant).",
        "type": "fill_blank",
        "answers": [
            "decreases"
        ],
        "other_options": [
            "increases",
            "multiplies",
            "squaring"
        ]
    },
    {
        "q": "Adding an irrelevant variable to a linear regression model will decrease the Adjusted R-squared, even if the R-squared increases slightly.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this code snippet regarding the shape of the prediction?",
        "c": "from sklearn.linear_model import LinearRegression\nimport numpy as np\nX = np.random.rand(10, 3)\ny = np.random.rand(10, 2) # Multi-output regression\nmodel = LinearRegression().fit(X, y)\n# Predicting for 1 sample\npred = model.predict([[0.5, 0.5, 0.5]])\nprint(pred.shape)",
        "type": "mcq",
        "o": [
            "(1, 2)",
            "(1, 3)",
            "(2, 1)",
            "(3, 1)"
        ]
    },
    {
        "q": "Match the Scikit-Learn metric to its ideal value (best performance):",
        "type": "match",
        "left": [
            "R-squared",
            "Mean Squared Error",
            "Explained Variance Score",
            "Max Error"
        ],
        "right": [
            "1.0",
            "0.0",
            "1.0",
            "0.0"
        ]
    },
    {
        "q": "The 'Condition Number' of the design matrix X is a diagnostic used to assess the severity of ______.",
        "type": "fill_blank",
        "answers": [
            "multicollinearity"
        ],
        "other_options": [
            "non-linearity",
            "outliers",
            "heteroscedasticity"
        ]
    },
    {
        "q": "Which regression technique fits a separate line (or plane) for different quantiles of the data, allowing for a robust analysis of the median or extremes?",
        "type": "mcq",
        "o": [
            "Quantile Regression",
            "Ridge Regression",
            "Stepwise Regression",
            "Logistic Regression"
        ]
    },
    {
        "q": "Rearrange the components of the 'Total Error' decomposition in the bias-variance tradeoff:",
        "type": "rearrange",
        "words": [
            "Bias Squared",
            "Plus",
            "Variance",
            "Plus",
            "Irreducible Error"
        ]
    },
    {
        "q": "In Python's `statsmodels`, the `summary()` output for Logistic Regression includes 'LLR p-value'. This tests the significance of the ______.",
        "type": "mcq",
        "o": [
            "entire model vs null model",
            "intercept only",
            "last variable added",
            "normality of residuals"
        ]
    },
    {
        "q": "A 'High Leverage' point is an observation with an extreme value in the dependent variable (y), regardless of its x value.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which function allows you to create a pipeline that includes polynomial feature generation followed by linear regression?",
        "c": "from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n# model = make_pipeline(PolynomialFeatures(2), LinearRegression())",
        "type": "mcq",
        "o": [
            "make_pipeline",
            "make_regression",
            "combine_models",
            "stacking_regressor"
        ]
    },
    {
        "q": "Which baseline model strategy predicts the mean of the training target values for all future inputs, serving as a reference point for R-squared calculation?",
        "type": "mcq",
        "o": [
            "Dummy Regressor (Mean strategy)",
            "Random Forest Regressor",
            "Linear Regressor",
            "Support Vector Regressor"
        ]
    },
    {
        "q": "In the context of Time Series regression, if the 'Durbin-Watson' statistic is close to 0, it indicates strong ______ autocorrelation.",
        "type": "fill_blank",
        "answers": [
            "positive"
        ],
        "other_options": [
            "negative",
            "zero",
            "neutral"
        ]
    },
    {
        "q": "What is the primary purpose of the 'link function' in a Generalized Linear Model (GLM)?",
        "type": "mcq",
        "o": [
            "To connect the linear predictor to the mean of the response distribution",
            "To link two different datasets together",
            "To calculate the correlation coefficient",
            "To normalize the residuals"
        ]
    },
    {
        "q": "Match the Regression Link Function to the distribution it is commonly paired with:",
        "type": "match",
        "left": [
            "Identity Link",
            "Logit Link",
            "Log Link",
            "Inverse Link"
        ],
        "right": [
            "Normal (Gaussian)",
            "Binomial (Bernoulli)",
            "Poisson",
            "Gamma"
        ]
    },
    {
        "q": "The 'Regression to the Mean' phenomenon suggests that if a variable is extreme on its first measurement, it will tend to be closer to the ______ on its second measurement.",
        "type": "fill_blank",
        "answers": [
            "average"
        ],
        "other_options": [
            "maximum",
            "minimum",
            "outlier"
        ]
    },
    {
        "q": "What output does this Scikit-Learn code produce?",
        "c": "from sklearn.linear_model import LinearRegression\nmodel = LinearRegression(fit_intercept=True)\n# If X has shape (100, 5) and y has shape (100, 1)\nmodel.fit(X, y)\nprint(model.coef_.shape)",
        "type": "mcq",
        "o": [
            "(1, 5)",
            "(5, 1)",
            "(100, 1)",
            "(5,)"
        ]
    },
    {
        "q": "Rearrange the logical phases of 'K-Fold Cross-Validation':",
        "type": "rearrange",
        "words": [
            "Partition data into K subsets",
            "Hold out one subset",
            "Train on remaining subsets",
            "Evaluate on held-out subset",
            "Average performance"
        ]
    },
    {
        "q": "In a 'Quantile Regression' model predicting the 90th percentile, the model penalizes ______ more heavily than over-predictions.",
        "type": "fill_blank",
        "answers": [
            "under-predictions"
        ],
        "other_options": [
            "over-predictions",
            "intercepts",
            "outliers"
        ]
    },
    {
        "q": "Which plot compares the distribution of the residuals against a theoretical normal distribution using dots and a 45-degree line?",
        "type": "mcq",
        "o": [
            "Q-Q Plot",
            "Scatter Plot",
            "Box Plot",
            "Violin Plot"
        ]
    },
    {
        "q": "If you perform a Linear Regression on a dataset where y is the 'Rank' (1st, 2nd, 3rd...), the assumption of continuous and normally distributed errors is violated.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the Scikit-Learn 'scoring' string to its metric:",
        "type": "match",
        "left": [
            "'neg_mean_squared_error'",
            "'neg_mean_absolute_error'",
            "'r2'",
            "'explained_variance'"
        ],
        "right": [
            "Maximizes toward 0 (from negative)",
            "Maximizes toward 0 (Linear penalty)",
            "Maximizes toward 1.0",
            "Variance explained ignoring mean offset"
        ]
    },
    {
        "q": "Rearrange the components of the 'Mean Absolute Percentage Error' (MAPE) formula:",
        "type": "rearrange",
        "words": [
            "Actual minus Forecast",
            "Divided by Actual",
            "Absolute Value",
            "Sum over n",
            "Divided by n"
        ]
    },
    {
        "q": "Which Scikit-Learn linear model uses the 'Huber' loss function to be robust against outliers?",
        "type": "mcq",
        "o": [
            "HuberRegressor",
            "LinearRegression",
            "Ridge",
            "Lasso"
        ]
    },
    {
        "q": "In Scikit-Learn, the 'GridSearchCV' tool allows you to tune hyperparameters like 'alpha' in Ridge regression by testing all possible combinations.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "If a regression model has a 'High Bias', it implies that the model is ______ the data.",
        "type": "fill_blank",
        "answers": [
            "underfitting"
        ],
        "other_options": [
            "overfitting",
            "optimizing",
            "memorizing"
        ]
    },
    {
        "q": "What is the specific utility of the 'make_column_transformer' function in a regression pipeline?",
        "c": "from sklearn.compose import make_column_transformer",
        "type": "mcq",
        "o": [
            "Applies different preprocessors to different columns",
            "Transforms the target variable only",
            "Creates new interaction features",
            "Performs dimensionality reduction"
        ]
    },
    {
        "q": "Rearrange the interpretation steps for a 'Log-Log' coefficient:",
        "type": "rearrange",
        "words": [
            "1 percent increase in X",
            "leads to",
            "Beta percent",
            "increase in Y"
        ]
    },
    {
        "q": "In Simple Linear Regression, if the Pearson correlation coefficient (r) between X and y is -0.8, what is the value of the Coefficient of Determination (R-squared)?",
        "type": "mcq",
        "o": [
            "0.64",
            "-0.64",
            "0.8",
            "-0.8"
        ]
    },
    {
        "q": "Which Scikit-Learn attribute typically stores the actual number of iterations the solver took to converge (e.g., in SGDRegressor or LogisticRegression)?",
        "c": "model = SGDRegressor(max_iter=1000)\nmodel.fit(X, y)\n# Check actual iterations\nprint(model.______)",
        "type": "mcq",
        "o": [
            "n_iter_",
            "iters_",
            "steps_",
            "count_"
        ]
    },
    {
        "q": "In a 'Log-Log' regression model (ln(y) vs ln(x)), the slope coefficient represents the ______ of y with respect to x (percent change in y for percent change in x).",
        "type": "fill_blank",
        "answers": [
            "elasticity"
        ],
        "other_options": [
            "slope",
            "variance",
            "intercept"
        ]
    },
    {
        "q": "The 'Softmax' function is a generalization of the Sigmoid function used for Multi-Class Logistic Regression.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the Scikit-Learn parameter to its effect on the model:",
        "type": "match",
        "left": [
            "fit_intercept=False",
            "warm_start=True",
            "shuffle=True",
            "random_state=42"
        ],
        "right": [
            "Forces line through origin",
            "Continues training from previous state",
            "Randomizes data order before epoch",
            "Ensures reproducible splits/init"
        ]
    },
    {
        "q": "Which specific assumption states that the variance of the residuals should be constant across all levels of the independent variables?",
        "type": "mcq",
        "o": [
            "Homoscedasticity",
            "Multicollinearity",
            "Normality",
            "Independence"
        ]
    },
    {
        "q": "Rearrange the steps to calculate the 'Standard Error of the Slope':",
        "type": "rearrange",
        "words": [
            "Standard Error of Estimate",
            "Divided By",
            "Square Root of",
            "Sum of Squared X Deviations"
        ]
    },
    {
        "q": "If the p-value of the F-statistic is 0.001, it suggests that ______ of the independent variables is statistically significant.",
        "type": "fill_blank",
        "answers": [
            "at least one"
        ],
        "other_options": [
            "none",
            "all",
            "exactly one"
        ]
    },
    {
        "q": "In Scikit-Learn, the `LinearRegression` class solves the Ordinary Least Squares problem using the singular value decomposition (SVD) of the matrix X.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this code snippet regarding model coefficients?",
        "c": "from sklearn.linear_model import Ridge\nimport numpy as np\nX = np.array([[0, 0], [0, 0], [1, 1]])\ny = np.array([0, .1, 1])\nclf = Ridge(alpha=1.0)\nclf.fit(X, y)\n# How many coefficients are learned?\nprint(len(clf.coef_))",
        "type": "mcq",
        "o": [
            "2",
            "3",
            "1",
            "0"
        ]
    },
    {
        "q": "Match the regression type to its target variable distribution assumption (GLM context):",
        "type": "match",
        "left": [
            "Linear Regression",
            "Logistic Regression",
            "Poisson Regression",
            "Gamma Regression"
        ],
        "right": [
            "Normal (Gaussian)",
            "Bernoulli",
            "Poisson (Counts)",
            "Gamma (Continuous positive)"
        ]
    },
    {
        "q": "A 'Studentized Residual' is a residual divided by an estimate of its standard deviation. If the absolute value is > 3, the point is likely an ______.",
        "type": "fill_blank",
        "answers": [
            "outlier"
        ],
        "other_options": [
            "inlier",
            "error",
            "intercept"
        ]
    },
    {
        "q": "Which plot is used to identify the 'Lag' at which the residuals might be autocorrelated (useful for time series regression)?",
        "type": "mcq",
        "o": [
            "ACF Plot (Autocorrelation Function)",
            "Box Plot",
            "Histogram",
            "Scatter Matrix"
        ]
    },
    {
        "q": "Rearrange the components of the 'Mean Squared Error' formula:",
        "type": "rearrange",
        "words": [
            "1 over n",
            "Times Sum of",
            "Squared Differences",
            "Between Actual and Predicted"
        ]
    },
    {
        "q": "Using 'StratifiedKFold' for regression problems (continuous target) is technically valid and recommended by Scikit-Learn.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which method is used to determine the optimal lambda (penalty) value in Ridge/Lasso regression by testing a range of values?",
        "type": "mcq",
        "o": [
            "Cross-Validation",
            "Gradient Descent",
            "Standardization",
            "Backpropagation"
        ]
    },
    {
        "q": "In a linear model, if two independent variables are perfectly correlated (r=1), the 'Determinant' of the matrix (X-transpose * X) is equal to ______.",
        "type": "fill_blank",
        "answers": [
            "zero"
        ],
        "other_options": [
            "one",
            "infinity",
            "negative"
        ]
    },
    {
        "q": "Which Scikit-Learn method allows you to generate polynomial features (e.g., x^2, x^3) to model non-linear relationships using Linear Regression?",
        "c": "from sklearn.preprocessing import ______\npoly = ______(degree=2)",
        "type": "mcq",
        "o": [
            "PolynomialFeatures",
            "StandardScaler",
            "FunctionTransformer",
            "NonLinearFeatures"
        ]
    },
    {
        "q": "In a 'Log-Log' regression model (ln(y) = beta0 + beta1 * ln(x)), the coefficient beta1 represents the ______ of y with respect to x.",
        "type": "fill_blank",
        "answers": [
            "elasticity"
        ],
        "other_options": [
            "slope",
            "intercept",
            "variance"
        ]
    },
    {
        "q": "If the 'Condition Number' of the design matrix X is very large (e.g., > 30), it is a strong indicator of ______.",
        "type": "fill_blank",
        "answers": [
            "multicollinearity"
        ],
        "other_options": [
            "underfitting",
            "autocorrelation",
            "non-normality"
        ]
    },
    {
        "q": "The 'Gauss-Markov Theorem' states that if the linear regression assumptions are met, the Ordinary Least Squares (OLS) estimator is the Best Linear Unbiased Estimator (BLUE).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the specific Residual Plot pattern to the assumption it violates:",
        "type": "match",
        "left": [
            "Fan/Cone Shape",
            "U-Shape / Curve",
            "Sine Wave / Cycles",
            "Flat horizontal band"
        ],
        "right": [
            "Homoscedasticity (Constant Variance)",
            "Linearity (Need polynomial term)",
            "Independence (Autocorrelation)",
            "No Violation (Ideal)"
        ]
    },
    {
        "q": "In Logistic Regression, the 'Decision Boundary' that separates classes (probability = 0.5) is defined by the equation ______.",
        "type": "mcq",
        "o": [
            "beta0 + beta1*x = 0",
            "beta0 + beta1*x = 1",
            "beta0 + beta1*x = 0.5",
            "beta0 + beta1*x = infinity"
        ]
    },
    {
        "q": "Rearrange the logical steps for performing 'Forward Selection' in variable selection:",
        "type": "rearrange",
        "words": [
            "Start with Null Model",
            "Calculate p-values for all candidates",
            "Add variable with lowest p-value",
            "Refit and Repeat"
        ]
    },
    {
        "q": "Which metric in Scikit-Learn's 'classification_report' represents the ratio of True Positives to the sum of True Positives and False Positives?",
        "type": "mcq",
        "o": [
            "Precision",
            "Recall",
            "F1-Score",
            "Support"
        ]
    },
    {
        "q": "When using 'Ridge' regression, scaling the input variables (Standardization) is optional and does not affect the final model coefficients.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What is the shape of the 'coef_' attribute in a Scikit-Learn LinearRegression model trained with 10 features and 1 target?",
        "c": "model = LinearRegression()\nmodel.fit(X_train, y_train) # X_train has 10 columns\nprint(model.coef_.shape)",
        "type": "mcq",
        "o": [
            "(10,)",
            "(1, 10)",
            "(10, 1)",
            "(1,)"
        ]
    },
    {
        "q": "Match the Regression Evaluation Metric to its primary characteristic:",
        "type": "match",
        "left": [
            "Mean Absolute Error (MAE)",
            "Root Mean Squared Error (RMSE)",
            "R-squared",
            "Adjusted R-squared"
        ],
        "right": [
            "Robust to outliers (linear penalty)",
            "Sensitive to outliers (squared penalty)",
            "Proportion of variance explained",
            "Penalizes adding useless features"
        ]
    },
    {
        "q": "In the context of Goodness of Fit, a 'Saturated Model' is a model that fits the data perfectly because it has as many parameters as ______.",
        "type": "fill_blank",
        "answers": [
            "observations"
        ],
        "other_options": [
            "features",
            "outliers",
            "errors"
        ]
    },
    {
        "q": "Which function allows you to combine a preprocessor (like StandardScaler) and a regressor into a single object to prevent data leakage?",
        "c": "from sklearn.pipeline import ______\nmodel = ______(StandardScaler(), LinearRegression())",
        "type": "mcq",
        "o": [
            "make_pipeline",
            "make_union",
            "ColumnTransformer",
            "StackingRegressor"
        ]
    },
    {
        "q": "Rearrange the components of the 'F-statistic' formula concept:",
        "type": "rearrange",
        "words": [
            "Mean Square Regression",
            "Divided By",
            "Mean Square Error",
            "Equals F-Value"
        ]
    },
    {
        "q": "A 'High Leverage' point is an observation that has an extreme value in the ______ space (predictor variables).",
        "type": "fill_blank",
        "answers": [
            "feature"
        ],
        "other_options": [
            "target",
            "error",
            "time"
        ]
    },
    {
        "q": "Regularization (Lasso/Ridge) introduces 'Bias' into the model in exchange for a reduction in 'Variance', often leading to lower total error.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the specific utility of the 'class_weight=\"balanced\"' parameter in Logistic Regression?",
        "type": "mcq",
        "o": [
            "Adjusts weights inversely proportional to class frequencies",
            "Balances the number of features",
            "Normalizes the input data",
            "Uses balanced accuracy score optimization"
        ]
    },
    {
        "q": "The Gauss-Markov theorem states that the Ordinary Least Squares (OLS) estimator is the Best Linear Unbiased Estimator (BLUE) provided that the errors have expectation zero and are ______.",
        "type": "fill_blank",
        "answers": [
            "uncorrelated"
        ],
        "other_options": [
            "normal",
            "infinite",
            "biased"
        ]
    },
    {
        "q": "What will likely happen if you attempt to predict using a Linear Regression model trained on 5 features, but pass input data with only 4 features?",
        "c": "from sklearn.linear_model import LinearRegression\n# Train on 5 features\nmodel.fit(X_train_5_cols, y)\n# Predict on 4 features\nmodel.predict(X_new_4_cols)",
        "type": "mcq",
        "o": [
            "ValueError (Dimension mismatch)",
            "It predicts normally",
            "It assumes 0 for the missing feature",
            "It predicts NaN"
        ]
    },
    {
        "q": "In the context of the Bias-Variance tradeoff, as you increase the complexity of a model (e.g., higher degree polynomial), the Variance typically ______.",
        "type": "fill_blank",
        "answers": [
            "increases"
        ],
        "other_options": [
            "decreases",
            "stays constant",
            "becomes zero"
        ]
    },
    {
        "q": "The Adjusted R-squared value can decrease if you add a new predictor variable that does not improve the model fit enough to justify the loss of a degree of freedom.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the specific Hypothesis Test to what it validates in a Regression context:",
        "type": "match",
        "left": [
            "Jarque-Bera Test",
            "Breusch-Pagan Test",
            "Durbin-Watson Test",
            "Variance Inflation Factor"
        ],
        "right": [
            "Normality of Residuals",
            "Homoscedasticity of Residuals",
            "Independence of Residuals",
            "Multicollinearity of Features"
        ]
    },
    {
        "q": "In Logistic Regression, the 'logit' function is the inverse of the ______ function.",
        "type": "mcq",
        "o": [
            "sigmoid",
            "linear",
            "tangent",
            "relu"
        ]
    },
    {
        "q": "Rearrange the logical steps to calculate the F-statistic for model significance:",
        "type": "rearrange",
        "words": [
            "Mean Square Regression",
            "Divided By",
            "Mean Square Error",
            "Equals F-Value"
        ]
    },
    {
        "q": "If a regression model excludes a significant variable that is correlated with the included variables, the estimated coefficients will suffer from ______ Bias.",
        "type": "fill_blank",
        "answers": [
            "Omitted Variable"
        ],
        "other_options": [
            "Selection",
            "Survival",
            "Confirmation"
        ]
    },
    {
        "q": "Which Scikit-Learn parameter should be set to 'False' if your data is already centered or if you want to force the regression line through the origin (0,0)?",
        "c": "model = LinearRegression(______=False)",
        "type": "mcq",
        "o": [
            "fit_intercept",
            "normalize",
            "copy_X",
            "positive"
        ]
    },
    {
        "q": "In a 'Log-Level' regression (log(y) on x), a coefficient of 0.05 implies that a one-unit increase in x is associated with roughly a 5% increase in y.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the goodness-of-fit metric to its formula concept:",
        "type": "match",
        "left": [
            "AIC",
            "BIC",
            "RMSE",
            "MAE"
        ],
        "right": [
            "Penalizes 2*k (parameters)",
            "Penalizes k*ln(n) (sample size)",
            "Square root of average squared error",
            "Average of absolute errors"
        ]
    },
    {
        "q": "What is the specific range of the Durbin-Watson statistic, where a value of 2 indicates no autocorrelation?",
        "type": "mcq",
        "o": [
            "0 to 4",
            "-1 to 1",
            "0 to 1",
            "-inf to +inf"
        ]
    },
    {
        "q": "Rearrange the components of the 'Standard Error of the Regression' (S) formula:",
        "type": "rearrange",
        "words": [
            "Square Root of",
            "Sum of Squared Errors",
            "Divided By",
            "Degrees of Freedom (n-p-1)"
        ]
    },
    {
        "q": "If an observation has a 'Cook's Distance' greater than 1 (or 4/n), it is generally considered to be an ______ point.",
        "type": "fill_blank",
        "answers": [
            "influential"
        ],
        "other_options": [
            "outlier",
            "average",
            "irrelevant"
        ]
    },
    {
        "q": "In Scikit-Learn, which attribute of the fitted model object would you access to see the independent term (bias) of the linear equation?",
        "c": "from sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, y)\n# print(reg.______)",
        "type": "mcq",
        "o": [
            "intercept_",
            "bias_",
            "coef0_",
            "offset_"
        ]
    },
    {
        "q": "An interaction term (e.g., X1 * X2) allows the slope of X1 to change depending on the value of X2.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "If you have a categorical variable with 4 levels (A, B, C, D) and you use One-Hot Encoding with 'drop_first=True', how many new binary columns are created?",
        "type": "mcq",
        "o": [
            "3",
            "4",
            "1",
            "2"
        ]
    },
    {
        "q": "Which method in Scikit-Learn is used to standardize features by removing the mean and scaling to unit variance?",
        "c": "from sklearn.preprocessing import ______\nscaler = ______()",
        "type": "mcq",
        "o": [
            "StandardScaler",
            "MinMaxScaler",
            "RobustScaler",
            "Normalizer"
        ]
    },
    {
        "q": "In a Linear Regression model, the degrees of freedom associated with the Residual Sum of Squares (SSE) is n - p - 1, where 'n' is observations and 'p' is ______.",
        "type": "fill_blank",
        "answers": [
            "predictors"
        ],
        "other_options": [
            "parameters",
            "classes",
            "layers"
        ]
    },
    {
        "q": "The 'Null Hypothesis' for the t-test of a regression coefficient states that the coefficient is equal to one.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the Regression diagnostic tool to what it helps identify:",
        "type": "match",
        "left": [
            "Variance Inflation Factor (VIF)",
            "Durbin-Watson",
            "Cook's Distance",
            "Q-Q Plot"
        ],
        "right": [
            "Multicollinearity",
            "Autocorrelation",
            "Influential Observations",
            "Non-normality of residuals"
        ]
    },
    {
        "q": "What is the primary reason for adding a constant (intercept) column manually when using 'statsmodels.api' for OLS, unlike Scikit-Learn?",
        "c": "import statsmodels.api as sm\nX = sm.add_constant(X)",
        "type": "mcq",
        "o": [
            "Statsmodels does not add an intercept by default",
            "To normalize the data",
            "To remove collinearity",
            "To speed up computation"
        ]
    },
    {
        "q": "Rearrange the logical steps for evaluating a model using K-Fold Cross-Validation:",
        "type": "rearrange",
        "words": [
            "Split data into K folds",
            "Iterate through folds",
            "Train on K-1 folds",
            "Validate on remaining fold",
            "Average the scores"
        ]
    },
    {
        "q": "In Logistic Regression, the 'Odds' of an event occurring is defined as the probability of the event divided by the probability of the ______.",
        "type": "fill_blank",
        "answers": [
            "non-event"
        ],
        "other_options": [
            "total",
            "population",
            "sample"
        ]
    },
    {
        "q": "When a model performs well on training data but poorly on unseen test data, it is a classic sign of ______.",
        "type": "mcq",
        "o": [
            "Overfitting",
            "Underfitting",
            "Bias",
            "Convergence"
        ]
    },
    {
        "q": "Ridge Regression (L2 regularization) can zero out coefficients completely, effectively performing feature selection.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What does the 'score()' method return for a 'LinearRegression' model in Scikit-Learn?",
        "c": "model = LinearRegression().fit(X, y)\nprint(model.score(X_test, y_test))",
        "type": "mcq",
        "o": [
            "R-squared value",
            "Mean Squared Error",
            "Root Mean Squared Error",
            "Accuracy"
        ]
    },
    {
        "q": "Match the Scikit-Learn estimator to its optimization approach:",
        "type": "match",
        "left": [
            "LinearRegression",
            "SGDRegressor",
            "Ridge",
            "Lasso"
        ],
        "right": [
            "Ordinary Least Squares (Closed Form)",
            "Stochastic Gradient Descent",
            "OLS with L2 Penalty",
            "OLS with L1 Penalty"
        ]
    },
    {
        "q": "If the residuals of a regression model show a 'funnel' shape (widening as fitted values increase), the assumption of ______ is violated.",
        "type": "fill_blank",
        "answers": [
            "homoscedasticity"
        ],
        "other_options": [
            "independence",
            "linearity",
            "normality"
        ]
    },
    {
        "q": "Rearrange the components of the Simple Linear Regression equation:",
        "type": "rearrange",
        "words": [
            "Response Variable (y)",
            "Equals",
            "Intercept (beta-0)",
            "Plus",
            "Slope (beta-1) times x"
        ]
    },
    {
        "q": "Which metric is generally preferred when the target variable has outliers that you do not want to heavily penalize?",
        "type": "mcq",
        "o": [
            "Mean Absolute Error (MAE)",
            "Mean Squared Error (MSE)",
            "Root Mean Squared Error (RMSE)",
            "R-squared"
        ]
    },
    {
        "q": "In a 'Log-Log' model (log y vs log x), the slope coefficient is interpreted as the ______ of y with respect to x.",
        "type": "fill_blank",
        "answers": [
            "elasticity"
        ],
        "other_options": [
            "velocity",
            "correlation",
            "variance"
        ]
    },
    {
        "q": "The 'predict_proba()' method in Logistic Regression returns the class labels (0 or 1).",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which Scikit-Learn tool is used to generate interaction terms (e.g., x1 * x2) for a linear model?",
        "c": "from sklearn.preprocessing import ______",
        "type": "mcq",
        "o": [
            "PolynomialFeatures",
            "InteractionEncoder",
            "FeatureUnion",
            "OneHotEncoder"
        ]
    },
    {
        "q": "Which method in Scikit-Learn's SGDRegressor allows for 'online learning', meaning the model can be updated with new batches of data without retraining from scratch?",
        "c": "from sklearn.linear_model import SGDRegressor\nmodel = SGDRegressor()\n# Which method updates weights incrementally?\nmodel.______(X_batch, y_batch)",
        "type": "mcq",
        "o": [
            "partial_fit",
            "update",
            "online_fit",
            "incremental_fit"
        ]
    },
    {
        "q": "In a Generalized Linear Model (GLM) with a Poisson distribution, the variance of the response variable is assumed to be equal to its ______.",
        "type": "fill_blank",
        "answers": [
            "mean"
        ],
        "other_options": [
            "median",
            "square",
            "standard deviation"
        ]
    },
    {
        "q": "Match the Logistic Regression 'solver' to its specific constraint or capability:",
        "type": "match",
        "left": [
            "liblinear",
            "lbfgs",
            "newton-cg",
            "sag"
        ],
        "right": [
            "Supports L1 penalty (good for small data)",
            "Default solver, supports Multinomial loss",
            "Uses Hessian matrix (computationally expensive)",
            "Stochastic Average Gradient (fast for large N)"
        ]
    },
    {
        "q": "If you center your data (subtract mean from X and y) before running Simple Linear Regression, the resulting intercept (beta-0) will always be exactly zero.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the order of operations in a typical Regression Pipeline:",
        "type": "rearrange",
        "words": [
            "Impute Missing Values",
            "Scale Features",
            "Generate Polynomial Features",
            "Fit Regressor"
        ]
    },
    {
        "q": "What is the shape of the 'powers_' attribute in a PolynomialFeatures transformer with 2 input features and degree=2?",
        "c": "from sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=2)\npoly.fit(X) # X has 2 columns\nprint(poly.powers_.shape)\n# Features produced: 1, a, b, a^2, ab, b^2",
        "type": "mcq",
        "o": [
            "(6, 2)",
            "(2, 6)",
            "(6,)",
            "(5, 2)"
        ]
    },
    {
        "q": "In the context of 'Elastic Net' regularization, the parameter 'l1_ratio' controls the mix. If 'l1_ratio' is 0, the penalty is purely ______.",
        "type": "fill_blank",
        "answers": [
            "L2"
        ],
        "other_options": [
            "L1",
            "None",
            "Infinite"
        ]
    },
    {
        "q": "Which function allows for 'Quantile Regression' in Python's statsmodels, which is robust to outliers in the y-direction?",
        "c": "import statsmodels.formula.api as smf\nmod = smf.______(formula, data)",
        "type": "mcq",
        "o": [
            "quantreg",
            "ols",
            "glm",
            "rlm"
        ]
    },
    {
        "q": "Simpson's Paradox occurs in regression when a trend present in different groups reverses when the groups are combined.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the metric to its calculation basis:",
        "type": "match",
        "left": [
            "Log Loss",
            "Brier Score",
            "ROC AUC",
            "Cohen's Kappa"
        ],
        "right": [
            "Based on uncertainty (entropy)",
            "Mean Squared Error of probabilities",
            "Rank ordering of probabilities",
            "Agreement corrected for chance"
        ]
    },
    {
        "q": "In a 'Partial Least Squares' (PLS) regression, the new features (components) are constructed to maximize the covariance between X and ______.",
        "type": "fill_blank",
        "answers": [
            "y"
        ],
        "other_options": [
            "X",
            "residuals",
            "errors"
        ]
    },
    {
        "q": "Rearrange the steps to calculate 'Cook's Distance' for observation i:",
        "type": "rearrange",
        "words": [
            "Remove observation i",
            "Refit model",
            "Sum squared differences in predictions",
            "Normalize by MSE and p"
        ]
    },
    {
        "q": "What does the 'multioutput' parameter in 'r2_score' default to, which averages the scores of multiple targets uniformly?",
        "type": "mcq",
        "o": [
            "uniform_average",
            "variance_weighted",
            "raw_values",
            "sum_average"
        ]
    },
    {
        "q": "The 'Softmax' function is used in Multinomial Logistic Regression to ensure that the predicted probabilities across all classes sum to ______.",
        "type": "fill_blank",
        "answers": [
            "1"
        ],
        "other_options": [
            "0",
            "infinity",
            "100"
        ]
    },
    {
        "q": "If the 'kurtosis' of the residuals is much less than 3 (Platykurtic), it implies the residual distribution has lighter tails (fewer outliers) than a normal distribution.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which Scikit-Learn attribute allows you to verify the names of the features seen during training (if X was a DataFrame)?",
        "c": "model.fit(df, y)\nprint(model.______)",
        "type": "mcq",
        "o": [
            "feature_names_in_",
            "columns_",
            "feature_names_",
            "input_names_"
        ]
    },
    {
        "q": "In a Regression Pipeline, why must the scaler be fit only on the training set and not the entire dataset?",
        "type": "mcq",
        "o": [
            "To prevent data leakage",
            "To save computation time",
            "Because the test set is always empty",
            "It does not matter, both are valid"
        ]
    },
    {
        "q": "Which Scikit-Learn estimator efficiently performs Ridge Regression with built-in Cross-Validation to select the optimal alpha?",
        "type": "mcq",
        "o": [
            "RidgeCV",
            "GridSearchCV",
            "LinearRegressionCV",
            "CrossValRidge"
        ]
    },
    {
        "q": "Regularization methods like Ridge and Lasso are also known as ______ methods because they reduce the magnitude of the coefficients.",
        "type": "fill_blank",
        "answers": [
            "shrinkage"
        ],
        "other_options": [
            "expansion",
            "inflation",
            "boosting"
        ]
    },
    {
        "q": "When using 'OrdinalEncoder' for a categorical feature in Linear Regression, the model incorrectly assumes that the categories have a mathematical ______.",
        "type": "fill_blank",
        "answers": [
            "order"
        ],
        "other_options": [
            "distribution",
            "variance",
            "name"
        ]
    },
    {
        "q": "Match the Regression Solver to its primary characteristic:",
        "type": "match",
        "left": [
            "Coordinate Descent",
            "L-BFGS",
            "Newton-CG",
            "SVD (Singular Value Decomposition)"
        ],
        "right": [
            "Used for Lasso/ElasticNet",
            "Approximates Hessian (Memory efficient)",
            "Uses full Hessian (Expensive)",
            "Used for Ordinary Least Squares"
        ]
    },
    {
        "q": "Two variables that are statistically correlated but have no causal link, often due to a lurking third variable or time trends, are said to have a ______ correlation.",
        "type": "fill_blank",
        "answers": [
            "spurious"
        ],
        "other_options": [
            "perfect",
            "negative",
            "linear"
        ]
    },
    {
        "q": "Rearrange the order of data flow in a correct Scikit-Learn prediction pipeline:",
        "type": "rearrange",
        "words": [
            "Receive Raw Data",
            "Apply Preprocessing Transforms",
            "Pass to Trained Model",
            "Output Prediction"
        ]
    },
    {
        "q": "Which Scikit-Learn function is specifically useful for calculating regression metrics when the target is strictly positive (like prices) and you care about relative error?",
        "type": "mcq",
        "o": [
            "mean_absolute_percentage_error",
            "mean_squared_error",
            "max_error",
            "accuracy_score"
        ]
    },
    {
        "q": "If a feature has zero variance (constant value across all samples), Scikit-Learn's LinearRegression will treat it as having no predictive power.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does the 'degree' parameter control in this preprocessing step?",
        "c": "from sklearn.preprocessing import PolynomialFeatures\ntrans = PolynomialFeatures(degree=3)",
        "type": "mcq",
        "o": [
            "The maximum power of the features",
            "The number of features to select",
            "The number of classes",
            "The level of regularization"
        ]
    },
    {
        "q": "Match the technique to the problem it addresses:",
        "type": "match",
        "left": [
            "Drop one dummy column",
            "Log-transform Target",
            "Standardize Features",
            "Add Polynomial Terms"
        ],
        "right": [
            "Dummy Variable Trap",
            "Heteroscedasticity / Skew",
            "Scale sensitivity in SGD/Ridge",
            "Underfitting / Non-linearity"
        ]
    },
    {
        "q": "The 'explained_variance_score' explains the dispersion of errors. If the score is 1.0, it means the errors have a variance of ______.",
        "type": "fill_blank",
        "answers": [
            "zero"
        ],
        "other_options": [
            "one",
            "infinity",
            "constant"
        ]
    },
    {
        "q": "Rearrange the components of the 'Elastic Net' Loss function:",
        "type": "rearrange",
        "words": [
            "MSE",
            "Plus",
            "Alpha times L1 Ratio times L1 Penalty",
            "Plus",
            "Alpha times (1 - L1 Ratio) times L2 Penalty"
        ]
    },
    {
        "q": "In Scikit-Learn, which regressor implements the 'LARS' (Least Angle Regression) algorithm, known for being efficient with high-dimensional data?",
        "type": "mcq",
        "o": [
            "Lars",
            "SGDRegressor",
            "Ridge",
            "BayesianRidge"
        ]
    },
    {
        "q": "Making predictions for x-values that lie outside the range of the training data is known as ______.",
        "type": "fill_blank",
        "answers": [
            "extrapolation"
        ],
        "other_options": [
            "interpolation",
            "imputation",
            "estimation"
        ]
    },
    {
        "q": "A 'Negative' R-squared on the test set implies that the trained model performs worse than a horizontal line predicting the mean of the test data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the result of using 'interaction_only=True' in PolynomialFeatures?",
        "c": "poly = PolynomialFeatures(degree=2, interaction_only=True)",
        "type": "mcq",
        "o": [
            "Produces x1*x2 but excludes x1^2 and x2^2",
            "Produces only x1^2 and x2^2",
            "Produces all terms up to degree 2",
            "Produces no new features"
        ]
    },
    {
        "q": "Which of these is NOT an assumption of linear regression?",
        "type": "mcq",
        "o": [
            "Homoscedasticity of residuals",
            "Multicollinearity among predictors",
            "Normality of residuals",
            "Independence of observations"
        ]
    },
    {
        "q": "In logistic regression, the ______ function transforms linear combinations into probabilities between 0 and 1.",
        "type": "fill_blank",
        "answers": ["logistic"],
        "other_options": ["exponential", "linear", "polynomial"]
    },
    {
        "q": "Match the regression diagnostic with its purpose:",
        "type": "match",
        "left": ["Q-Q Plot", "Residuals vs Fitted", "Variance Inflation Factor", "Cook's Distance"],
        "right": ["Check normality of residuals", "Detect heteroscedasticity", "Measure multicollinearity", "Identify influential points"]
    },
    {
        "q": "R-squared measures the proportion of ______ in the dependent variable explained by the model.",
        "type": "fill_blank",
        "answers": ["variance"],
        "other_options": ["mean", "standard deviation", "range"]
    },
    {
        "q": "Logistic regression is used for classification problems while linear regression is used for continuous outcomes.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the steps for building a regression model in correct order:",
        "type": "rearrange",
        "words": ["Data Preparation", "Exploratory Analysis", "Model Fitting", "Residual Analysis", "Model Validation"]
    },
    {
        "q": "What is the primary purpose of residual analysis in regression?",
        "type": "mcq",
        "o": [
            "Check model assumptions",
            "Increase R-squared value",
            "Reduce number of predictors",
            "Transform the response variable"
        ]
    },
    {
        "q": "In logistic regression, the output represents the ______ of the event occurring.",
        "type": "fill_blank",
        "answers": ["probability"],
        "other_options": ["certainty", "frequency", "magnitude"]
    },
    {
        "q": "Which goodness-of-fit metric is preferred for comparing logistic regression models with different numbers of predictors?",
        "type": "mcq",
        "o": [
            "AIC",
            "R-squared",
            "MSE",
            "Correlation coefficient"
        ]
    },
    {
        "q": "The condition where predictor variables are highly correlated is called ______.",
        "type": "fill_blank",
        "answers": ["multicollinearity"],
        "other_options": ["heteroscedasticity", "autocorrelation", "endogeneity"]
    },
    {
        "q": "Adjusted R-squared penalizes models for adding irrelevant predictors.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the regression concept with its description:",
        "type": "match",
        "left": ["Homoscedasticity", "Leverage", "Deviance", "Odds Ratio"],
        "right": ["Constant variance of residuals", "Influence of individual data points", "Goodness of fit measure in logistic regression", "Effect size in logistic regression"]
    },
    {
        "q": "What does this code output indicate about the regression model?\n\nimport statsmodels.api as sm\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n# In the output:\n# Durbin-Watson: 1.2",
        "type": "mcq",
        "c": "import statsmodels.api as sm\nmodel = sm.OLS(y, X).fit()\nprint(model.summary())\n\n# In the output:\n# Durbin-Watson: 1.2",
        "o": [
            "Positive autocorrelation in residuals",
            "Perfect independence of residuals",
            "Heteroscedasticity present",
            "Normal distribution of residuals"
        ]
    },
    {
        "q": "Rearrange the process for evaluating regression model assumptions:",
        "type": "rearrange",
        "words": ["Check linearity", "Test independence", "Verify homoscedasticity", "Assess normality", "Identify outliers"]
    },
    {
        "q": "In linear regression, the ______ assumption states that the relationship between predictors and response is linear.",
        "type": "fill_blank",
        "answers": ["linearity"],
        "other_options": ["normality", "independence", "homogeneity"]
    },
    {
        "q": "Which diagnostic plot is most useful for detecting heteroscedasticity?",
        "type": "mcq",
        "o": [
            "Residuals vs Fitted values",
            "Q-Q plot",
            "Scale-Location plot",
            "Leverage vs Residuals"
        ]
    },
    {
        "q": "The Akaike Information Criterion (AIC) balances model fit with ______.",
        "type": "fill_blank",
        "answers": ["complexity"],
        "other_options": ["interpretability", "speed", "accuracy"]
    },
    {
        "q": "Logistic regression can handle both binary and multiclass classification problems.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does a VIF (Variance Inflation Factor) value greater than 10 typically indicate?",
        "type": "mcq",
        "o": [
            "Severe multicollinearity",
            "Perfect model fit",
            "Heteroscedasticity",
            "Non-normal residuals"
        ]
    },
    {
        "q": "Match the regression metric with its interpretation:",
        "type": "match",
        "left": ["MSE", "R-squared", "p-value", "Confidence Interval"],
        "right": ["Average squared prediction error", "Proportion of variance explained", "Statistical significance of coefficient", "Range of plausible values for parameter"]
    },
    {
        "q": "In a linear regression output, you see: Coefficients: Intercept=2.5, X1=1.8 (p=0.03), X2=0.4 (p=0.62). Which predictor is statistically significant at alpha=0.05?",
        "type": "mcq",
        "o": [
            "X1 only",
            "X2 only",
            "Both X1 and X2",
            "Neither X1 nor X2"
        ]
    },
    {
        "q": "The ______ plot shows if residuals have constant variance across predicted values, testing the homoscedasticity assumption.",
        "type": "fill_blank",
        "answers": ["scale-location"],
        "other_options": ["residuals", "Q-Q", "leverage"]
    },
    {
        "q": "Match the logistic regression output term with its meaning:",
        "type": "match",
        "left": ["Coefficient", "Odds Ratio", "Log-Likelihood", "Pseudo R-squared"],
        "right": ["Change in log-odds per unit X", "Multiplicative effect on odds", "Measure of model fit", "Variance explained analog"]
    },
    {
        "q": "When residuals show a clear U-shaped pattern in residuals vs fitted plot, this indicates violation of the ______ assumption.",
        "type": "fill_blank",
        "answers": ["linearity"],
        "other_options": ["normality", "independence", "homoscedasticity"]
    },
    {
        "q": "Adjusted R-squared can decrease when adding new variables that don't improve model fit sufficiently.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the hierarchy of regression model complexity from simplest to most complex:",
        "type": "rearrange",
        "words": ["Simple Linear", "Multiple Linear", "Polynomial", "Regularized"]
    },
    {
        "q": "What does a Q-Q plot that curves away from the diagonal line at both ends indicate about residuals?",
        "type": "mcq",
        "o": [
            "Heavy-tailed distribution",
            "Light-tailed distribution", 
            "Perfect normality",
            "Skewed distribution"
        ]
    },
    {
        "q": "In logistic regression, the logit function is defined as log(______ / (1 - ______))",
        "type": "fill_blank",
        "answers": ["p"],
        "other_options": ["x", "y", "beta"]
    },
    {
        "q": "Which metric is NOT appropriate for evaluating logistic regression models?",
        "type": "mcq",
        "o": [
            "R-squared",
            "AUC-ROC",
            "Confusion Matrix",
            "Log-Loss"
        ]
    },
    {
        "q": "The Durbin-Watson test statistic ranges from 0 to 4, where values near ______ indicate no autocorrelation.",
        "type": "fill_blank",
        "answers": ["2"],
        "other_options": ["0", "1", "4"]
    },
    {
        "q": "Cook's distance combines leverage and residual magnitude to identify influential observations.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the regression problem with its diagnostic tool:",
        "type": "match",
        "left": ["Non-constant variance", "Non-linearity", "Influential points", "Multicollinearity"],
        "right": ["Breusch-Pagan test", "Partial residual plot", "Cook's distance", "Variance Inflation Factor"]
    },
    {
        "q": "What does this Python code calculate for a logistic regression model?\n\nfrom sklearn.metrics import log_loss\nlog_loss(y_true, y_pred)",
        "type": "mcq",
        "c": "from sklearn.metrics import log_loss\nlog_loss(y_true, y_pred)",
        "o": [
            "Cross-entropy loss",
            "Accuracy score",
            "R-squared value",
            "Mean squared error"
        ]
    },
    {
        "q": "Rearrange the steps for conducting residual analysis:",
        "type": "rearrange",
        "words": ["Plot residuals", "Check patterns", "Test assumptions", "Identify issues", "Apply fixes"]
    },
    {
        "q": "The ______ R-squared is used for logistic regression as an analog to linear regression's R-squared.",
        "type": "fill_blank",
        "answers": ["pseudo"],
        "other_options": ["adjusted", "multiple", "logarithmic"]
    },
    {
        "q": "In linear regression, if the p-value for the F-statistic is less than alpha, what does this indicate?",
        "type": "mcq",
        "o": [
            "At least one predictor is significant",
            "All predictors are significant",
            "The model has perfect fit",
            "Residuals are normally distributed"
        ]
    },
    {
        "q": "The formula y = b0 + b1*x1 + b2*x2 + ... + bn*xn represents ______ regression.",
        "type": "fill_blank",
        "answers": ["linear"],
        "other_options": ["logistic", "polynomial", "ridge"]
    },
    {
        "q": "Hosmer-Lemeshow test is used to assess goodness of fit for linear regression models.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What does a leverage value greater than 2*(k+1)/n indicate about an observation?",
        "type": "mcq",
        "o": [
            "High leverage point",
            "Perfect predictor",
            "Normal observation",
            "Missing value"
        ]
    },
    {
        "q": "Match the statistical test with its regression application:",
        "type": "match",
        "left": ["Breusch-Pagan", "Shapiro-Wilk", "Durbin-Watson", "VIF threshold"],
        "right": ["Heteroscedasticity", "Normality", "Autocorrelation", "Multicollinearity"]
    },
    {
        "q": "A researcher finds that adding a new variable increases R-squared from 0.75 to 0.76 but decreases Adjusted R-squared from 0.74 to 0.73. What should they conclude?",
        "type": "mcq",
        "o": [
            "The new variable doesn't improve model enough to justify complexity",
            "The new variable significantly improves model fit",
            "The model was overfitting before adding the variable",
            "R-squared is more reliable than Adjusted R-squared"
        ]
    },
    {
        "q": "The ______ statistic measures how many standard deviations a coefficient is from zero in logistic regression output.",
        "type": "fill_blank",
        "answers": ["z"],
        "other_options": ["t", "F", "chi-square"]
    },
    {
        "q": "Match the residual pattern with its likely cause:",
        "type": "match",
        "left": ["Fan-shaped pattern", "Curved pattern", "Random scatter", "Outliers present"],
        "right": ["Heteroscedasticity", "Non-linear relationship", "Good model fit", "Influential points"]
    },
    {
        "q": "In logistic regression, a coefficient of 0.5 means the odds of success multiply by exp(______) for each unit increase in X.",
        "type": "fill_blank",
        "answers": ["0.5"],
        "other_options": ["1.65", "0.25", "2.0"]
    },
    {
        "q": "The AIC penalty term increases with the number of parameters in the model.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these model evaluation metrics from most sensitive to outliers to least sensitive:",
        "type": "rearrange",
        "words": ["R-squared", "MAE", "RMSE", "Adjusted R-squared"]
    },
    {
        "q": "What does a leverage plot showing points far from the horizontal line at zero indicate?",
        "type": "mcq",
        "o": [
            "High influence observations",
            "Perfect multicollinearity",
            "Homoscedastic residuals",
            "Normal distribution"
        ]
    },
    {
        "q": "The ______ plot displays residuals against each predictor to check for non-linear patterns.",
        "type": "fill_blank",
        "answers": ["partial residual"],
        "other_options": ["Q-Q", "scale-location", "leverage"]
    },
    {
        "q": "Which condition would make logistic regression preferred over linear regression for a binary outcome?",
        "type": "mcq",
        "o": [
            "When predictions must stay between 0 and 1",
            "When you have more than 10 predictors",
            "When the data is normally distributed",
            "When R-squared is high"
        ]
    },
    {
        "q": "A VIF value of 15 for a predictor suggests serious ______ problems.",
        "type": "fill_blank",
        "answers": ["multicollinearity"],
        "other_options": ["heteroscedasticity", "autocorrelation", "non-linearity"]
    },
    {
        "q": "The deviance residual in logistic regression follows approximately normal distribution when the model fits well.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the diagnostic measure with what it detects:",
        "type": "match",
        "left": ["Cook's D > 1", "VIF > 10", "DW  0", "BP p-value < 0.05"],
        "right": ["Highly influential point", "Severe multicollinearity", "Positive autocorrelation", "Heteroscedasticity present"]
    },
    {
        "q": "What does this regression output interpretation indicate? 'For each year of education, the odds of employment increase by 20%'",
        "type": "mcq",
        "o": [
            "Logistic regression with odds ratio = 1.2",
            "Linear regression with coefficient = 0.2",
            "Poisson regression with rate ratio = 1.2",
            "Probit regression with z-score = 1.2"
        ]
    },
    {
        "q": "Rearrange the process for checking regression assumptions in logical order:",
        "type": "rearrange",
        "words": ["Fit initial model", "Examine residuals", "Check influence points", "Verify assumptions", "Refine model"]
    },
    {
        "q": "The ______ test compares nested logistic regression models using the difference in -2 log likelihood.",
        "type": "fill_blank",
        "answers": ["likelihood ratio"],
        "other_options": ["F-test", "t-test", "Wald test"]
    },
    {
        "q": "What does a scale-location plot with a horizontal line and random points indicate?",
        "type": "mcq",
        "o": [
            "Constant variance of residuals",
            "Non-linear relationship",
            "Autocorrelated errors",
            "Poor model fit"
        ]
    },
    {
        "q": "In linear regression, the ______ assumption requires that errors have same variance across all X values.",
        "type": "fill_blank",
        "answers": ["homoscedasticity"],
        "other_options": ["normality", "independence", "linearity"]
    },
    {
        "q": "McFadden's pseudo R-squared values above 0.4 indicate excellent model fit in logistic regression.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which scenario would cause the Breusch-Pagan test to reject the null hypothesis?",
        "type": "mcq",
        "o": [
            "When residual variance changes with predictors",
            "When residuals are perfectly normal",
            "When all coefficients are significant",
            "When R-squared is very high"
        ]
    },
    {
        "q": "Match the goodness-of-fit statistic with its regression type:",
        "type": "match",
        "left": ["R-squared", "AUC", "AIC", "Mean Deviance"],
        "right": ["Linear regression", "Logistic regression", "Model comparison", "Poisson regression"]
    },
    {
        "q": "When interpreting a logistic regression coefficient of -0.8 for variable 'smoker', what does this mean for the outcome probability?",
        "type": "mcq",
        "o": [
            "Smoking decreases the log-odds of the outcome",
            "Smoking increases the probability by 80%",
            "Smoking has no significant effect",
            "The model has perfect separation"
        ]
    },
    {
        "q": "The ______ plot helps identify outliers by showing standardized residuals against leverage values.",
        "type": "fill_blank",
        "answers": ["residuals vs leverage"],
        "other_options": ["Q-Q", "fitted values", "partial regression"]
    },
    {
        "q": "Match the regression assumption violation with its most appropriate corrective action:",
        "type": "match",
        "left": ["Non-linearity", "Heteroscedasticity", "Non-normality", "Autocorrelation"],
        "right": ["Add polynomial terms", "Use weighted least squares", "Transform response variable", "Add lag variables"]
    },
    {
        "q": "A logistic regression model predicting customer churn shows AUC = 0.92, which indicates ______ discriminatory power.",
        "type": "fill_blank",
        "answers": ["excellent"],
        "other_options": ["poor", "moderate", "random"]
    },
    {
        "q": "The condition index is an alternative to VIF for detecting multicollinearity problems.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these model building steps in proper sequence:",
        "type": "rearrange",
        "words": ["Univariate analysis", "Bivariate analysis", "Multicollinearity check", "Model fitting", "Diagnostic checking"]
    },
    {
        "q": "What does a studentized residual value greater than 3 indicate about an observation?",
        "type": "mcq",
        "o": [
            "It is a potential outlier",
            "It has high leverage",
            "The model fits perfectly",
            "Multicollinearity is present"
        ]
    },
    {
        "q": "The ______ test assesses whether a logistic regression model fits the data by comparing observed and expected frequencies.",
        "type": "fill_blank",
        "answers": ["Hosmer-Lemeshow"],
        "other_options": ["Breusch-Pagan", "Durbin-Watson", "Shapiro-Wilk"]
    },
    {
        "q": "Which diagnostic would be most concerning for making predictions with a linear regression model?",
        "type": "mcq",
        "o": [
            "Heteroscedastic residuals with funnel pattern",
            "Slightly skewed residual distribution",
            "One high leverage point with small residual",
            "VIF values around 2.5 for all predictors"
        ]
    },
    {
        "q": "In logistic regression, the ______ function converts probabilities to the log-odds scale.",
        "type": "fill_blank",
        "answers": ["logit"],
        "other_options": ["probit", "loglog", "cauchit"]
    },
    {
        "q": "Partial residual plots can reveal non-linear relationships that residual vs fitted plots might miss.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the regression concept with its interpretation:",
        "type": "match",
        "left": ["Odds ratio = 1", "R-squared = 0", "VIF = 1", "DW = 2"],
        "right": ["No effect of predictor", "No linear relationship", "No multicollinearity", "No autocorrelation"]
    },
    {
        "q": "What does this Python code help diagnose in a linear regression model?\n\nimport scipy.stats as stats\nstats.probplot(residuals, dist='norm', plot=plt)",
        "type": "mcq",
        "c": "import scipy.stats as stats\nstats.probplot(residuals, dist='norm', plot=plt)",
        "o": [
            "Normality of residuals",
            "Homoscedasticity",
            "Multicollinearity",
            "Linearity assumption"
        ]
    },
    {
        "q": "Rearrange these goodness-of-fit measures from most strict to least strict about model complexity:",
        "type": "rearrange",
        "words": ["AIC", "BIC", "Adjusted R-squared", "R-squared"]
    },
    {
        "q": "The ______ statistic measures the proportional reduction in deviance when adding predictors to a logistic regression model.",
        "type": "fill_blank",
        "answers": ["pseudo R-squared"],
        "other_options": ["F-statistic", "t-statistic", "chi-square"]
    },
    {
        "q": "What pattern in a residuals vs fitted plot would suggest the need for a quadratic term?",
        "type": "mcq",
        "o": [
            "U-shaped or inverted U-shaped curve",
            "Random scatter around zero",
            "Fan-shaped pattern widening",
            "Horizontal band with outliers"
        ]
    },
    {
        "q": "The ______ assumption requires that errors are not correlated with each other in time series data.",
        "type": "fill_blank",
        "answers": ["independence"],
        "other_options": ["normality", "linearity", "homoscedasticity"]
    },
    {
        "q": "A leverage point always has a large effect on the regression coefficients.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which situation would make the Wald test for a logistic regression coefficient unreliable?",
        "type": "mcq",
        "o": [
            "When the coefficient estimate is very large",
            "When the sample size is very large",
            "When all predictors are continuous",
            "When the model has few predictors"
        ]
    },
    {
        "q": "Match the regression diagnostic with the plot used to visualize it:",
        "type": "match",
        "left": ["Normality", "Influence", "Leverage", "Non-linearity"],
        "right": ["Q-Q plot", "Cook's distance plot", "Leverage plot", "Partial residual plot"]
    },
    {
        "q": "A linear regression model shows significant predictors but very low R-squared (0.08). What is the most likely interpretation?",
        "type": "mcq",
        "o": [
            "Predictors are statistically significant but explain little variance",
            "The model has multicollinearity issues",
            "Residuals are not normally distributed",
            "There are influential outliers affecting results"
        ]
    },
    {
        "q": "The ______ curve plots true positive rate against false positive rate to evaluate logistic regression classification performance.",
        "type": "fill_blank",
        "answers": ["ROC"],
        "other_options": ["precision-recall", "lift", "calibration"]
    },
    {
        "q": "Match the regression scenario with the most appropriate diagnostic check:",
        "type": "match",
        "left": ["Time series data", "Binary outcome data", "Count data", "Continuous outcome"],
        "right": ["Durbin-Watson test", "Hosmer-Lemeshow test", "Deviance residuals", "Q-Q plot of residuals"]
    },
    {
        "q": "In multiple linear regression, adding a new variable that is highly correlated with existing predictors will ______ the standard errors of coefficients.",
        "type": "fill_blank",
        "answers": ["increase"],
        "other_options": ["decrease", "stabilize", "not affect"]
    },
    {
        "q": "The link function in logistic regression ensures predicted values stay between 0 and 1.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these regression modeling steps in correct workflow order:",
        "type": "rearrange",
        "words": ["Hypothesis formulation", "Data collection", "Exploratory analysis", "Model specification", "Validation"]
    },
    {
        "q": "What does a leverage value of 0.15 indicate when the average leverage is 0.05 in a dataset with 100 observations?",
        "type": "mcq",
        "o": [
            "The point has above-average influence on parameter estimates",
            "The point is an outlier in the response variable",
            "The model has perfect multicollinearity",
            "Residuals are heteroscedastic"
        ]
    },
    {
        "q": "The ______ plot shows the relationship between one predictor and the response while accounting for other variables in the model.",
        "type": "fill_blank",
        "answers": ["added variable"],
        "other_options": ["residual", "influence", "correlation"]
    },
    {
        "q": "Which metric becomes problematic for imbalanced datasets in logistic regression evaluation?",
        "type": "mcq",
        "o": [
            "Accuracy",
            "AUC-ROC",
            "Log-loss",
            "Deviance"
        ]
    },
    {
        "q": "A ______ residual is calculated by deleting each observation and refitting the model to check for outliers.",
        "type": "fill_blank",
        "answers": ["studentized"],
        "other_options": ["standardized", "Pearson", "deviance"]
    },
    {
        "q": "The Box-Tidwell test specifically checks the linearity assumption in logistic regression.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the regression problem with its characteristic pattern in diagnostic plots:",
        "type": "match",
        "left": ["Heteroscedasticity", "Non-linearity", "Autocorrelation", "Influential points"],
        "right": ["Fan shape in residuals vs fitted", "Systematic curve in residuals", "Runs pattern in residual sequence", "Large Cook's distance values"]
    },
    {
        "q": "What does this R code help identify in regression analysis?\n\nlibrary(car)\nvif(model)",
        "type": "mcq",
        "c": "library(car)\nvif(model)",
        "o": [
            "Multicollinearity among predictors",
            "Heteroscedasticity in residuals",
            "Non-linearity in relationships",
            "Autocorrelation in errors"
        ]
    },
    {
        "q": "Rearrange these model comparison criteria from most conservative to least conservative:",
        "type": "rearrange",
        "words": ["BIC", "AIC", "Adjusted R-squared", "R-squared"]
    },
    {
        "q": "The ______ statistic tests whether a logistic regression model with predictors fits better than an intercept-only model.",
        "type": "fill_blank",
        "answers": ["likelihood ratio"],
        "other_options": ["Wald", "score", "deviance"]
    },
    {
        "q": "What does a calibration plot that curves below the diagonal line indicate about a logistic regression model?",
        "type": "mcq",
        "o": [
            "The model underestimates probabilities for high-risk cases",
            "The model overestimates probabilities systematically",
            "The model has perfect calibration",
            "There is multicollinearity in predictors"
        ]
    },
    {
        "q": "The ______ assumption requires that the relationship between predictors and log-odds is linear in logistic regression.",
        "type": "fill_blank",
        "answers": ["linearity"],
        "other_options": ["independence", "multicollinearity", "influence"]
    },
    {
        "q": "Pearson residuals in logistic regression should approximately follow a standard normal distribution if the model fits well.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which situation would make a logistic regression coefficient difficult to interpret directly?",
        "type": "mcq",
        "o": [
            "When there is interaction between predictors",
            "When the sample size is large",
            "When all predictors are standardized",
            "When the outcome is balanced"
        ]
    },
    {
        "q": "Match the regression diagnostic with its mathematical foundation:",
        "type": "match",
        "left": ["Cook's distance", "Variance Inflation Factor", "Durbin-Watson", "Studentized residual"],
        "right": ["Combines leverage and residual", "Correlation among predictors", "Autocorrelation of errors", "Outlier detection"]
    },
    {
        "q": "A perfect straight line in a Normal Q-Q plot of residuals indicates the ______ assumption is perfectly met.",
        "type": "fill_blank",
        "answers": ["normality"],
        "other_options": ["linearity", "independence", "homoscedasticity"]
    },
    {
        "q": "Rearrange the steps for a thorough residual analysis after fitting a regression model:",
        "type": "rearrange",
        "words": ["Plot Residuals vs Fitted", "Check Q-Q Plot", "Calculate Influence Stats", "Test for Autocorrelation"]
    },
    {
        "q": "The formula for a simple linear regression model is y = b0 + b1*x + error.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the type of regression with its primary use case:",
        "type": "match",
        "left": ["Linear Regression", "Logistic Regression", "Poisson Regression", "Cox Regression"],
        "right": ["Predicting continuous outcomes", "Predicting binary outcomes", "Predicting count outcomes", "Predicting time-to-event outcomes"]
    },
    {
        "q": "In the context of logistic regression, the term 'logit' refers to:",
        "type": "mcq",
        "o": [
            "The natural log of the odds",
            "The probability of the event",
            "The baseline hazard rate",
            "The residual sum of squares"
        ]
    },
    {
        "q": "The ______ of a regression model tells us how well the model fits the data.",
        "type": "fill_blank",
        "answers": ["goodness of fit"],
        "other_options": ["significance", "coefficient", "intercept"]
    },
    {
        "q": "A researcher is checking the assumptions for their linear model. They should verify that the residuals have constant variance, are independent, and are normally distributed.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the primary goal of residual analysis?",
        "type": "mcq",
        "o": [
            "To check if the model's assumptions are violated",
            "To increase the R-squared value of the model",
            "To select the most significant variables",
            "To transform the response variable"
        ]
    },
    {
        "q": "The process of using a regression model to make predictions for data points outside the range of the original data is called ______.",
        "type": "fill_blank",
        "answers": ["extrapolation"],
        "other_options": ["interpolation", "validation", "estimation"]
    },
    {
        "q": "Rearrange the typical workflow for building a predictive model using regression:",
        "type": "rearrange",
        "words": ["Define Problem", "Gather Data", "Clean Data", "Fit Model", "Evaluate Model"]
    },
    {
        "q": "A p-value for a regression coefficient that is less than the significance level alpha (e.g., 0.05) suggests that the predictor is statistically significant.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which of the following is NOT a common method for assessing the goodness of fit of a logistic regression model?",
        "type": "mcq",
        "o": [
            "R-squared",
            "Hosmer-Lemeshow test",
            "Area under the ROC curve (AUC)",
            "Confusion matrix metrics"
        ]
    },
    {
        "q": "The difference between the observed value and the value predicted by the regression model is called the ______.",
        "type": "fill_blank",
        "answers": ["residual"],
        "other_options": ["error", "bias", "variance"]
    },
    {
        "q": "Match the regression diagnostic plot with what it is primarily used to check:",
        "type": "match",
        "left": ["Residuals vs Fitted", "Scale-Location Plot", "Normal Q-Q Plot", "Residuals vs Leverage"],
        "right": ["Linearity & Homoscedasticity", "Homoscedasticity", "Normality of Residuals", "Influential Points"]
    },
    {
        "q": "In linear regression, the coefficient of determination (R-squared) can be interpreted as:",
        "type": "mcq",
        "o": [
            "The proportion of variance in the outcome explained by the model",
            "The average error of the predictions",
            "The significance level of the predictors",
            "The correlation between the predictors"
        ]
    },
    {
        "q": "A key assumption of linear regression is that the relationship between the predictors and the outcome is ______.",
        "type": "fill_blank",
        "answers": ["linear"],
        "other_options": ["curved", "categorical", "random"]
    },
    {
        "q": "Logistic regression can be used for multi-class classification problems (e.g., more than two outcome categories).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "When the variance of the residuals is not constant, we say the residuals exhibit ______.",
        "type": "fill_blank",
        "answers": ["heteroscedasticity"],
        "other_options": ["autocorrelation", "multicollinearity", "non-normality"]
    },
    {
        "q": "Rearrange the following terms from the most general model evaluation concept to the most specific statistic for linear regression:",
        "type": "rearrange",
        "words": ["Model Performance", "Goodness of Fit", "R-squared", "Adjusted R-squared"]
    },
    {
        "q": "The null hypothesis for the overall F-test in a linear regression is that all of the regression coefficients are equal to zero.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "When interpreting a logistic regression output, an odds ratio of 3.0 for a predictor variable means:",
        "type": "mcq",
        "o": [
            "The odds of the outcome are 3 times higher per unit increase in the predictor",
            "The probability increases by 300% per unit increase in the predictor",
            "The coefficient is statistically significant at p < 0.001",
            "The predictor explains 30% of the variance in the outcome"
        ]
    },
    {
        "q": "The ______ plot displays the square root of standardized residuals against fitted values to check for constant variance.",
        "type": "fill_blank",
        "answers": ["scale-location"],
        "other_options": ["residual", "leverage", "influence"]
    },
    {
        "q": "Match the regression diagnostic test with its specific purpose:",
        "type": "match",
        "left": ["Breusch-Pagan Test", "Durbin-Watson Test", "Variance Inflation Factor", "Shapiro-Wilk Test"],
        "right": ["Detect heteroscedasticity", "Check for autocorrelation", "Identify multicollinearity", "Test normality of residuals"]
    },
    {
        "q": "In multiple regression, the ______ effect occurs when the relationship between two variables changes at different levels of a third variable.",
        "type": "fill_blank",
        "answers": ["interaction"],
        "other_options": ["confounding", "mediation", "suppression"]
    },
    {
        "q": "Studentized residuals are more effective than standardized residuals for identifying outliers in small datasets.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these regression modeling concepts from most fundamental to most advanced:",
        "type": "rearrange",
        "words": ["Correlation", "Simple Linear Regression", "Multiple Regression", "Model Diagnostics"]
    },
    {
        "q": "What does a Cook's distance value greater than 1.0 typically indicate about an observation?",
        "type": "mcq",
        "o": [
            "The observation has substantial influence on the regression results",
            "The observation is a perfect predictor of the outcome",
            "The model explains 100% of the variance for that observation",
            "The residual for that observation is exactly zero"
        ]
    },
    {
        "q": "The ______ statistic measures how much the fitted values change when a particular observation is omitted from the dataset.",
        "type": "fill_blank",
        "answers": ["DFFITS"],
        "other_options": ["leverage", "covariance", "variance"]
    },
    {
        "q": "Which pattern in a partial regression plot would suggest the need to transform a predictor variable?",
        "type": "mcq",
        "o": [
            "A clear curved relationship between partial residuals and the predictor",
            "A perfectly straight horizontal line with no slope",
            "Random scatter with no discernible pattern",
            "All points clustered tightly around the mean"
        ]
    },
    {
        "q": "The ______ assumption in linear regression requires that the expected value of errors is zero for all combinations of predictors.",
        "type": "fill_blank",
        "answers": ["zero conditional mean"],
        "other_options": ["constant variance", "normal distribution", "linearity"]
    },
    {
        "q": "The condition number is a more comprehensive measure of multicollinearity than individual VIF values.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the regression scenario with the most appropriate goodness-of-fit measure:",
        "type": "match",
        "left": ["Linear regression with many predictors", "Logistic regression model comparison", "Nested linear models", "Overall logistic model fit"],
        "right": ["Adjusted R-squared", "Akaike Information Criterion", "Partial F-test", "Hosmer-Lemeshow test"]
    },
    {
        "q": "What does this diagnostic output primarily indicate about a linear regression model?\n\nBreusch-Pagan Test: p-value = 0.003\nWhite Test: p-value = 0.005",
        "type": "mcq",
        "o": [
            "Significant evidence of heteroscedasticity in the residuals",
            "Perfect normality of the error distribution",
            "Severe multicollinearity among predictors",
            "Autocorrelation in time series residuals"
        ]
    },
    {
        "q": "Rearrange these residual analysis steps in the recommended order of execution:",
        "type": "rearrange",
        "words": ["Visual Inspection", "Formal Testing", "Influence Analysis", "Remedial Actions"]
    },
    {
        "q": "The ______ plot helps visualize the combined effect of leverage and residual size on model coefficients.",
        "type": "fill_blank",
        "answers": ["influence"],
        "other_options": ["residual", "partial", "component"]
    },
    {
        "q": "In logistic regression, what does a classification table (confusion matrix) primarily help evaluate?",
        "type": "mcq",
        "o": [
            "The model's predictive accuracy and classification performance",
            "The statistical significance of individual coefficients",
            "The overall goodness-of-fit compared to a null model",
            "The presence of multicollinearity among predictors"
        ]
    },
    {
        "q": "The ______ residual in logistic regression is calculated as (observed - predicted) / sqrt(predicted*(1-predicted)).",
        "type": "fill_blank",
        "answers": ["Pearson"],
        "other_options": ["deviance", "studentized", "standardized"]
    },
    {
        "q": "A leverage point that lies close to the regression line will have minimal effect on the estimated coefficients.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which diagnostic would be most concerning for the validity of confidence intervals in linear regression?",
        "type": "mcq",
        "o": [
            "Severely non-normal residuals with heavy tails",
            "Slight curvature in the residuals vs fitted plot",
            "One observation with moderate leverage",
            "VIF values between 2 and 3 for all predictors"
        ]
    },
    {
        "q": "Match the regression concept with its interpretation in model diagnostics:",
        "type": "match",
        "left": ["Hat Values > 2p/n", "Studentized Residual > 3", "Cook's D > 4/n", "VIF > 5"],
        "right": ["High leverage observation", "Potential outlier", "Influential data point", "Moderate multicollinearity"]
    },
    {
        "q": "When comparing two logistic regression models, a likelihood ratio test p-value of 0.02 suggests:",
        "type": "mcq",
        "o": [
            "The more complex model provides significantly better fit",
            "Both models are equally adequate for the data",
            "The simpler model should be preferred due to parsimony",
            "There is severe multicollinearity in the complex model"
        ]
    },
    {
        "q": "The ______ statistic measures the change in a specific coefficient when an observation is removed from the dataset.",
        "type": "fill_blank",
        "answers": ["DFBETAS"],
        "other_options": ["leverage", "covratio", "influence"]
    },
    {
        "q": "Match the regression assumption violation with its most characteristic residual pattern:",
        "type": "match",
        "left": ["Heteroscedasticity", "Non-linearity", "Autocorrelation", "Outliers"],
        "right": ["Fan-shaped spread in residuals vs fitted", "Systematic curved pattern in residuals", "Runs of consecutive same-sign residuals", "Points with large studentized residuals"]
    },
    {
        "q": "In logistic regression, the ______ function maps probabilities to the real number line from negative infinity to positive infinity.",
        "type": "fill_blank",
        "answers": ["logit"],
        "other_options": ["probit", "loglog", "cauchit"]
    },
    {
        "q": "The partial residual plot can reveal non-linear relationships that are masked in ordinary residual plots.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these influence measures from most sensitive to individual observations to least sensitive:",
        "type": "rearrange",
        "words": ["DFBETAS", "Cook's Distance", "DFFITS", "Leverage"]
    },
    {
        "q": "What does a variance inflation factor (VIF) of 8.5 indicate about a predictor variable?",
        "type": "mcq",
        "o": [
            "Moderate to severe multicollinearity with other predictors",
            "The variable explains 85% of the outcome variance",
            "The coefficient is 8.5 times its standard error",
            "Perfect independence from other predictors"
        ]
    },
    {
        "q": "The ______ plot shows the relationship between a predictor and response after adjusting for all other variables in the model.",
        "type": "fill_blank",
        "answers": ["added-variable"],
        "other_options": ["component-plus-residual", "partial-regression", "influence"]
    },
    {
        "q": "Which diagnostic approach is most appropriate for detecting non-linearity in logistic regression?",
        "type": "mcq",
        "o": [
            "Box-Tidwell test for interaction with logit",
            "Breusch-Pagan test for heteroscedasticity",
            "Durbin-Watson test for autocorrelation",
            "Variance inflation factor calculation"
        ]
    },
    {
        "q": "A ______ residual in logistic regression measures the contribution of each observation to the overall model deviance.",
        "type": "fill_blank",
        "answers": ["deviance"],
        "other_options": ["Pearson", "working", "response"]
    },
    {
        "q": "The condition index is considered more reliable than VIF for detecting multicollinearity in datasets with many predictors.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the goodness-of-fit statistic with its primary advantage:",
        "type": "match",
        "left": ["AIC", "BIC", "Adjusted R-squared", "Cross-validated R-squared"],
        "right": ["Balances fit and complexity for prediction", "Strong penalty for model complexity", "Adjusts for number of predictors", "Estimates out-of-sample performance"]
    },
    {
        "q": "What pattern in a component-plus-residual plot would indicate the need for a quadratic term?",
        "type": "mcq",
        "o": [
            "Clear U-shaped or inverted U-shaped curvature",
            "Perfect straight line with slope of 1",
            "Random scatter with no discernible pattern",
            "Horizontal line at zero across all values"
        ]
    },
    {
        "q": "Rearrange these model building phases from initial to final stage:",
        "type": "rearrange",
        "words": ["Exploratory Analysis", "Model Specification", "Assumption Checking", "Model Validation", "Final Interpretation"]
    },
    {
        "q": "The ______ test evaluates whether the variance of residuals depends on the values of the predictor variables.",
        "type": "fill_blank",
        "answers": ["Breusch-Pagan"],
        "other_options": ["Shapiro-Wilk", "Durbin-Watson", "White"]
    },
    {
        "q": "In logistic regression, what does a calibration plot that shows predicted probabilities systematically higher than observed probabilities indicate?",
        "type": "mcq",
        "o": [
            "The model is overestimating the true probabilities",
            "Perfect model calibration has been achieved",
            "The model underestimates probabilities for all cases",
            "There is complete separation in the data"
        ]
    },
    {
        "q": "The ______ assumption requires that the correlation between any two different error terms is zero.",
        "type": "fill_blank",
        "answers": ["independence"],
        "other_options": ["exogeneity", "homoscedasticity", "linearity"]
    },
    {
        "q": "A high leverage point with a small residual has minimal effect on the regression coefficients.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which situation would make the Hosmer-Lemeshow test potentially unreliable?",
        "type": "mcq",
        "o": [
            "When the dataset has many continuous predictors",
            "When the sample size is very small",
            "When all predictors are categorical",
            "When the outcome is perfectly balanced"
        ]
    },
    {
        "q": "Match the diagnostic measure with its interpretation threshold:",
        "type": "match",
        "left": ["VIF > 10", "Cook's D > 1", "Studentized Residual > 2", "Leverage > 2p/n"],
        "right": ["Severe multicollinearity", "Highly influential point", "Potential outlier", "High leverage observation"]
    },
    {
        "q": "A researcher notices that the confidence intervals for regression coefficients become extremely wide after adding a new predictor. This most likely indicates:",
        "type": "mcq",
        "o": [
            "Severe multicollinearity with existing predictors",
            "Perfect model fit has been achieved",
            "The new predictor has no relationship with the outcome",
            "Homoscedasticity assumptions are violated"
        ]
    },
    {
        "q": "The ______ plot displays the cumulative distribution of residuals to assess departure from normality.",
        "type": "fill_blank",
        "answers": ["P-P"],
        "other_options": ["Q-Q", "residual", "influence"]
    },
    {
        "q": "Match the regression diagnostic technique with what it specifically examines:",
        "type": "match",
        "left": ["Added Variable Plot", "Partial Residual Plot", "Leverage Plot", "Influence Plot"],
        "right": ["Unique contribution of each predictor", "Non-linearity in predictor relationships", "Impact of individual observations", "Combined effect of leverage and residuals"]
    },
    {
        "q": "In logistic regression, complete ______ occurs when a predictor perfectly separates outcome classes, making coefficient estimates unstable.",
        "type": "fill_blank",
        "answers": ["separation"],
        "other_options": ["collinearity", "specification", "independence"]
    },
    {
        "q": "The White test is more general than the Breusch-Pagan test as it can detect non-linear forms of heteroscedasticity.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these multicollinearity detection methods from simplest to most comprehensive:",
        "type": "rearrange",
        "words": ["Correlation Matrix", "Variance Inflation Factor", "Tolerance", "Condition Index"]
    },
    {
        "q": "What does a significant Box-Tidwell test result indicate in logistic regression diagnostics?",
        "type": "mcq",
        "o": [
            "Non-linearity in the logit for continuous predictors",
            "Severe multicollinearity among all predictors",
            "Autocorrelation in the residual sequence",
            "Heteroscedasticity in the working residuals"
        ]
    },
    {
        "q": "The ______ statistic measures how much the covariance matrix of coefficients changes when an observation is deleted.",
        "type": "fill_blank",
        "answers": ["COVRATIO"],
        "other_options": ["DFBETAS", "DFFITS", "leverage"]
    },
    {
        "q": "Which residual pattern would be most concerning for a time series regression model?",
        "type": "mcq",
        "o": [
            "Runs of positive residuals followed by runs of negative residuals",
            "Random scatter around the horizontal zero line",
            "Slight curvature in the residual distribution",
            "A few isolated outliers at extreme predictor values"
        ]
    },
    {
        "q": "The ______ assumption in linear regression requires that predictors are measured without error.",
        "type": "fill_blank",
        "answers": ["fixed regressors"],
        "other_options": ["random sampling", "linearity", "exogeneity"]
    },
    {
        "q": "AIC and BIC will always select the same model when comparing nested regression models.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the regression scenario with the most appropriate remedial action:",
        "type": "match",
        "left": ["Heteroscedastic residuals", "Non-linear relationships", "Influential outliers", "Autocorrelated errors"],
        "right": ["Use robust standard errors", "Add polynomial terms", "Apply robust regression", "Include lagged variables"]
    },
    {
        "q": "What does this diagnostic output suggest about a logistic regression model?\n\nHosmer-Lemeshow Test: p-value = 0.83\nArea under ROC curve: 0.91",
        "type": "mcq",
        "o": [
            "Good overall fit with excellent discrimination",
            "Poor calibration but good discrimination",
            "Severe multicollinearity issues",
            "Non-linearity in predictor relationships"
        ]
    },
    {
        "q": "Rearrange these model evaluation steps for logistic regression in logical order:",
        "type": "rearrange",
        "words": ["Check Residual Patterns", "Assess Model Calibration", "Evaluate Discrimination", "Test Overall Fit"]
    },
    {
        "q": "The ______ plot helps identify observations that have unusual combinations of predictor values.",
        "type": "fill_blank",
        "answers": ["leverage"],
        "other_options": ["residual", "partial", "component"]
    },
    {
        "q": "In multiple regression, what does a non-significant overall F-test with significant individual t-tests suggest?",
        "type": "mcq",
        "o": [
            "Severe multicollinearity among predictors",
            "Perfect homoscedasticity of residuals",
            "Complete independence of errors",
            "Excellent model specification"
        ]
    },
    {
        "q": "The ______ residual in logistic regression represents the difference between the observed and predicted values on the response scale.",
        "type": "fill_blank",
        "answers": ["response"],
        "other_options": ["deviance", "Pearson", "working"]
    },
    {
        "q": "A high leverage point will always have a large Cook's distance value.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which diagnostic is most appropriate for detecting non-constant error variance in the presence of non-normality?",
        "type": "mcq",
        "o": [
            "White test for heteroscedasticity",
            "Shapiro-Wilk test for normality",
            "Durbin-Watson test for autocorrelation",
            "Variance inflation factor"
        ]
    },
    {
        "q": "Match the influence measure with what it specifically quantifies:",
        "type": "match",
        "left": ["DFBETAS", "DFFITS", "Cook's Distance", "Leverage"],
        "right": ["Change in specific coefficients", "Change in fitted values", "Overall influence on all parameters", "Potential for influence"]
    },
    {
        "q": "When a logistic regression model shows perfect prediction for some combinations of predictors, this indicates:",
        "type": "mcq",
        "o": [
            "Complete separation in the data",
            "Perfect multicollinearity",
            "Homoscedastic residuals",
            "Normal distribution of errors"
        ]
    },
    {
        "q": "The ______ test evaluates whether the variance of errors is constant across different values of the predictors.",
        "type": "fill_blank",
        "answers": ["Breusch-Pagan"],
        "other_options": ["Durbin-Watson", "Shapiro-Wilk", "Jarque-Bera"]
    },
    {
        "q": "Match the regression diagnostic with its specific focus area:",
        "type": "match",
        "left": ["Q-Q Plot", "Residuals vs Leverage", "Partial Regression Plot", "Scale-Location Plot"],
        "right": ["Normality of errors", "Influential observations", "Relationship after adjusting for other predictors", "Constant variance assumption"]
    },
    {
        "q": "In multiple linear regression, the ______ effect occurs when the inclusion of a third variable changes the relationship between two other variables.",
        "type": "fill_blank",
        "answers": ["suppressor"],
        "other_options": ["mediator", "confounding", "interaction"]
    },
    {
        "q": "A leverage point must always be an outlier in the response variable.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these regression diagnostic checks from most basic to most advanced:",
        "type": "rearrange",
        "words": ["Residual Plots", "Influence Measures", "Multicollinearity Diagnostics", "Specification Tests"]
    },
    {
        "q": "What does a variance inflation factor (VIF) of 1.2 indicate about a predictor?",
        "type": "mcq",
        "o": [
            "Minimal multicollinearity with other predictors",
            "Severe correlation with the outcome variable",
            "The predictor explains 20% of the variance",
            "The coefficient is 1.2 times its standard error"
        ]
    },
    {
        "q": "The ______ statistic measures the change in the determinant of the covariance matrix when an observation is removed.",
        "type": "fill_blank",
        "answers": ["COVRATIO"],
        "other_options": ["DFBETAS", "DFFITS", "leverage"]
    },
    {
        "q": "Which pattern in a residual plot would suggest the need for a log transformation of the response variable?",
        "type": "mcq",
        "o": [
            "Fan-shaped pattern where spread increases with fitted values",
            "Random scatter around the horizontal line",
            "Curved pattern that resembles a parabola",
            "Clustered points with no clear relationship"
        ]
    },
    {
        "q": "The ______ assumption requires that the error terms are uncorrelated with each other in time series data.",
        "type": "fill_blank",
        "answers": ["independence"],
        "other_options": ["exogeneity", "stationarity", "linearity"]
    },
    {
        "q": "The Hosmer-Lemeshow test is used to assess goodness of fit in linear regression models.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the regression problem with its most characteristic diagnostic signature:",
        "type": "match",
        "left": ["Heteroscedasticity", "Autocorrelation", "Non-linearity", "Influential Points"],
        "right": ["Increasing spread in residual plot", "Runs pattern in residual sequence", "Systematic curvature in residuals", "Large Cook's distance values"]
    },
    {
        "q": "What does a significant Durbin-Watson test statistic (d < 1.5) indicate in time series regression?",
        "type": "mcq",
        "o": [
            "Positive autocorrelation in residuals",
            "Perfect independence of errors",
            "Severe multicollinearity",
            "Non-constant variance"
        ]
    },
    {
        "q": "Rearrange these model building phases in sequential order:",
        "type": "rearrange",
        "words": ["Data Preparation", "Model Specification", "Parameter Estimation", "Diagnostic Checking", "Model Validation"]
    },
    {
        "q": "The ______ plot helps visualize the relationship between a single predictor and the response while controlling for other variables.",
        "type": "fill_blank",
        "answers": ["partial regression"],
        "other_options": ["residual", "influence", "component"]
    },
    {
        "q": "In logistic regression, what does a deviance residual that is much larger than 2 indicate?",
        "type": "mcq",
        "o": [
            "Poor fit for that particular observation",
            "Perfect prediction for that case",
            "High leverage of the observation",
            "Multicollinearity with other predictors"
        ]
    },
    {
        "q": "The ______ assumption in linear regression requires that the expected value of errors is zero.",
        "type": "fill_blank",
        "answers": ["zero mean"],
        "other_options": ["constant variance", "normal distribution", "linearity"]
    },
    {
        "q": "A high leverage point will always have a large effect on the regression coefficients.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which situation would make the likelihood ratio test preferred over the Wald test in logistic regression?",
        "type": "mcq",
        "o": [
            "When sample size is small to moderate",
            "When there are many categorical predictors",
            "When the outcome is perfectly balanced",
            "When all predictors are continuous"
        ]
    },
    {
        "q": "Match the goodness-of-fit measure with its primary limitation:",
        "type": "match",
        "left": ["R-squared", "Adjusted R-squared", "AIC", "BIC"],
        "right": ["Always increases with more predictors", "Can be negative with poor models", "No formal hypothesis test", "Strong small-sample bias"]
    },
    {
        "q": "The process of adding polynomial terms to a regression model to capture non-linear relationships is called ______.",
        "type": "fill_blank",
        "answers": ["polynomial regression"],
        "other_options": ["logarithmic transformation", "interaction effects", "regularization"]
    },
    {
        "q": "Rearrange the typical workflow for addressing regression assumption violations:",
        "type": "rearrange",
        "words": ["Identify Violation", "Choose Remediation", "Apply Transformation", "Re-check Assumptions"]
    },
    {
        "q": "A leverage point that lies far from the center of the predictor space but close to the regression line is called a ______.",
        "type": "fill_blank",
        "answers": ["good leverage point"],
        "other_options": ["bad leverage point", "influential point", "outlier"]
    },
    {
        "q": "In logistic regression, the link function that uses the inverse of the standard normal CDF is called the ______ function.",
        "type": "fill_blank",
        "answers": ["probit"],
        "other_options": ["logit", "cauchit", "cloglog"]
    },
    {
        "q": "The condition index is calculated from the eigenvalues of the correlation matrix of predictors.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the type of residual with its specific use case in regression diagnostics:",
        "type": "match",
        "left": ["Pearson Residuals", "Studentized Residuals", "Deviance Residuals", "Working Residuals"],
        "right": ["Standardized measure of fit", "Outlier detection in linear models", "Goodness of fit in GLMs", "Iterative estimation process"]
    },
    {
        "q": "A researcher finds that the correlation between two predictors is 0.95. This suggests potential ______ problems.",
        "type": "fill_blank",
        "answers": ["multicollinearity"],
        "other_options": ["heteroscedasticity", "autocorrelation", "non-linearity"]
    },
    {
        "q": "The ______ effect occurs when the relationship between a predictor and outcome changes direction when another variable is controlled for.",
        "type": "fill_blank",
        "answers": ["suppressor"],
        "other_options": ["mediator", "confounding", "interaction"]
    },
    {
        "q": "In weighted least squares regression, observations with larger variances receive ______ weights.",
        "type": "fill_blank",
        "answers": ["smaller"],
        "other_options": ["larger", "equal", "zero"]
    },
    {
        "q": "The Goldfeld-Quandt test is specifically designed to detect heteroscedasticity in linear regression.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these concepts from most specific to most general in regression analysis:",
        "type": "rearrange",
        "words": ["Residual", "Error Term", "Model Deviation", "Total Variation"]
    },
    {
        "q": "A ______ plot shows the relationship between each predictor and the residuals to check for omitted variable bias.",
        "type": "fill_blank",
        "answers": ["partial residual"],
        "other_options": ["component plus residual", "added variable", "residuals vs leverage"]
    },
    {
        "q": "The ______ statistic measures how much the covariance matrix of coefficients changes when an observation is deleted.",
        "type": "fill_blank",
        "answers": ["COVRATIO"],
        "other_options": ["DFBETAS", "DFFITS", "leverage"]
    },
    {
        "q": "In robust regression, the ______ estimator minimizes the sum of absolute residuals rather than squared residuals.",
        "type": "fill_blank",
        "answers": ["LAD"],
        "other_options": ["M-estimator", "S-estimator", "MM-estimator"]
    },
    {
        "q": "The link function in a generalized linear model must be monotonic and differentiable.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the regression diagnostic with the specific pattern it detects:",
        "type": "match",
        "left": ["Fan Shape", "Curved Pattern", "Runs Pattern", "Leverage Points"],
        "right": ["Heteroscedasticity", "Non-linearity", "Autocorrelation", "Unusual predictor combinations"]
    },
    {
        "q": "The ______ test compares the fit of two nested models using the difference in their residual sum of squares.",
        "type": "fill_blank",
        "answers": ["F-test"],
        "other_options": ["t-test", "Wald test", "Lagrange multiplier"]
    },
    {
        "q": "In ______ regression, the response variable follows a binomial distribution with logit link function.",
        "type": "fill_blank",
        "answers": ["logistic"],
        "other_options": ["Poisson", "gamma", "inverse Gaussian"]
    },
    {
        "q": "A ______ point has both high leverage and a large residual, making it particularly influential.",
        "type": "fill_blank",
        "answers": ["bad leverage"],
        "other_options": ["good leverage", "regular", "typical"]
    },
    {
        "q": "The Breusch-Godfrey test is an extension of the Durbin-Watson test for higher-order autocorrelation.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "When the variance of residuals increases with fitted values, this violates the ______ assumption.",
        "type": "fill_blank",
        "answers": ["homoscedasticity"],
        "other_options": ["linearity", "independence", "normality"]
    },
    {
        "q": "Rearrange these regression diagnostic procedures from quickest to most computationally intensive:",
        "type": "rearrange",
        "words": ["Residual Plots", "VIF Calculation", "Influence Measures", "Bootstrap Validation"]
    },
    {
        "q": "A point with high ______ has unusual predictor values that can potentially influence the regression line.",
        "type": "fill_blank",
        "answers": ["leverage"],
        "other_options": ["residual", "variance", "correlation"]
    },
    {
        "q": "The ______ function transforms probabilities to the log-odds scale in logistic regression.",
        "type": "fill_blank",
        "answers": ["logit"],
        "other_options": ["probit", "loglog", "identity"]
    },
    {
        "q": "Cook's distance combines information about both leverage and residual size.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the regression assumption with its primary diagnostic check:",
        "type": "match",
        "left": ["Linearity", "Constant Variance", "Normality", "Independence"],
        "right": ["Residuals vs Fitted plot", "Scale-Location plot", "Q-Q plot", "Durbin-Watson test"]
    },
    {
        "q": "A VIF value of 12 indicates ______ multicollinearity among predictors.",
        "type": "fill_blank",
        "answers": ["severe"],
        "other_options": ["moderate", "mild", "no"]
    },
    {
        "q": "The ______ plot helps identify observations that have substantial impact on the regression coefficients.",
        "type": "fill_blank",
        "answers": ["influence"],
        "other_options": ["residual", "partial", "component"]
    },
    {
        "q": "In logistic regression, the ______ test evaluates whether observed frequencies match expected frequencies across probability deciles.",
        "type": "fill_blank",
        "answers": ["Hosmer-Lemeshow"],
        "other_options": ["Breusch-Pagan", "Shapiro-Wilk", "Jarque-Bera"]
    },
    {
        "q": "A perfectly horizontal line in a residuals vs fitted plot indicates ideal model fit.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these model comparison criteria from least to most conservative about model complexity:",
        "type": "rearrange",
        "words": ["R-squared", "AIC", "BIC", "Adjusted R-squared"]
    },
    {
        "q": "The ______ residual is calculated by dividing the ordinary residual by its standard deviation estimate.",
        "type": "fill_blank",
        "answers": ["standardized"],
        "other_options": ["studentized", "Pearson", "deviance"]
    },
    {
        "q": "When errors are correlated over time, this violates the ______ assumption.",
        "type": "fill_blank",
        "answers": ["independence"],
        "other_options": ["linearity", "homoscedasticity", "normality"]
    },
    {
        "q": "The ______ statistic measures the change in a specific coefficient when an observation is deleted.",
        "type": "fill_blank",
        "answers": ["DFBETAS"],
        "other_options": ["DFFITS", "leverage", "covratio"]
    },
    {
        "q": "A fan-shaped pattern in a residuals vs fitted plot suggests heteroscedasticity.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the goodness-of-fit measure with its regression context:",
        "type": "match",
        "left": ["R-squared", "AUC", "Deviance", "Mallows' Cp"],
        "right": ["Linear regression", "Logistic regression", "Generalized linear models", "Model selection"]
    },
    {
        "q": "The ______ assumption requires that the relationship between predictors and response is linear in the parameters.",
        "type": "fill_blank",
        "answers": ["linearity"],
        "other_options": ["independence", "exogeneity", "homoscedasticity"]
    },
    {
        "q": "A ______ point has both high leverage and a large residual, making it particularly influential.",
        "type": "fill_blank",
        "answers": ["bad leverage"],
        "other_options": ["good leverage", "regular", "typical"]
    },
    {
        "q": "The Breusch-Pagan test specifically checks for heteroscedasticity in linear regression.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these steps for building a reliable regression model:",
        "type": "rearrange",
        "words": ["Exploratory Analysis", "Model Specification", "Assumption Checking", "Model Refinement", "Validation"]
    },
    {
        "q": "When the relationship between a predictor and outcome differs across levels of another variable, this indicates an ______ effect.",
        "type": "fill_blank",
        "answers": ["interaction"],
        "other_options": ["additive", "suppressor", "confounding"]
    },
    {
        "q": "Rearrange these influence diagnostics from most specific to most general:",
        "type": "rearrange",
        "words": ["DFBETAS", "DFFITS", "Cook's D", "Leverage"]
    },
    {
        "q": "A ______ residual plot shows the relationship between each predictor and the response after accounting for other variables.",
        "type": "fill_blank",
        "answers": ["partial"],
        "other_options": ["component", "added", "incremental"]
    },
    {
        "q": "The ______ test evaluates whether the log-odds are linear in the predictors for logistic regression.",
        "type": "fill_blank",
        "answers": ["Box-Tidwell"],
        "other_options": ["Breusch-Pagan", "Durbin-Watson", "White"]
    },
    {
        "q": "A high leverage point will always substantially change the regression coefficients when removed.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the regression problem with its characteristic diagnostic pattern:",
        "type": "match",
        "left": ["Non-constant variance", "Omitted variables", "Correlated errors", "Non-normal errors"],
        "right": ["Fan-shaped residual spread", "Systematic patterns in residuals", "Runs of same-sign residuals", "Heavy tails in Q-Q plot"]
    },
    {
        "q": "The ______ ratio compares the likelihood of the fitted model to that of the null model in logistic regression.",
        "type": "fill_blank",
        "answers": ["likelihood"],
        "other_options": ["odds", "risk", "hazard"]
    },
    {
        "q": "A ______ point has extreme predictor values but lies close to the regression line.",
        "type": "fill_blank",
        "answers": ["good leverage"],
        "other_options": ["bad leverage", "influential", "outlier"]
    },
    {
        "q": "The ______ index helps identify multicollinearity by examining the condition number of the predictor matrix.",
        "type": "fill_blank",
        "answers": ["condition"],
        "other_options": ["variance", "stability", "collinearity"]
    },
    {
        "q": "Studentized residuals follow a t-distribution when the model assumptions are met.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these regression validation approaches from simplest to most robust:",
        "type": "rearrange",
        "words": ["Data Splitting", "Cross-Validation", "Bootstrap", "External Validation"]
    },
    {
        "q": "The ______ function maps probabilities to the real number line in generalized linear models.",
        "type": "fill_blank",
        "answers": ["link"],
        "other_options": ["transfer", "transform", "connection"]
    },
    {
        "q": "When predictors are highly correlated, this creates ______ problems in regression estimation.",
        "type": "fill_blank",
        "answers": ["multicollinearity"],
        "other_options": ["heteroscedasticity", "autocorrelation", "endogeneity"]
    },
    {
        "q": "The ______ statistic measures the change in predicted values when an observation is deleted.",
        "type": "fill_blank",
        "answers": ["DFFITS"],
        "other_options": ["DFBETAS", "leverage", "covratio"]
    },
    {
        "q": "A perfectly calibrated logistic regression model has predicted probabilities that match observed frequencies.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the residual type with its primary use in diagnostics:",
        "type": "match",
        "left": ["Ordinary residuals", "Standardized residuals", "Studentized residuals", "Deviance residuals"],
        "right": ["Basic model checking", "Comparing across observations", "Outlier detection", "GLM goodness of fit"]
    },
    {
        "q": "The ______ assumption requires that errors have the same variance for all combinations of predictor values.",
        "type": "fill_blank",
        "answers": ["homoscedasticity"],
        "other_options": ["linearity", "independence", "exogeneity"]
    },
    {
        "q": "A ______ effect occurs when controlling for a variable reveals a relationship that was previously masked.",
        "type": "fill_blank",
        "answers": ["suppressor"],
        "other_options": ["mediator", "confounding", "interaction"]
    },
    {
        "q": "The Hosmer-Lemeshow test assesses goodness of fit specifically for linear regression models.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these multicollinearity diagnostics from most commonly used to most comprehensive:",
        "type": "rearrange",
        "words": ["Correlation Matrix", "VIF", "Tolerance", "Condition Number"]
    },
    {
        "q": "When a logistic regression coefficient has a very large standard error, this may indicate:",
        "type": "mcq",
        "o": [
            "Complete or quasi-complete separation in the data",
            "Perfect homoscedasticity of residuals",
            "Excellent model calibration",
            "All assumptions are perfectly met"
        ]
    },
    {
        "q": "The ______ plot displays the square root of absolute standardized residuals against fitted values to assess constant variance.",
        "type": "fill_blank",
        "answers": ["scale-location"],
        "other_options": ["residual", "leverage", "influence"]
    },
    {
        "q": "Match the regression diagnostic technique with what it specifically detects:",
        "type": "match",
        "left": ["Partial Residual Plot", "Added Variable Plot", "Component Plus Residual Plot", "Leverage Plot"],
        "right": ["Non-linearity in specific predictors", "Unique contribution of each predictor", "Overall relationship checking", "Influence of individual cases"]
    },
    {
        "q": "In multiple regression, the ______ effect occurs when a predictor appears non-significant due to correlation with other variables.",
        "type": "fill_blank",
        "answers": ["masking"],
        "other_options": ["suppression", "confounding", "mediation"]
    },
    {
        "q": "The White test can detect both linear and non-linear forms of heteroscedasticity.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these model building stages from initial conceptualization to final validation:",
        "type": "rearrange",
        "words": ["Theory Development", "Variable Selection", "Model Estimation", "Diagnostic Checking", "Out-of-Sample Testing"]
    },
    {
        "q": "What does a significant Box-Tidwell test specifically indicate about a continuous predictor in logistic regression?",
        "type": "mcq",
        "o": [
            "The linearity in logit assumption is violated",
            "The predictor has perfect multicollinearity",
            "The residuals are autocorrelated",
            "The link function is misspecified"
        ]
    },
    {
        "q": "The ______ statistic measures how much the covariance matrix of coefficients changes when each observation is sequentially deleted.",
        "type": "fill_blank",
        "answers": ["COVRATIO"],
        "other_options": ["DFBETAS", "DFFITS", "leverage"]
    },
    {
        "q": "Which pattern in a time series residual plot would be most indicative of positive autocorrelation?",
        "type": "mcq",
        "o": [
            "Runs of positive residuals followed by runs of negative residuals",
            "Random scatter around the zero line",
            "Fan-shaped spread increasing over time",
            "All residuals clustered near zero"
        ]
    },
    {
        "q": "The ______ assumption in regression requires that the error terms are uncorrelated with the predictor variables.",
        "type": "fill_blank",
        "answers": ["exogeneity"],
        "other_options": ["homoscedasticity", "linearity", "normality"]
    },
    {
        "q": "AIC and BIC will always select the same model when the sample size is very large.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the regression violation with its most appropriate remedial approach:",
        "type": "match",
        "left": ["Non-constant variance", "Non-linear relationships", "Influential observations", "Correlated errors"],
        "right": ["Robust standard errors", "Polynomial terms", "Robust regression methods", "Time series corrections"]
    },
    {
        "q": "What does a non-significant Hosmer-Lemeshow test (p > 0.05) indicate about a logistic regression model?",
        "type": "mcq",
        "o": [
            "Adequate calibration between predicted and observed probabilities",
            "Poor discrimination between classes",
            "Severe multicollinearity issues",
            "Perfect separation in the data"
        ]
    },
    {
        "q": "Rearrange these influence detection methods from most specific to coefficient level to most general:",
        "type": "rearrange",
        "words": ["DFBETAS", "DFFITS", "Cook's Distance", "Leverage"]
    },
    {
        "q": "The ______ plot helps visualize the unique relationship between a predictor and response after removing effects of other variables.",
        "type": "fill_blank",
        "answers": ["added-variable"],
        "other_options": ["partial-regression", "component-residual", "influence"]
    },
    {
        "q": "In logistic regression, what does a large deviance residual (absolute value > 2) typically indicate?",
        "type": "mcq",
        "o": [
            "Poor model fit for that particular observation",
            "Perfect prediction for that case",
            "High leverage of the observation",
            "Severe multicollinearity"
        ]
    },
    {
        "q": "The ______ assumption requires that the expected value of the error term is zero for all observations.",
        "type": "fill_blank",
        "answers": ["zero mean"],
        "other_options": ["constant variance", "normal distribution", "independence"]
    },
    {
        "q": "A high leverage point must always be removed from the regression analysis.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which diagnostic is most appropriate for detecting non-constant variance when the error distribution is non-normal?",
        "type": "mcq",
        "o": [
            "White test for heteroscedasticity",
            "Shapiro-Wilk test for normality",
            "Durbin-Watson test for autocorrelation",
            "Variance inflation factor"
        ]
    },
    {
        "q": "Match the influence measure with what it specifically quantifies:",
        "type": "match",
        "left": ["DFBETAS", "DFFITS", "Cook's D", "Leverage"],
        "right": ["Change in specific coefficient estimates", "Change in fitted values", "Overall influence on all parameters", "Potential for influence based on X-values"]
    },
    {
        "q": "When the variance of the error term depends on the level of the predictors, this violates the ______ assumption.",
        "type": "fill_blank",
        "answers": ["homoscedasticity"],
        "other_options": ["linearity", "independence", "normality"]
    },
    {
        "q": "Rearrange these steps for conducting a thorough regression diagnostic analysis:",
        "type": "rearrange",
        "words": ["Check Residual Plots", "Test for Multicollinearity", "Assess Influence Points", "Verify Normality"]
    },
    {
        "q": "A point with high ______ has predictor values that are far from the mean of the predictors.",
        "type": "fill_blank",
        "answers": ["leverage"],
        "other_options": ["residual", "variance", "bias"]
    },
    {
        "q": "The ______ function converts probabilities to log-odds in binary logistic regression.",
        "type": "fill_blank",
        "answers": ["logit"],
        "other_options": ["probit", "loglog", "cloglog"]
    },
    {
        "q": "A perfectly random scatter in a residuals vs fitted plot indicates all linear regression assumptions are met.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the regression diagnostic with its primary purpose:",
        "type": "match",
        "left": ["Q-Q Plot", "Variance Inflation Factor", "Cook's Distance", "Durbin-Watson Test"],
        "right": ["Check normality of residuals", "Detect multicollinearity", "Identify influential points", "Test for autocorrelation"]
    },
    {
        "q": "A VIF value greater than 10 indicates serious ______ problems.",
        "type": "fill_blank",
        "answers": ["multicollinearity"],
        "other_options": ["heteroscedasticity", "autocorrelation", "non-linearity"]
    },
    {
        "q": "The ______ plot helps visualize observations that have unusual impact on the regression results.",
        "type": "fill_blank",
        "answers": ["influence"],
        "other_options": ["residual", "partial", "component"]
    },
    {
        "q": "In logistic regression, the ______ test assesses how well predicted probabilities match observed outcomes.",
        "type": "fill_blank",
        "answers": ["Hosmer-Lemeshow"],
        "other_options": ["Breusch-Pagan", "Shapiro-Wilk", "Jarque-Bera"]
    },
    {
        "q": "A curved pattern in a residuals vs fitted plot suggests potential non-linearity.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these model evaluation metrics from most optimistic to most conservative about model complexity:",
        "type": "rearrange",
        "words": ["R-squared", "Adjusted R-squared", "AIC", "BIC"]
    },
    {
        "q": "The ______ residual is calculated by standardizing the ordinary residual by an estimate of its standard deviation.",
        "type": "fill_blank",
        "answers": ["studentized"],
        "other_options": ["Pearson", "deviance", "working"]
    },
    {
        "q": "When errors from consecutive observations are correlated, this violates the ______ assumption.",
        "type": "fill_blank",
        "answers": ["independence"],
        "other_options": ["linearity", "homoscedasticity", "normality"]
    },
    {
        "q": "The ______ statistic measures how much a specific coefficient changes when an observation is removed.",
        "type": "fill_blank",
        "answers": ["DFBETAS"],
        "other_options": ["DFFITS", "leverage", "covratio"]
    },
    {
        "q": "A fan-shaped pattern in a residual plot always indicates non-constant variance.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the goodness-of-fit measure with its appropriate regression context:",
        "type": "match",
        "left": ["R-squared", "Area Under ROC", "Pseudo R-squared", "Mean Squared Error"],
        "right": ["Linear regression", "Logistic regression", "Generalized linear models", "Prediction accuracy"]
    },
    {
        "q": "The ______ assumption requires that the expected value of errors is zero.",
        "type": "fill_blank",
        "answers": ["zero mean"],
        "other_options": ["constant variance", "normal distribution", "independence"]
    },
    {
        "q": "A point with both high leverage and large residual is called a ______ point.",
        "type": "fill_blank",
        "answers": ["bad leverage"],
        "other_options": ["good leverage", "regular", "typical"]
    },
    {
        "q": "The Breusch-Pagan test is used to detect autocorrelation in time series data.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these regression analysis phases in logical order:",
        "type": "rearrange",
        "words": ["Data Preparation", "Model Fitting", "Diagnostic Checking", "Model Interpretation"]
    },
    {
        "q": "When the relationship between predictors and outcome is not straight-line, this violates the ______ assumption.",
        "type": "fill_blank",
        "answers": ["linearity"],
        "other_options": ["independence", "homoscedasticity", "normality"]
    },
    {
        "q": "Rearrange these residual analysis components from visual to numerical methods:",
        "type": "rearrange",
        "words": ["Residual Plots", "Influence Measures", "Statistical Tests", "Numerical Summaries"]
    },
    {
        "q": "A point with extreme predictor values that doesn't affect the regression line is called a ______ point.",
        "type": "fill_blank",
        "answers": ["good leverage"],
        "other_options": ["bad leverage", "influential", "outlier"]
    },
    {
        "q": "The ______ function is the inverse of the logit function in logistic regression.",
        "type": "fill_blank",
        "answers": ["logistic"],
        "other_options": ["exponential", "probit", "identity"]
    },
    {
        "q": "All high leverage points are influential points that should be removed from analysis.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the regression concept with its diagnostic application:",
        "type": "match",
        "left": ["Heteroscedasticity", "Multicollinearity", "Autocorrelation", "Non-linearity"],
        "right": ["Scale-location plot", "Variance inflation factor", "Durbin-Watson statistic", "Partial residual plot"]
    },
    {
        "q": "A tolerance value close to 0 indicates severe ______ problems.",
        "type": "fill_blank",
        "answers": ["multicollinearity"],
        "other_options": ["heteroscedasticity", "autocorrelation", "non-linearity"]
    },
    {
        "q": "The ______ plot shows the relationship between residuals and each predictor variable individually.",
        "type": "fill_blank",
        "answers": ["residuals vs predictors"],
        "other_options": ["Q-Q", "influence", "component"]
    },
    {
        "q": "In logistic regression, the ______ statistic compares observed and expected frequencies across probability groups.",
        "type": "fill_blank",
        "answers": ["Hosmer-Lemeshow"],
        "other_options": ["Pearson chi-square", "Likelihood ratio", "Wald"]
    },
    {
        "q": "A perfectly horizontal line in a scale-location plot indicates ideal constant variance.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these influence diagnostics from most specific to most general impact:",
        "type": "rearrange",
        "words": ["DFBETAS", "DFFITS", "Cook's D", "Leverage"]
    },
    {
        "q": "The ______ residual is used in generalized linear models to assess individual observation fit.",
        "type": "fill_blank",
        "answers": ["deviance"],
        "other_options": ["Pearson", "working", "response"]
    },
    {
        "q": "When the same subjects are measured multiple times, this may violate the ______ assumption.",
        "type": "fill_blank",
        "answers": ["independence"],
        "other_options": ["linearity", "homoscedasticity", "normality"]
    },
    {
        "q": "The ______ statistic measures the change in the covariance matrix when an observation is deleted.",
        "type": "fill_blank",
        "answers": ["COVRATIO"],
        "other_options": ["DFBETAS", "DFFITS", "leverage"]
    },
    {
        "q": "A U-shaped pattern in a residuals vs fitted plot always indicates non-linearity.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the regression metric with its interpretation:",
        "type": "match",
        "left": ["R-squared", "Adjusted R-squared", "AIC", "BIC"],
        "right": ["Proportion of variance explained", "Variance explained penalized for predictors", "Balance of fit and complexity", "Strong penalty for model complexity"]
    },
    {
        "q": "The ______ assumption requires that errors are normally distributed around zero.",
        "type": "fill_blank",
        "answers": ["normality"],
        "other_options": ["linearity", "homoscedasticity", "independence"]
    },
    {
        "q": "A point that is both an outlier and has high leverage is called a ______ point.",
        "type": "fill_blank",
        "answers": ["bad leverage"],
        "other_options": ["good leverage", "regular", "typical"]
    },
    {
        "q": "The Durbin-Watson test is used to check for heteroscedasticity in residuals.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these model building steps from foundation to refinement:",
        "type": "rearrange",
        "words": ["Theoretical Framework", "Variable Selection", "Model Estimation", "Diagnostic Checking", "Model Improvement"]
    },
    {
        "q": "When residuals show a systematic pattern rather than random scatter, this suggests potential ______ in the model specification.",
        "type": "fill_blank",
        "answers": ["misspecification"],
        "other_options": ["perfection", "randomness", "completeness"]
    },
    {
        "q": "Rearrange these aspects of model validation from internal to external assessment:",
        "type": "rearrange",
        "words": ["Residual Analysis", "Cross-Validation", "Holdout Testing", "External Dataset"]
    },
    {
        "q": "A ______ point has unusual predictor values but follows the overall pattern of the data.",
        "type": "fill_blank",
        "answers": ["good leverage"],
        "other_options": ["bad leverage", "influential", "outlying"]
    },
    {
        "q": "The ______ curve shows the trade-off between sensitivity and specificity in logistic regression classification.",
        "type": "fill_blank",
        "answers": ["ROC"],
        "other_options": ["precision-recall", "calibration", "lift"]
    },
    {
        "q": "A perfectly straight diagonal line in a Q-Q plot indicates residuals follow a normal distribution exactly.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the regression diagnostic approach with its focus area:",
        "type": "match",
        "left": ["Partial Regression Plots", "Influence Statistics", "Multicollinearity Diagnostics", "Autocorrelation Tests"],
        "right": ["Relationship clarity", "Observation impact", "Predictor interdependence", "Error dependence"]
    },
    {
        "q": "A condition number above 30 indicates substantial ______ in the predictor matrix.",
        "type": "fill_blank",
        "answers": ["multicollinearity"],
        "other_options": ["heteroscedasticity", "autocorrelation", "non-linearity"]
    },
    {
        "q": "The ______ plot displays residuals against the order of data collection to detect time-based patterns.",
        "type": "fill_blank",
        "answers": ["residuals vs order"],
        "other_options": ["Q-Q", "scale-location", "leverage"]
    },
    {
        "q": "In logistic regression, the ______ statistic measures the improvement over a null model.",
        "type": "fill_blank",
        "answers": ["likelihood ratio"],
        "other_options": ["Wald", "score", "deviance"]
    },
    {
        "q": "All outliers in the response variable are high leverage points.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these model comparison approaches from least to most stringent:",
        "type": "rearrange",
        "words": ["R-squared Comparison", "Likelihood Ratio Test", "AIC Difference", "BIC Difference"]
    },
    {
        "q": "The ______ residual accounts for the variability in estimating the error variance.",
        "type": "fill_blank",
        "answers": ["studentized"],
        "other_options": ["standardized", "Pearson", "deviance"]
    },
    {
        "q": "When the variance of errors changes with the level of the predictors, this is called ______.",
        "type": "fill_blank",
        "answers": ["heteroscedasticity"],
        "other_options": ["autocorrelation", "multicollinearity", "endogeneity"]
    },
    {
        "q": "The ______ measure assesses how much the fitted values change when each observation is omitted.",
        "type": "fill_blank",
        "answers": ["DFFITS"],
        "other_options": ["DFBETAS", "leverage", "covratio"]
    },
    {
        "q": "A random scatter in a residuals vs fitted plot guarantees all regression assumptions are satisfied.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the goodness-of-fit indicator with its primary strength:",
        "type": "match",
        "left": ["Adjusted R-squared", "AIC", "Cross-validated R-squared", "BIC"],
        "right": ["Penalizes extra predictors", "Good for prediction", "Estimates generalization", "Strong complexity penalty"]
    },
    {
        "q": "The ______ assumption requires that no important predictors are omitted from the model.",
        "type": "fill_blank",
        "answers": ["specification"],
        "other_options": ["linearity", "independence", "homoscedasticity"]
    },
    {
        "q": "A point that strongly affects the regression coefficients when removed is called an ______ point.",
        "type": "fill_blank",
        "answers": ["influential"],
        "other_options": ["outlying", "leverage", "extreme"]
    },
    {
        "q": "The Breusch-Godfrey test extends the Durbin-Watson test to higher-order autocorrelation.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these regression analysis stages from data-oriented to model-oriented:",
        "type": "rearrange",
        "words": ["Data Cleaning", "Exploratory Analysis", "Model Specification", "Diagnostic Checking", "Interpretation"]
    },
    {
        "q": "A researcher notices that the standard errors of coefficients become extremely large after adding a new variable. This most likely indicates:",
        "type": "mcq",
        "o": [
            "Severe multicollinearity with existing predictors",
            "Perfect model specification achieved",
            "Homoscedasticity of residuals",
            "Excellent goodness of fit"
        ]
    },
    {
        "q": "The ______ effect occurs when a predictor's relationship with the outcome changes direction when other variables are controlled.",
        "type": "fill_blank",
        "answers": ["suppressor"],
        "other_options": ["mediator", "confounding", "interaction"]
    },
    {
        "q": "Match the specialized regression diagnostic with its specific purpose:",
        "type": "match",
        "left": ["Component-Plus-Residual Plot", "Added-Variable Plot", "Marginal Model Plot", "CERES Plot"],
        "right": ["Detects non-linearity in multiple regression", "Shows unique contribution of each predictor", "Compares parametric and nonparametric fits", "Generalized partial residual plot"]
    },
    {
        "q": "In ______ regression, the focus shifts from predicting values to identifying which observations are poorly explained by the model.",
        "type": "fill_blank",
        "answers": ["diagnostic"],
        "other_options": ["predictive", "explanatory", "causal"]
    },
    {
        "q": "The rainbow test checks for non-constant variance by examining residuals from different subsets of the data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these advanced influence detection methods from most specific to most comprehensive:",
        "type": "rearrange",
        "words": ["DFBETAS", "Cook's Distance", "Likelihood Distance", "Welsch Distance"]
    },
    {
        "q": "What does a significant non-constant variance score test indicate about regression assumptions?",
        "type": "mcq",
        "o": [
            "Heteroscedasticity is present in the residuals",
            "All linearity assumptions are perfectly met",
            "Multicollinearity is not a concern",
            "The model has perfect calibration"
        ]
    },
    {
        "q": "The ______ plot displays the confidence bands for the true regression function to assess linearity.",
        "type": "fill_blank",
        "answers": ["confidence band"],
        "other_options": ["prediction interval", "residual", "influence"]
    },
    {
        "q": "Which diagnostic approach is most effective for detecting structural changes in regression relationships?",
        "type": "mcq",
        "o": [
            "CUSUM and CUSUMSQ plots",
            "Simple residual scatter plots",
            "Correlation matrix examination",
            "Variance inflation factors"
        ]
    },
    {
        "q": "A ______ point has extreme values on both predictors and response, making it particularly influential.",
        "type": "fill_blank",
        "answers": ["bad leverage"],
        "other_options": ["good leverage", "regular", "typical"]
    },
    {
        "q": "The Harvey-Collier test specifically checks the linearity assumption using recursive residuals.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the advanced regression concept with its diagnostic application:",
        "type": "match",
        "left": ["Leverage-Variance Tradeoff", "Bias-Variance Decomposition", "Model Misspecification", "Overfitting Detection"],
        "right": ["Influence of individual points", "Prediction error analysis", "Omitted variable checking", "Cross-validation utility"]
    },
    {
        "q": "What pattern in a recursive residual plot would indicate structural instability in the model?",
        "type": "mcq",
        "o": [
            "Systematic deviation from zero mean",
            "Perfect random scatter around zero",
            "Constant variance throughout",
            "All points within confidence bands"
        ]
    },
    {
        "q": "Rearrange these model adequacy checks from assumption-based to prediction-based:",
        "type": "rearrange",
        "words": ["Residual Normality", "Homoscedasticity Check", "Influence Analysis", "Prediction Interval Coverage"]
    },
    {
        "q": "The ______ function in logistic regression ensures predictions stay between 0 and 1 by mapping linear combinations to probabilities.",
        "type": "fill_blank",
        "answers": ["logistic"],
        "other_options": ["probit", "cauchit", "cloglog"]
    },
    {
        "q": "In regression diagnostics, the ______ approach examines how small changes in data affect parameter estimates.",
        "type": "fill_blank",
        "answers": ["local influence"],
        "other_options": ["global influence", "residual analysis", "goodness of fit"]
    },
    {
        "q": "A perfectly calibrated logistic regression model shows predicted probabilities that match observed event rates across all levels.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which situation would make a likelihood displacement measure particularly useful?",
        "type": "mcq",
        "o": [
            "When assessing influence in generalized linear models",
            "When checking for simple linearity violations",
            "When computing basic correlation coefficients",
            "When all assumptions are perfectly met"
        ]
    },
    {
        "q": "Match the specialized test with its regression application:",
        "type": "match",
        "left": ["Rainbow Test", "Harrison-McCabe Test", "RESET Test", "Andrews Test"],
        "right": ["Non-constant variance detection", "Autocorrelation in small samples", "Omitted variable checking", "Structural break identification"]
    },
    {
        "q": "The ______ statistic measures the change in the likelihood function when each observation is removed.",
        "type": "fill_blank",
        "answers": ["likelihood displacement"],
        "other_options": ["Cook's distance", "leverage", "covratio"]
    },
    {
        "q": "When the relationship between a predictor and outcome differs depending on the value of another variable, this indicates an ______ effect.",
        "type": "fill_blank",
        "answers": ["interaction"],
        "other_options": ["additive", "suppressor", "confounding"]
    },
    {
        "q": "Rearrange these regression diagnostic procedures from detecting simple to complex problems:",
        "type": "rearrange",
        "words": ["Residual Plots", "VIF Calculation", "Influence Measures", "Specification Tests"]
    },
    {
        "q": "A point with extreme predictor values that lies close to the regression line is called a ______ point.",
        "type": "fill_blank",
        "answers": ["good leverage"],
        "other_options": ["bad leverage", "influential", "outlier"]
    },
    {
        "q": "The ______ function transforms probabilities to log-odds in binary classification models.",
        "type": "fill_blank",
        "answers": ["logit"],
        "other_options": ["probit", "loglog", "cloglog"]
    },
    {
        "q": "All high leverage points significantly alter the regression coefficients when removed.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the regression violation with its characteristic diagnostic pattern:",
        "type": "match",
        "left": ["Heteroscedasticity", "Autocorrelation", "Non-linearity", "Multicollinearity"],
        "right": ["Fan-shaped residual spread", "Runs of same-sign residuals", "Systematic curved pattern", "Large coefficient standard errors"]
    },
    {
        "q": "A tolerance value near zero indicates serious ______ issues.",
        "type": "fill_blank",
        "answers": ["multicollinearity"],
        "other_options": ["heteroscedasticity", "autocorrelation", "non-linearity"]
    },
    {
        "q": "The ______ plot helps identify observations that have disproportionate impact on the regression results.",
        "type": "fill_blank",
        "answers": ["influence"],
        "other_options": ["residual", "partial", "component"]
    },
    {
        "q": "In logistic regression, the ______ test assesses calibration by comparing predicted and observed probabilities.",
        "type": "fill_blank",
        "answers": ["Hosmer-Lemeshow"],
        "other_options": ["Breusch-Pagan", "Shapiro-Wilk", "Jarque-Bera"]
    },
    {
        "q": "A perfectly horizontal line in a scale-location plot confirms constant error variance.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these model selection criteria from least to most conservative about complexity:",
        "type": "rearrange",
        "words": ["R-squared", "Adjusted R-squared", "AIC", "BIC"]
    },
    {
        "q": "The ______ residual accounts for estimation uncertainty in the error variance.",
        "type": "fill_blank",
        "answers": ["studentized"],
        "other_options": ["standardized", "Pearson", "deviance"]
    },
    {
        "q": "When errors from adjacent observations are correlated, this violates the ______ assumption.",
        "type": "fill_blank",
        "answers": ["independence"],
        "other_options": ["linearity", "homoscedasticity", "normality"]
    },
    {
        "q": "The ______ statistic measures how much a specific coefficient changes when each observation is omitted.",
        "type": "fill_blank",
        "answers": ["DFBETAS"],
        "other_options": ["DFFITS", "leverage", "covratio"]
    },
    {
        "q": "A U-shaped pattern in a residuals vs fitted plot suggests potential non-linearity.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the goodness-of-fit measure with its regression context:",
        "type": "match",
        "left": ["R-squared", "Area Under ROC", "Pseudo R-squared", "Mean Squared Error"],
        "right": ["Linear regression", "Logistic regression", "Generalized linear models", "Prediction accuracy"]
    },
    {
        "q": "The ______ assumption requires that the expected value of errors is zero.",
        "type": "fill_blank",
        "answers": ["zero mean"],
        "other_options": ["constant variance", "normal distribution", "independence"]
    },
    {
        "q": "A point with both high leverage and large residual is called a ______ point.",
        "type": "fill_blank",
        "answers": ["bad leverage"],
        "other_options": ["good leverage", "regular", "typical"]
    },
    {
        "q": "The Breusch-Pagan test detects autocorrelation in time series data.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these regression analysis phases in logical sequence:",
        "type": "rearrange",
        "words": ["Data Preparation", "Model Specification", "Parameter Estimation", "Diagnostic Checking"]
    },
    {
        "q": "When a predictor's effect on the outcome depends on the level of another variable, this represents an ______ term in the model.",
        "type": "fill_blank",
        "answers": ["interaction"],
        "other_options": ["additive", "suppressor", "confounding"]
    },
    {
        "q": "Rearrange these regression diagnostic approaches from visual inspection to formal testing:",
        "type": "rearrange",
        "words": ["Residual Plots", "Influence Measures", "Statistical Tests", "Model Comparison"]
    },
    {
        "q": "A data point with unusual predictor values that doesn't substantially affect the regression line is termed a ______ point.",
        "type": "fill_blank",
        "answers": ["good leverage"],
        "other_options": ["bad leverage", "influential", "outlying"]
    },
    {
        "q": "The ______ function serves as the inverse of the logit transformation in binary outcome models.",
        "type": "fill_blank",
        "answers": ["logistic"],
        "other_options": ["exponential", "probit", "identity"]
    },
    {
        "q": "High leverage points always have large residuals and should be excluded from analysis.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the regression assumption violation with its corresponding diagnostic method:",
        "type": "match",
        "left": ["Non-constant variance", "Correlated errors", "Non-linear relationships", "Predictor interdependence"],
        "right": ["Scale-location plot", "Durbin-Watson test", "Partial residual plot", "Variance inflation factor"]
    },
    {
        "q": "A condition number exceeding 30 suggests substantial ______ among predictors.",
        "type": "fill_blank",
        "answers": ["multicollinearity"],
        "other_options": ["heteroscedasticity", "autocorrelation", "non-linearity"]
    },
    {
        "q": "The ______ plot illustrates the relationship between residuals and individual predictor variables.",
        "type": "fill_blank",
        "answers": ["residuals vs predictors"],
        "other_options": ["Q-Q", "influence", "component"]
    },
    {
        "q": "In logistic regression, the ______ statistic evaluates agreement between predicted probabilities and observed outcomes.",
        "type": "fill_blank",
        "answers": ["Hosmer-Lemeshow"],
        "other_options": ["Pearson chi-square", "Likelihood ratio", "Wald"]
    },
    {
        "q": "A random scatter pattern in a residuals vs fitted plot confirms all regression assumptions are satisfied.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these influence diagnostics from coefficient-specific to overall model impact:",
        "type": "rearrange",
        "words": ["DFBETAS", "DFFITS", "Cook's Distance", "Leverage"]
    },
    {
        "q": "The ______ residual is particularly useful for assessing fit in generalized linear models.",
        "type": "fill_blank",
        "answers": ["deviance"],
        "other_options": ["Pearson", "working", "response"]
    },
    {
        "q": "When repeated measurements are taken on the same subjects, this may violate the ______ assumption.",
        "type": "fill_blank",
        "answers": ["independence"],
        "other_options": ["linearity", "homoscedasticity", "normality"]
    },
    {
        "q": "The ______ measure quantifies changes in the covariance matrix when observations are removed.",
        "type": "fill_blank",
        "answers": ["COVRATIO"],
        "other_options": ["DFBETAS", "DFFITS", "leverage"]
    },
    {
        "q": "A systematic curved pattern in a residuals vs fitted plot always indicates non-linearity.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the model evaluation metric with its key characteristic:",
        "type": "match",
        "left": ["R-squared", "Adjusted R-squared", "AIC", "BIC"],
        "right": ["Proportion of variance explained", "Penalizes additional predictors", "Balances fit and complexity", "Strong penalty for model size"]
    },
    {
        "q": "The ______ assumption requires that error terms follow a normal distribution.",
        "type": "fill_blank",
        "answers": ["normality"],
        "other_options": ["linearity", "homoscedasticity", "independence"]
    },
    {
        "q": "An observation that is both an outlier and has high leverage is classified as a ______ point.",
        "type": "fill_blank",
        "answers": ["bad leverage"],
        "other_options": ["good leverage", "regular", "typical"]
    },
    {
        "q": "The Durbin-Watson test is designed to detect heteroscedasticity in regression residuals.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these regression modeling stages from conceptualization to validation:",
        "type": "rearrange",
        "words": ["Theoretical Framework", "Variable Selection", "Model Estimation", "Diagnostic Checking", "Out-of-Sample Testing"]
    },
    {
        "q": "When a logistic regression model predicts probabilities near 0 or 1 for most observations, this may indicate:",
        "type": "mcq",
        "o": [
            "Complete or quasi-complete separation",
            "Perfect multicollinearity among predictors",
            "Excellent model calibration",
            "Homoscedastic error variance"
        ]
    },
    {
        "q": "The ______ test evaluates whether the variance of residuals changes systematically with predictor values.",
        "type": "fill_blank",
        "answers": ["Breusch-Pagan"],
        "other_options": ["Durbin-Watson", "Shapiro-Wilk", "Jarque-Bera"]
    },
    {
        "q": "Match the specialized regression plot with its specific diagnostic purpose:",
        "type": "match",
        "left": ["CERES Plot", "Marginal Model Plot", "CUSUM Plot", "Bubble Plot"],
        "right": ["Non-linearity detection in multiple regression", "Parametric vs nonparametric comparison", "Structural change identification", "Visualizing influence and leverage"]
    },
    {
        "q": "In ______ analysis, researchers examine how small perturbations in data affect parameter estimates and conclusions.",
        "type": "fill_blank",
        "answers": ["sensitivity"],
        "other_options": ["residual", "influence", "goodness-of-fit"]
    },
    {
        "q": "The rainbow test checks for non-linearity by comparing residuals from different data subsets.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these model validation techniques from simplest to most computationally intensive:",
        "type": "rearrange",
        "words": ["Data Splitting", "Cross-Validation", "Bootstrap Resampling", "External Validation"]
    },
    {
        "q": "What does a significant RESET test indicate about a regression model?",
        "type": "mcq",
        "o": [
            "Omitted variables or incorrect functional form",
            "Perfect multicollinearity among all predictors",
            "Homoscedasticity of error terms",
            "Normally distributed residuals"
        ]
    },
    {
        "q": "The ______ distance measures influence by comparing parameter estimates with and without each observation.",
        "type": "fill_blank",
        "answers": ["Cook's"],
        "other_options": ["Mahalanobis", "Euclidean", "Manhattan"]
    },
    {
        "q": "Which diagnostic approach is most effective for detecting structural breaks in time series regression?",
        "type": "mcq",
        "o": [
            "Chow test for parameter stability",
            "Simple correlation analysis",
            "Variance inflation factors",
            "Normal Q-Q plots"
        ]
    },
    {
        "q": "A ______ residual plot shows the relationship between each predictor and response after removing effects of other variables.",
        "type": "fill_blank",
        "answers": ["partial"],
        "other_options": ["component", "added", "incremental"]
    },
    {
        "q": "The Harvey-Collier test uses recursive residuals to check the linearity assumption.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the advanced regression concept with its diagnostic implication:",
        "type": "match",
        "left": ["Leverage Points", "Influential Observations", "Outliers", "High Cook's Distance"],
        "right": ["Extreme predictor values", "Substantial impact on estimates", "Large prediction errors", "Overall model influence"]
    },
    {
        "q": "What pattern in a CUSUMSQ plot would suggest parameter instability over time?",
        "type": "mcq",
        "o": [
            "Systematic deviation from expected path",
            "Random fluctuation within confidence bands",
            "Perfect horizontal line at zero",
            "All points clustered near the mean"
        ]
    },
    {
        "q": "Rearrange these regression assumption checks from most commonly tested to least commonly tested:",
        "type": "rearrange",
        "words": ["Linearity", "Homoscedasticity", "Normality", "Independence"]
    },
    {
        "q": "The ______ function ensures predicted probabilities remain between 0 and 1 in binary outcome models.",
        "type": "fill_blank",
        "answers": ["logistic"],
        "other_options": ["probit", "cauchit", "cloglog"]
    },
    {
        "q": "In diagnostic analysis, the ______ approach examines how parameter estimates change with data perturbations.",
        "type": "fill_blank",
        "answers": ["local influence"],
        "other_options": ["global influence", "residual analysis", "goodness-of-fit"]
    },
    {
        "q": "A perfectly calibrated model shows predicted probabilities that exactly match observed event rates.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which situation would make a likelihood displacement measure particularly valuable?",
        "type": "mcq",
        "o": [
            "When assessing influence in complex statistical models",
            "When checking basic linearity assumptions",
            "When computing simple correlation coefficients",
            "When all model assumptions are perfectly satisfied"
        ]
    },
    {
        "q": "Match the specialized test with its specific regression application:",
        "type": "match",
        "left": ["Rainbow Test", "Harrison-McCabe Test", "RESET Test", "Andrews Test"],
        "right": ["Non-constant variance detection", "Autocorrelation in small samples", "Omitted variable identification", "Structural break detection"]
    },
    {
        "q": "The ______ statistic quantifies changes in the likelihood function when observations are removed.",
        "type": "fill_blank",
        "answers": ["likelihood displacement"],
        "other_options": ["Cook's distance", "leverage", "covratio"]
    },
    {
        "q": "When the variance of prediction errors differs across levels of predictors, this violates the ______ assumption.",
        "type": "fill_blank",
        "answers": ["homoscedasticity"],
        "other_options": ["linearity", "independence", "normality"]
    },
    {
        "q": "Rearrange these regression diagnostic methods from basic visual to advanced numerical:",
        "type": "rearrange",
        "words": ["Residual Scatter Plots", "Influence Statistics", "Cross-Validation", "Bootstrap Diagnostics"]
    },
    {
        "q": "A data point with extreme predictor values that aligns with the overall trend is called a ______ point.",
        "type": "fill_blank",
        "answers": ["good leverage"],
        "other_options": ["bad leverage", "influential", "outlying"]
    },
    {
        "q": "The ______ transformation converts probabilities to an unbounded scale in binary classification.",
        "type": "fill_blank",
        "answers": ["logit"],
        "other_options": ["probit", "loglog", "arcsine"]
    },
    {
        "q": "High leverage observations always have substantial impact on regression coefficients.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the regression problem with its primary diagnostic indicator:",
        "type": "match",
        "left": ["Non-constant variance", "Autocorrelated errors", "Non-linear patterns", "Predictor correlation"],
        "right": ["Fan-shaped residual plot", "Runs in residual sequence", "Curved residual pattern", "Large VIF values"]
    },
    {
        "q": "A condition index above 30 indicates serious ______ issues in the predictor matrix.",
        "type": "fill_blank",
        "answers": ["multicollinearity"],
        "other_options": ["heteroscedasticity", "autocorrelation", "non-linearity"]
    },
    {
        "q": "The ______ plot displays the relationship between residuals and individual explanatory variables.",
        "type": "fill_blank",
        "answers": ["residuals vs predictors"],
        "other_options": ["Q-Q", "influence", "component"]
    },
    {
        "q": "In logistic regression, the ______ test evaluates the match between predicted and observed outcome frequencies.",
        "type": "fill_blank",
        "answers": ["Hosmer-Lemeshow"],
        "other_options": ["Breusch-Pagan", "Shapiro-Wilk", "Jarque-Bera"]
    },
    {
        "q": "A perfectly horizontal line in a scale-location plot confirms homoscedastic errors.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these model selection criteria from most lenient to most strict about complexity:",
        "type": "rearrange",
        "words": ["R-squared", "Adjusted R-squared", "AIC", "BIC"]
    },
    {
        "q": "The ______ residual incorporates uncertainty in estimating the error variance.",
        "type": "fill_blank",
        "answers": ["studentized"],
        "other_options": ["standardized", "Pearson", "deviance"]
    },
    {
        "q": "When consecutive observations have correlated errors, this breaks the ______ assumption.",
        "type": "fill_blank",
        "answers": ["independence"],
        "other_options": ["linearity", "homoscedasticity", "normality"]
    },
    {
        "q": "The ______ measure assesses changes in specific coefficients when observations are removed.",
        "type": "fill_blank",
        "answers": ["DFBETAS"],
        "other_options": ["DFFITS", "leverage", "covratio"]
    },
    {
        "q": "A systematic U-shaped pattern in residuals suggests potential non-linearity.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the fit statistic with its appropriate model context:",
        "type": "match",
        "left": ["R-squared", "Area Under Curve", "Pseudo R-squared", "Root Mean Square Error"],
        "right": ["Linear regression", "Logistic regression", "Generalized linear models", "Prediction accuracy"]
    },
    {
        "q": "The ______ assumption requires that error terms have zero expected value.",
        "type": "fill_blank",
        "answers": ["zero mean"],
        "other_options": ["constant variance", "normal distribution", "independence"]
    },
    {
        "q": "An observation with high leverage and large residual is classified as a ______ point.",
        "type": "fill_blank",
        "answers": ["bad leverage"],
        "other_options": ["good leverage", "regular", "typical"]
    },
    {
        "q": "The Durbin-Watson test detects heteroscedasticity in regression models.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these modeling stages from initial exploration to final validation:",
        "type": "rearrange",
        "words": ["Exploratory Analysis", "Model Specification", "Parameter Estimation", "Diagnostic Checking", "Out-of-Sample Testing"]
    },
    {
        "q": "When predictor variables are highly correlated with each other, this creates issues with ______ in regression models.",
        "type": "fill_blank",
        "answers": ["multicollinearity"],
        "other_options": ["heteroscedasticity", "autocorrelation", "non-linearity"]
    },
    {
        "q": "Rearrange these regression diagnostic procedures from simplest to most complex implementation:",
        "type": "rearrange",
        "words": ["Residual Plots", "Correlation Matrix", "Influence Measures", "Cross-Validation"]
    },
    {
        "q": "A data point that has extreme values on the predictors but follows the model pattern is a ______ point.",
        "type": "fill_blank",
        "answers": ["good leverage"],
        "other_options": ["bad leverage", "influential", "outlier"]
    },
    {
        "q": "The ______ function maps probabilities to the log-odds scale in binary outcome models.",
        "type": "fill_blank",
        "answers": ["logit"],
        "other_options": ["probit", "loglog", "cauchit"]
    },
    {
        "q": "All observations with high leverage will significantly change the regression results if removed.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the regression assumption violation with its characteristic diagnostic pattern:",
        "type": "match",
        "left": ["Heteroscedasticity", "Autocorrelation", "Non-linearity", "Multicollinearity"],
        "right": ["Fan-shaped residual spread", "Runs of same-sign residuals", "Systematic curved pattern", "Large coefficient variances"]
    },
    {
        "q": "A variance inflation factor above 10 indicates severe ______ problems.",
        "type": "fill_blank",
        "answers": ["multicollinearity"],
        "other_options": ["heteroscedasticity", "autocorrelation", "non-linearity"]
    },
    {
        "q": "The ______ plot helps identify observations that have disproportionate influence on regression estimates.",
        "type": "fill_blank",
        "answers": ["influence"],
        "other_options": ["residual", "partial", "component"]
    },
    {
        "q": "In logistic regression, the ______ test assesses how well predicted probabilities match observed outcomes.",
        "type": "fill_blank",
        "answers": ["Hosmer-Lemeshow"],
        "other_options": ["Breusch-Pagan", "Shapiro-Wilk", "Jarque-Bera"]
    },
    {
        "q": "A random scatter in a residuals vs fitted plot guarantees all regression assumptions are met.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these influence diagnostics from most specific to most general:",
        "type": "rearrange",
        "words": ["DFBETAS", "DFFITS", "Cook's Distance", "Leverage"]
    },
    {
        "q": "The ______ residual accounts for the uncertainty in estimating the error standard deviation.",
        "type": "fill_blank",
        "answers": ["studentized"],
        "other_options": ["standardized", "Pearson", "deviance"]
    },
    {
        "q": "When errors from different observations are correlated, this violates the ______ assumption.",
        "type": "fill_blank",
        "answers": ["independence"],
        "other_options": ["linearity", "homoscedasticity", "normality"]
    },
    {
        "q": "The ______ statistic measures how much a coefficient changes when each observation is deleted.",
        "type": "fill_blank",
        "answers": ["DFBETAS"],
        "other_options": ["DFFITS", "leverage", "covratio"]
    },
    {
        "q": "A curved pattern in a residuals vs fitted plot suggests potential non-linearity.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the goodness-of-fit measure with its regression context:",
        "type": "match",
        "left": ["R-squared", "AUC", "Pseudo R-squared", "MSE"],
        "right": ["Linear regression", "Logistic regression", "GLMs", "Prediction error"]
    },
    {
        "q": "The ______ assumption requires that the expected value of errors is zero.",
        "type": "fill_blank",
        "answers": ["zero mean"],
        "other_options": ["constant variance", "normal distribution", "independence"]
    },
    {
        "q": "A point with both high leverage and large residual is called a ______ point.",
        "type": "fill_blank",
        "answers": ["bad leverage"],
        "other_options": ["good leverage", "regular", "typical"]
    },
    {
        "q": "The Breusch-Pagan test is used to detect autocorrelation in time series data.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these regression analysis steps in proper sequence:",
        "type": "rearrange",
        "words": ["Data Preparation", "Model Specification", "Parameter Estimation", "Diagnostic Checking"]
    },
    {
        "q": "When the effect of one predictor on the outcome changes across levels of another variable, this represents an ______ in the model.",
        "type": "fill_blank",
        "answers": ["interaction"],
        "other_options": ["additive effect", "suppressor effect", "confounding effect"]
    },
    {
        "q": "Rearrange these regression diagnostic approaches from visual examination to statistical testing:",
        "type": "rearrange",
        "words": ["Residual Plots", "Influence Measures", "Statistical Tests", "Model Comparison"]
    },
    {
        "q": "An observation with extreme predictor values that doesn't substantially affect parameter estimates is a ______ point.",
        "type": "fill_blank",
        "answers": ["good leverage"],
        "other_options": ["bad leverage", "influential", "outlying"]
    },
    {
        "q": "The ______ function serves as the inverse transformation of the logit in binary response models.",
        "type": "fill_blank",
        "answers": ["logistic"],
        "other_options": ["exponential", "probit", "identity"]
    },
    {
        "q": "High leverage points always have large residuals and should be removed from analysis.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the regression assumption violation with its corresponding diagnostic method:",
        "type": "match",
        "left": ["Non-constant variance", "Correlated errors", "Non-linear relationships", "Predictor interdependence"],
        "right": ["Scale-location plot", "Durbin-Watson test", "Partial residual plot", "Variance inflation factor"]
    },
    {
        "q": "A tolerance value approaching zero indicates severe ______ among predictors.",
        "type": "fill_blank",
        "answers": ["multicollinearity"],
        "other_options": ["heteroscedasticity", "autocorrelation", "non-linearity"]
    },
    {
        "q": "The ______ plot shows the relationship between residuals and individual explanatory variables.",
        "type": "fill_blank",
        "answers": ["residuals vs predictors"],
        "other_options": ["Q-Q", "influence", "component"]
    },
    {
        "q": "In logistic regression, the ______ statistic evaluates the agreement between predicted probabilities and observed outcomes.",
        "type": "fill_blank",
        "answers": ["Hosmer-Lemeshow"],
        "other_options": ["Pearson chi-square", "Likelihood ratio", "Wald"]
    },
    {
        "q": "A perfectly horizontal line in a scale-location plot confirms constant error variance.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these influence diagnostics from coefficient-specific to overall model impact:",
        "type": "rearrange",
        "words": ["DFBETAS", "DFFITS", "Cook's Distance", "Leverage"]
    },
    {
        "q": "The ______ residual is particularly useful for assessing model fit in generalized linear models.",
        "type": "fill_blank",
        "answers": ["deviance"],
        "other_options": ["Pearson", "working", "response"]
    },
    {
        "q": "When repeated measurements are collected from the same subjects, this may violate the ______ assumption.",
        "type": "fill_blank",
        "answers": ["independence"],
        "other_options": ["linearity", "homoscedasticity", "normality"]
    },
    {
        "q": "The ______ measure quantifies changes in the covariance matrix when observations are deleted.",
        "type": "fill_blank",
        "answers": ["COVRATIO"],
        "other_options": ["DFBETAS", "DFFITS", "leverage"]
    },
    {
        "q": "A systematic curved pattern in a residuals vs fitted plot always indicates non-linearity.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the model evaluation metric with its key characteristic:",
        "type": "match",
        "left": ["R-squared", "Adjusted R-squared", "AIC", "BIC"],
        "right": ["Proportion of variance explained", "Penalizes additional predictors", "Balances fit and complexity", "Strong penalty for model size"]
    },
    {
        "q": "The ______ assumption requires that error terms follow a normal distribution.",
        "type": "fill_blank",
        "answers": ["normality"],
        "other_options": ["linearity", "homoscedasticity", "independence"]
    },
    {
        "q": "An observation that is both an outlier and has high leverage is classified as a ______ point.",
        "type": "fill_blank",
        "answers": ["bad leverage"],
        "other_options": ["good leverage", "regular", "typical"]
    },
    {
        "q": "The Durbin-Watson test is designed to detect heteroscedasticity in regression residuals.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these regression modeling stages from theoretical foundation to empirical validation:",
        "type": "rearrange",
        "words": ["Theoretical Framework", "Variable Selection", "Model Estimation", "Diagnostic Checking", "Out-of-Sample Testing"]
    },
    {
        "q": "When the variance of residuals increases systematically with fitted values, this indicates violation of the ______ assumption.",
        "type": "fill_blank",
        "answers": ["homoscedasticity"],
        "other_options": ["linearity", "independence", "normality"]
    },
    {
        "q": "Rearrange these regression diagnostic techniques from basic to advanced:",
        "type": "rearrange",
        "words": ["Residual Scatterplots", "Correlation Analysis", "Influence Statistics", "Cross-Validation"]
    },
    {
        "q": "A data point with unusual predictor values that doesn't distort the regression line is a ______ point.",
        "type": "fill_blank",
        "answers": ["good leverage"],
        "other_options": ["bad leverage", "influential", "outlier"]
    },
    {
        "q": "The ______ transformation converts probabilities to an unbounded scale for binary classification modeling.",
        "type": "fill_blank",
        "answers": ["logit"],
        "other_options": ["probit", "loglog", "cloglog"]
    },
    {
        "q": "Observations with high leverage always substantially affect regression coefficients when removed.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the regression problem with its characteristic diagnostic signature:",
        "type": "match",
        "left": ["Heteroscedasticity", "Autocorrelation", "Non-linearity", "Multicollinearity"],
        "right": ["Fan-shaped residual pattern", "Sequential correlation in errors", "Systematic curvature in residuals", "High variance inflation factors"]
    },
    {
        "q": "A condition number exceeding 30 suggests serious ______ in the predictor matrix.",
        "type": "fill_blank",
        "answers": ["multicollinearity"],
        "other_options": ["heteroscedasticity", "autocorrelation", "non-linearity"]
    },
    {
        "q": "The ______ plot helps visualize observations that exert disproportionate influence on regression results.",
        "type": "fill_blank",
        "answers": ["influence"],
        "other_options": ["residual", "partial", "component"]
    },
    {
        "q": "In logistic regression, the ______ test assesses calibration by comparing predicted and observed outcome frequencies.",
        "type": "fill_blank",
        "answers": ["Hosmer-Lemeshow"],
        "other_options": ["Breusch-Pagan", "Shapiro-Wilk", "Jarque-Bera"]
    },
    {
        "q": "A perfectly horizontal line in a scale-location plot confirms constant error variance across fitted values.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these model selection criteria from least to most conservative about complexity:",
        "type": "rearrange",
        "words": ["R-squared", "Adjusted R-squared", "AIC", "BIC"]
    },
    {
        "q": "The ______ residual incorporates estimation uncertainty in the error standard deviation.",
        "type": "fill_blank",
        "answers": ["studentized"],
        "other_options": ["standardized", "Pearson", "deviance"]
    },
    {
        "q": "When errors from adjacent observations are correlated, this violates the ______ assumption.",
        "type": "fill_blank",
        "answers": ["independence"],
        "other_options": ["linearity", "homoscedasticity", "normality"]
    },
    {
        "q": "The ______ statistic measures changes in specific coefficients when observations are deleted.",
        "type": "fill_blank",
        "answers": ["DFBETAS"],
        "other_options": ["DFFITS", "leverage", "covratio"]
    },
    {
        "q": "A U-shaped pattern in a residuals vs fitted plot suggests potential non-linearity in the relationship.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the goodness-of-fit measure with its appropriate regression context:",
        "type": "match",
        "left": ["R-squared", "Area Under ROC", "Pseudo R-squared", "Mean Squared Error"],
        "right": ["Linear regression", "Logistic regression", "Generalized linear models", "Prediction accuracy"]
    },
    {
        "q": "The ______ assumption requires that the expected value of error terms is zero.",
        "type": "fill_blank",
        "answers": ["zero mean"],
        "other_options": ["constant variance", "normal distribution", "independence"]
    },
    {
        "q": "A point with both high leverage and large residual is classified as a ______ point.",
        "type": "fill_blank",
        "answers": ["bad leverage"],
        "other_options": ["good leverage", "regular", "typical"]
    },
    {
        "q": "The Breusch-Pagan test detects autocorrelation in time series regression models.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these regression analysis phases in logical order:",
        "type": "rearrange",
        "words": ["Data Preparation", "Model Specification", "Parameter Estimation", "Diagnostic Checking", "Model Validation"]
    },
    {
        "q": "When the relationship between predictors and outcome is not adequately captured by a straight line, this violates the ______ assumption.",
        "type": "fill_blank",
        "answers": ["linearity"],
        "other_options": ["independence", "homoscedasticity", "normality"]
    },
    {
        "q": "Rearrange these regression diagnostic methods from simple visual to complex numerical approaches:",
        "type": "rearrange",
        "words": ["Residual Plots", "Correlation Analysis", "Influence Measures", "Bootstrap Validation"]
    },
    {
        "q": "A data point with extreme predictor values that conforms to the model pattern is a ______ point.",
        "type": "fill_blank",
        "answers": ["good leverage"],
        "other_options": ["bad leverage", "influential", "outlying"]
    },
    {
        "q": "The ______ function transforms probabilities to log-odds for binary outcome modeling.",
        "type": "fill_blank",
        "answers": ["logit"],
        "other_options": ["probit", "loglog", "cauchit"]
    },
    {
        "q": "High leverage observations always have substantial impact on regression parameter estimates.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the regression assumption violation with its primary diagnostic indicator:",
        "type": "match",
        "left": ["Non-constant variance", "Correlated errors", "Non-linear patterns", "Predictor correlation"],
        "right": ["Scale-location plot", "Durbin-Watson statistic", "Partial residual plot", "Variance inflation factor"]
    },
    {
        "q": "A variance inflation factor above 10 indicates severe ______ issues.",
        "type": "fill_blank",
        "answers": ["multicollinearity"],
        "other_options": ["heteroscedasticity", "autocorrelation", "non-linearity"]
    },
    {
        "q": "The ______ plot helps identify observations that disproportionately influence regression results.",
        "type": "fill_blank",
        "answers": ["influence"],
        "other_options": ["residual", "partial", "component"]
    },
    {
        "q": "In logistic regression, the ______ test evaluates the match between predicted probabilities and observed outcomes.",
        "type": "fill_blank",
        "answers": ["Hosmer-Lemeshow"],
        "other_options": ["Breusch-Pagan", "Shapiro-Wilk", "Jarque-Bera"]
    },
    {
        "q": "A random scatter pattern in a residuals vs fitted plot confirms all regression assumptions are satisfied.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these influence diagnostics from most specific to most general impact:",
        "type": "rearrange",
        "words": ["DFBETAS", "DFFITS", "Cook's Distance", "Leverage"]
    },
    {
        "q": "The ______ residual accounts for uncertainty in estimating the error variance.",
        "type": "fill_blank",
        "answers": ["studentized"],
        "other_options": ["standardized", "Pearson", "deviance"]
    },
    {
        "q": "When errors from different observations are not independent, this violates the ______ assumption.",
        "type": "fill_blank",
        "answers": ["independence"],
        "other_options": ["linearity", "homoscedasticity", "normality"]
    },
    {
        "q": "The ______ statistic measures how much individual coefficients change when observations are removed.",
        "type": "fill_blank",
        "answers": ["DFBETAS"],
        "other_options": ["DFFITS", "leverage", "covratio"]
    },
    {
        "q": "A systematic curved pattern in a residuals vs fitted plot suggests potential non-linearity.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the model fit statistic with its appropriate application context:",
        "type": "match",
        "left": ["R-squared", "Area Under Curve", "Pseudo R-squared", "Root Mean Square Error"],
        "right": ["Linear regression", "Logistic regression", "Generalized linear models", "Prediction accuracy"]
    },
    {
        "q": "The ______ assumption requires that error terms have zero expected value.",
        "type": "fill_blank",
        "answers": ["zero mean"],
        "other_options": ["constant variance", "normal distribution", "independence"]
    },
    {
        "q": "An observation with high leverage and large residual is called a ______ point.",
        "type": "fill_blank",
        "answers": ["bad leverage"],
        "other_options": ["good leverage", "regular", "typical"]
    },
    {
        "q": "The Durbin-Watson test is used to detect heteroscedasticity in regression models.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these regression modeling stages from initial to final:",
        "type": "rearrange",
        "words": ["Theoretical Framework", "Variable Selection", "Model Estimation", "Diagnostic Checking", "Validation"]
    },
    {
        "q": "When predictor variables are strongly correlated with each other, this creates ______ problems in regression analysis.",
        "type": "fill_blank",
        "answers": ["multicollinearity"],
        "other_options": ["heteroscedasticity", "autocorrelation", "non-linearity"]
    },
    {
        "q": "Rearrange these regression diagnostic procedures from basic examination to advanced validation:",
        "type": "rearrange",
        "words": ["Residual Analysis", "Influence Checking", "Assumption Testing", "Cross-Validation"]
    },
    {
        "q": "A data point with extreme predictor values that doesn't distort the regression relationship is a ______ point.",
        "type": "fill_blank",
        "answers": ["good leverage"],
        "other_options": ["bad leverage", "influential", "outlier"]
    },
    {
        "q": "The ______ function maps probabilities to the log-odds scale in binary classification models.",
        "type": "fill_blank",
        "answers": ["logit"],
        "other_options": ["probit", "loglog", "cloglog"]
    },
    {
        "q": "All observations with high leverage will significantly change regression coefficients when removed.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the regression problem with its characteristic diagnostic pattern:",
        "type": "match",
        "left": ["Heteroscedasticity", "Autocorrelation", "Non-linearity", "Multicollinearity"],
        "right": ["Fan-shaped residual spread", "Runs of same-sign residuals", "Systematic curved pattern", "Large coefficient standard errors"]
    },
    {
        "q": "A tolerance value near zero indicates serious ______ issues among predictors.",
        "type": "fill_blank",
        "answers": ["multicollinearity"],
        "other_options": ["heteroscedasticity", "autocorrelation", "non-linearity"]
    },
    {
        "q": "The ______ plot helps identify observations that have disproportionate impact on regression estimates.",
        "type": "fill_blank",
        "answers": ["influence"],
        "other_options": ["residual", "partial", "component"]
    },
    {
        "q": "In logistic regression, the ______ test assesses how well predicted probabilities match observed outcomes.",
        "type": "fill_blank",
        "answers": ["Hosmer-Lemeshow"],
        "other_options": ["Breusch-Pagan", "Shapiro-Wilk", "Jarque-Bera"]
    },
    {
        "q": "A perfectly horizontal line in a scale-location plot confirms constant error variance.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these model selection criteria from least to most conservative about complexity:",
        "type": "rearrange",
        "words": ["R-squared", "Adjusted R-squared", "AIC", "BIC"]
    },
    {
        "q": "The ______ residual incorporates uncertainty in estimating the error standard deviation.",
        "type": "fill_blank",
        "answers": ["studentized"],
        "other_options": ["standardized", "Pearson", "deviance"]
    },
    {
        "q": "When errors from consecutive observations are correlated, this violates the ______ assumption.",
        "type": "fill_blank",
        "answers": ["independence"],
        "other_options": ["linearity", "homoscedasticity", "normality"]
    },
    {
        "q": "The ______ statistic measures how much a coefficient changes when each observation is deleted.",
        "type": "fill_blank",
        "answers": ["DFBETAS"],
        "other_options": ["DFFITS", "leverage", "covratio"]
    },
    {
        "q": "A U-shaped pattern in a residuals vs fitted plot suggests potential non-linearity.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the goodness-of-fit measure with its regression context:",
        "type": "match",
        "left": ["R-squared", "AUC", "Pseudo R-squared", "MSE"],
        "right": ["Linear regression", "Logistic regression", "GLMs", "Prediction error"]
    },
    {
        "q": "The ______ assumption requires that the expected value of errors is zero.",
        "type": "fill_blank",
        "answers": ["zero mean"],
        "other_options": ["constant variance", "normal distribution", "independence"]
    },
    {
        "q": "A point with both high leverage and large residual is called a ______ point.",
        "type": "fill_blank",
        "answers": ["bad leverage"],
        "other_options": ["good leverage", "regular", "typical"]
    },
    {
        "q": "The Breusch-Pagan test detects autocorrelation in time series data.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these regression analysis steps in proper sequence:",
        "type": "rearrange",
        "words": ["Data Preparation", "Model Specification", "Parameter Estimation", "Diagnostic Checking"]
    },
    {
        "q": "When the effect of a predictor on the outcome varies depending on the value of another variable, this indicates an ______ effect.",
        "type": "fill_blank",
        "answers": ["interaction"],
        "other_options": ["additive", "suppressor", "confounding"]
    },
    {
        "q": "Rearrange these regression diagnostic approaches from visual inspection to formal statistical testing:",
        "type": "rearrange",
        "words": ["Residual Plots", "Influence Measures", "Statistical Tests", "Model Comparison"]
    },
    {
        "q": "A data point with extreme predictor values that doesn't substantially affect the regression line is termed a ______ point.",
        "type": "fill_blank",
        "answers": ["good leverage"],
        "other_options": ["bad leverage", "influential", "outlying"]
    },
    {
        "q": "The ______ function serves as the inverse transformation of the logit in binary outcome models.",
        "type": "fill_blank",
        "answers": ["logistic"],
        "other_options": ["exponential", "probit", "identity"]
    },
    {
        "q": "High leverage points always have large residuals and should be excluded from analysis.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the regression assumption violation with its corresponding diagnostic method:",
        "type": "match",
        "left": ["Non-constant variance", "Correlated errors", "Non-linear relationships", "Predictor interdependence"],
        "right": ["Scale-location plot", "Durbin-Watson test", "Partial residual plot", "Variance inflation factor"]
    },
    {
        "q": "A condition number exceeding 30 suggests substantial ______ among predictors.",
        "type": "fill_blank",
        "answers": ["multicollinearity"],
        "other_options": ["heteroscedasticity", "autocorrelation", "non-linearity"]
    },
    {
        "q": "The ______ plot illustrates the relationship between residuals and individual predictor variables.",
        "type": "fill_blank",
        "answers": ["residuals vs predictors"],
        "other_options": ["Q-Q", "influence", "component"]
    },
    {
        "q": "In logistic regression, the ______ statistic evaluates agreement between predicted probabilities and observed outcomes.",
        "type": "fill_blank",
        "answers": ["Hosmer-Lemeshow"],
        "other_options": ["Pearson chi-square", "Likelihood ratio", "Wald"]
    },
    {
        "q": "A random scatter pattern in a residuals vs fitted plot confirms all regression assumptions are satisfied.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these influence diagnostics from coefficient-specific to overall model impact:",
        "type": "rearrange",
        "words": ["DFBETAS", "DFFITS", "Cook's Distance", "Leverage"]
    },
    {
        "q": "The ______ residual is particularly useful for assessing fit in generalized linear models.",
        "type": "fill_blank",
        "answers": ["deviance"],
        "other_options": ["Pearson", "working", "response"]
    },
    {
        "q": "When repeated measurements are taken on the same subjects, this may violate the ______ assumption.",
        "type": "fill_blank",
        "answers": ["independence"],
        "other_options": ["linearity", "homoscedasticity", "normality"]
    },
    {
        "q": "The ______ measure quantifies changes in the covariance matrix when observations are removed.",
        "type": "fill_blank",
        "answers": ["COVRATIO"],
        "other_options": ["DFBETAS", "DFFITS", "leverage"]
    },
    {
        "q": "A systematic curved pattern in a residuals vs fitted plot always indicates non-linearity.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the model evaluation metric with its key characteristic:",
        "type": "match",
        "left": ["R-squared", "Adjusted R-squared", "AIC", "BIC"],
        "right": ["Proportion of variance explained", "Penalizes additional predictors", "Balances fit and complexity", "Strong penalty for model size"]
    },
    {
        "q": "The ______ assumption requires that error terms follow a normal distribution.",
        "type": "fill_blank",
        "answers": ["normality"],
        "other_options": ["linearity", "homoscedasticity", "independence"]
    },
    {
        "q": "An observation that is both an outlier and has high leverage is classified as a ______ point.",
        "type": "fill_blank",
        "answers": ["bad leverage"],
        "other_options": ["good leverage", "regular", "typical"]
    },
    {
        "q": "The Durbin-Watson test is designed to detect heteroscedasticity in regression residuals.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these regression modeling stages from conceptualization to validation:",
        "type": "rearrange",
        "words": ["Theoretical Framework", "Variable Selection", "Model Estimation", "Diagnostic Checking", "Out-of-Sample Testing"]
    },
    {
        "q": "A researcher notices that after log-transforming the response variable, the fan-shaped pattern in residuals disappears. This suggests the original model had issues with:",
        "type": "mcq",
        "o": [
            "Heteroscedasticity due to multiplicative errors",
            "Multicollinearity among predictors",
            "Autocorrelation in time series data",
            "Non-linearity in predictor relationships"
        ]
    },
    {
        "q": "The ______ test specifically checks for higher-order autocorrelation beyond first-order in time series regression.",
        "type": "fill_blank",
        "answers": ["Breusch-Godfrey"],
        "other_options": ["Durbin-Watson", "Ljung-Box", "White"]
    },
    {
        "q": "Match the specialized regression diagnostic technique with its unique application:",
        "type": "match",
        "left": ["CERES Plots", "Atkinson's AC Statistic", "Andrews Plots", "Trellis Displays"],
        "right": ["Non-linearity in multiple regression", "Outlier detection in transformed data", "Structural break identification", "Multi-panel residual analysis"]
    },
    {
        "q": "In ______ regression diagnostics, the focus is on how small changes in data values affect parameter estimates and conclusions.",
        "type": "fill_blank",
        "answers": ["local influence"],
        "other_options": ["global influence", "residual", "goodness-of-fit"]
    },
    {
        "q": "The rainbow test for non-linearity compares residuals from the middle portion of data against those from the full dataset.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these advanced influence measures from most sensitive to individual observations to most robust:",
        "type": "rearrange",
        "words": ["DFBETAS", "Likelihood Displacement", "Cook's Distance", "Weighted Influence"]
    },
    {
        "q": "What does a significant non-constant variance score test using studentized residuals indicate?",
        "type": "mcq",
        "o": [
            "Systematic pattern in error variance across predictor space",
            "Perfect normality of residual distribution",
            "Complete independence of all error terms",
            "Optimal model specification achieved"
        ]
    },
    {
        "q": "The ______ plot displays confidence bands around the estimated regression function to assess linearity adequacy.",
        "type": "fill_blank",
        "answers": ["confidence band"],
        "other_options": ["prediction interval", "residual", "influence"]
    },
    {
        "q": "Which diagnostic approach is most effective for detecting parameter instability in recursive estimation?",
        "type": "mcq",
        "o": [
            "CUSUM and CUSUMSQ plots of recursive residuals",
            "Simple correlation matrix examination",
            "Variance inflation factor calculation",
            "Normal Q-Q plots of ordinary residuals"
        ]
    },
    {
        "q": "A ______ point has extreme values on multiple predictors simultaneously, creating high multidimensional leverage.",
        "type": "fill_blank",
        "answers": ["multivariate outlier"],
        "other_options": ["univariate outlier", "influential point", "regular observation"]
    },
    {
        "q": "The Harvey-Collier test uses the sequence of recursive residuals to test the linearity assumption.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the advanced diagnostic concept with its specific application in regression analysis:",
        "type": "match",
        "left": ["Partial Influence Functions", "Generalized Cook's Distance", "Likelihood Displacement", "Local Influence Graph"],
        "right": ["Case influence on specific parameters", "Influence in generalized linear models", "Overall model fit sensitivity", "Visualization of perturbation effects"]
    },
    {
        "q": "What pattern in a recursive residual plot would strongly suggest model misspecification over time?",
        "type": "mcq",
        "o": [
            "Systematic deviation from zero with non-random pattern",
            "Perfect random scatter within confidence limits",
            "All points clustered tightly around the mean",
            "Horizontal line exactly at zero throughout"
        ]
    },
    {
        "q": "Rearrange these model adequacy assessment methods from assumption-based to prediction-focused:",
        "type": "rearrange",
        "words": ["Normality Tests", "Heteroscedasticity Checks", "Influence Analysis", "Prediction Interval Coverage"]
    },
    {
        "q": "The ______ approach in diagnostic analysis examines the effect of perturbing individual observations on model results.",
        "type": "fill_blank",
        "answers": ["case-deletion"],
        "other_options": ["residual-analysis", "goodness-of-fit", "specification-testing"]
    },
    {
        "q": "In ______ regression diagnostics, the focus is on identifying observations that have unusual combinations of predictor values.",
        "type": "fill_blank",
        "answers": ["leverage-based"],
        "other_options": ["residual-based", "influence-based", "goodness-of-fit"]
    },
    {
        "q": "A perfectly specified regression model will always show completely random patterns in all diagnostic plots.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which situation makes the use of robust regression diagnostics particularly important?",
        "type": "mcq",
        "o": [
            "When the data contain influential outliers and heavy-tailed errors",
            "When all model assumptions are perfectly satisfied",
            "When working with perfectly balanced experimental designs",
            "When all predictors are completely uncorrelated"
        ]
    },
    {
        "q": "Match the specialized test with its specific regression diagnostic purpose:",
        "type": "match",
        "left": ["Rainbow Test", "RESET Test", "Chow Test", "M-test"],
        "right": ["Non-linearity detection", "Omitted variable checking", "Structural break identification", "Heteroscedasticity in GLMs"]
    },
    {
        "q": "The ______ statistic measures the change in the determinant of the information matrix when observations are deleted.",
        "type": "fill_blank",
        "answers": ["likelihood displacement"],
        "other_options": ["Cook's distance", "leverage", "covratio"]
    },
    {
        "q": "When the variance of errors changes systematically with the level of predictors, this indicates violation of the ______ assumption.",
        "type": "fill_blank",
        "answers": ["homoscedasticity"],
        "other_options": ["linearity", "independence", "normality"]
    },
    {
        "q": "Rearrange these regression diagnostic procedures from basic visual to advanced numerical methods:",
        "type": "rearrange",
        "words": ["Residual Plots", "Correlation Analysis", "Influence Statistics", "Cross-Validation"]
    },
    {
        "q": "A data point with extreme predictor values that doesn't distort the regression relationship is a ______ point.",
        "type": "fill_blank",
        "answers": ["good leverage"],
        "other_options": ["bad leverage", "influential", "outlier"]
    },
    {
        "q": "The ______ function transforms probabilities to log-odds for binary classification modeling.",
        "type": "fill_blank",
        "answers": ["logit"],
        "other_options": ["probit", "loglog", "cloglog"]
    },
    {
        "q": "High leverage observations always substantially affect regression coefficients when removed.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the regression problem with its characteristic diagnostic pattern:",
        "type": "match",
        "left": ["Heteroscedasticity", "Autocorrelation", "Non-linearity", "Multicollinearity"],
        "right": ["Fan-shaped residual spread", "Runs of same-sign residuals", "Systematic curved pattern", "Large coefficient variances"]
    },
    {
        "q": "A variance inflation factor above 10 indicates severe ______ problems.",
        "type": "fill_blank",
        "answers": ["multicollinearity"],
        "other_options": ["heteroscedasticity", "autocorrelation", "non-linearity"]
    },
    {
        "q": "The ______ plot helps identify observations that have disproportionate influence on regression results.",
        "type": "fill_blank",
        "answers": ["influence"],
        "other_options": ["residual", "partial", "component"]
    },
    {
        "q": "In logistic regression, the ______ test assesses how well predicted probabilities match observed outcomes.",
        "type": "fill_blank",
        "answers": ["Hosmer-Lemeshow"],
        "other_options": ["Breusch-Pagan", "Shapiro-Wilk", "Jarque-Bera"]
    },
    {
        "q": "A perfectly horizontal line in a scale-location plot confirms constant error variance.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these model selection criteria from least to most conservative about complexity:",
        "type": "rearrange",
        "words": ["R-squared", "Adjusted R-squared", "AIC", "BIC"]
    },
    {
        "q": "The ______ residual accounts for uncertainty in estimating the error standard deviation.",
        "type": "fill_blank",
        "answers": ["studentized"],
        "other_options": ["standardized", "Pearson", "deviance"]
    },
    {
        "q": "When errors from consecutive observations are correlated, this violates the ______ assumption.",
        "type": "fill_blank",
        "answers": ["independence"],
        "other_options": ["linearity", "homoscedasticity", "normality"]
    },
    {
        "q": "The ______ statistic measures how much a coefficient changes when each observation is deleted.",
        "type": "fill_blank",
        "answers": ["DFBETAS"],
        "other_options": ["DFFITS", "leverage", "covratio"]
    },
    {
        "q": "A U-shaped pattern in a residuals vs fitted plot suggests potential non-linearity.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the goodness-of-fit measure with its regression context:",
        "type": "match",
        "left": ["R-squared", "AUC", "Pseudo R-squared", "MSE"],
        "right": ["Linear regression", "Logistic regression", "GLMs", "Prediction error"]
    },
    {
        "q": "The ______ assumption requires that the expected value of errors is zero.",
        "type": "fill_blank",
        "answers": ["zero mean"],
        "other_options": ["constant variance", "normal distribution", "independence"]
    },
    {
        "q": "A point with both high leverage and large residual is called a ______ point.",
        "type": "fill_blank",
        "answers": ["bad leverage"],
        "other_options": ["good leverage", "regular", "typical"]
    },
    {
        "q": "The Breusch-Pagan test detects autocorrelation in time series data.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these regression analysis steps in proper sequence:",
        "type": "rearrange",
        "words": ["Data Preparation", "Model Specification", "Parameter Estimation", "Diagnostic Checking"]
    },
    {
        "q": "When the effect of one predictor on the outcome changes across levels of another variable, this represents an ______ in the model.",
        "type": "fill_blank",
        "answers": ["interaction"],
        "other_options": ["additive effect", "suppressor effect", "confounding effect"]
    },
    {
        "q": "Rearrange these regression diagnostic approaches from visual examination to statistical testing:",
        "type": "rearrange",
        "words": ["Residual Plots", "Influence Measures", "Statistical Tests", "Model Comparison"]
    },
    {
        "q": "An observation with extreme predictor values that doesn't substantially affect parameter estimates is a ______ point.",
        "type": "fill_blank",
        "answers": ["good leverage"],
        "other_options": ["bad leverage", "influential", "outlying"]
    },
    {
        "q": "The ______ function serves as the inverse transformation of the logit in binary response models.",
        "type": "fill_blank",
        "answers": ["logistic"],
        "other_options": ["exponential", "probit", "identity"]
    },
    {
        "q": "High leverage points always have large residuals and should be removed from analysis.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the regression assumption violation with its corresponding diagnostic method:",
        "type": "match",
        "left": ["Non-constant variance", "Correlated errors", "Non-linear relationships", "Predictor interdependence"],
        "right": ["Scale-location plot", "Durbin-Watson test", "Partial residual plot", "Variance inflation factor"]
    },
    {
        "q": "A condition number exceeding 30 suggests substantial ______ among predictors.",
        "type": "fill_blank",
        "answers": ["multicollinearity"],
        "other_options": ["heteroscedasticity", "autocorrelation", "non-linearity"]
    },
    {
        "q": "The ______ plot shows the relationship between residuals and individual explanatory variables.",
        "type": "fill_blank",
        "answers": ["residuals vs predictors"],
        "other_options": ["Q-Q", "influence", "component"]
    },
    {
        "q": "In logistic regression, the ______ statistic evaluates the agreement between predicted probabilities and observed outcomes.",
        "type": "fill_blank",
        "answers": ["Hosmer-Lemeshow"],
        "other_options": ["Pearson chi-square", "Likelihood ratio", "Wald"]
    },
    {
        "q": "A perfectly horizontal line in a scale-location plot confirms constant error variance.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these influence diagnostics from coefficient-specific to overall model impact:",
        "type": "rearrange",
        "words": ["DFBETAS", "DFFITS", "Cook's Distance", "Leverage"]
    },
    {
        "q": "The ______ residual is particularly useful for assessing model fit in generalized linear models.",
        "type": "fill_blank",
        "answers": ["deviance"],
        "other_options": ["Pearson", "working", "response"]
    },
    {
        "q": "When repeated measurements are collected from the same subjects, this may violate the ______ assumption.",
        "type": "fill_blank",
        "answers": ["independence"],
        "other_options": ["linearity", "homoscedasticity", "normality"]
    },
    {
        "q": "The ______ measure quantifies changes in the covariance matrix when observations are deleted.",
        "type": "fill_blank",
        "answers": ["COVRATIO"],
        "other_options": ["DFBETAS", "DFFITS", "leverage"]
    },
    {
        "q": "A systematic curved pattern in a residuals vs fitted plot always indicates non-linearity.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the model evaluation metric with its key characteristic:",
        "type": "match",
        "left": ["R-squared", "Adjusted R-squared", "AIC", "BIC"],
        "right": ["Proportion of variance explained", "Penalizes additional predictors", "Balances fit and complexity", "Strong penalty for model size"]
    },
    {
        "q": "The ______ assumption requires that error terms follow a normal distribution.",
        "type": "fill_blank",
        "answers": ["normality"],
        "other_options": ["linearity", "homoscedasticity", "independence"]
    },
    {
        "q": "An observation that is both an outlier and has high leverage is classified as a ______ point.",
        "type": "fill_blank",
        "answers": ["bad leverage"],
        "other_options": ["good leverage", "regular", "typical"]
    },
    {
        "q": "The Durbin-Watson test is designed to detect heteroscedasticity in regression residuals.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these regression modeling stages from theoretical foundation to empirical validation:",
        "type": "rearrange",
        "words": ["Theoretical Framework", "Variable Selection", "Model Estimation", "Diagnostic Checking", "Out-of-Sample Testing"]
    },
    {
        "q": "When the variance of prediction errors differs across levels of predictors, this violates the ______ assumption.",
        "type": "fill_blank",
        "answers": ["homoscedasticity"],
        "other_options": ["linearity", "independence", "normality"]
    },
    {
        "q": "Rearrange these regression diagnostic methods from basic visual to advanced numerical:",
        "type": "rearrange",
        "words": ["Residual Scatter Plots", "Influence Statistics", "Cross-Validation", "Bootstrap Diagnostics"]
    },
    {
        "q": "A data point with extreme predictor values that aligns with the overall trend is called a ______ point.",
        "type": "fill_blank",
        "answers": ["good leverage"],
        "other_options": ["bad leverage", "influential", "outlying"]
    },
    {
        "q": "The ______ transformation converts probabilities to an unbounded scale in binary classification.",
        "type": "fill_blank",
        "answers": ["logit"],
        "other_options": ["probit", "loglog", "arcsine"]
    },
    {
        "q": "High leverage observations always have substantial impact on regression coefficients.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the regression problem with its primary diagnostic indicator:",
        "type": "match",
        "left": ["Non-constant variance", "Autocorrelated errors", "Non-linear patterns", "Predictor correlation"],
        "right": ["Fan-shaped residual plot", "Runs in residual sequence", "Curved residual pattern", "Large VIF values"]
    },
    {
        "q": "A condition index above 30 indicates serious ______ issues in the predictor matrix.",
        "type": "fill_blank",
        "answers": ["multicollinearity"],
        "other_options": ["heteroscedasticity", "autocorrelation", "non-linearity"]
    },
    {
        "q": "The ______ plot displays the relationship between residuals and individual explanatory variables.",
        "type": "fill_blank",
        "answers": ["residuals vs predictors"],
        "other_options": ["Q-Q", "influence", "component"]
    },
    {
        "q": "In logistic regression, the ______ test evaluates the match between predicted and observed outcome frequencies.",
        "type": "fill_blank",
        "answers": ["Hosmer-Lemeshow"],
        "other_options": ["Breusch-Pagan", "Shapiro-Wilk", "Jarque-Bera"]
    },
    {
        "q": "A perfectly horizontal line in a scale-location plot confirms homoscedastic errors.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these model selection criteria from most lenient to most strict about complexity:",
        "type": "rearrange",
        "words": ["R-squared", "Adjusted R-squared", "AIC", "BIC"]
    },
    {
        "q": "The ______ residual incorporates uncertainty in estimating the error variance.",
        "type": "fill_blank",
        "answers": ["studentized"],
        "other_options": ["standardized", "Pearson", "deviance"]
    },
    {
        "q": "When consecutive observations have correlated errors, this breaks the ______ assumption.",
        "type": "fill_blank",
        "answers": ["independence"],
        "other_options": ["linearity", "homoscedasticity", "normality"]
    },
    {
        "q": "The ______ measure assesses changes in specific coefficients when observations are removed.",
        "type": "fill_blank",
        "answers": ["DFBETAS"],
        "other_options": ["DFFITS", "leverage", "covratio"]
    },
    {
        "q": "A systematic U-shaped pattern in residuals suggests potential non-linearity.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the fit statistic with its appropriate model context:",
        "type": "match",
        "left": ["R-squared", "Area Under Curve", "Pseudo R-squared", "Root Mean Square Error"],
        "right": ["Linear regression", "Logistic regression", "Generalized linear models", "Prediction accuracy"]
    },
    {
        "q": "The ______ assumption requires that error terms have zero expected value.",
        "type": "fill_blank",
        "answers": ["zero mean"],
        "other_options": ["constant variance", "normal distribution", "independence"]
    },
    {
        "q": "An observation with high leverage and large residual is classified as a ______ point.",
        "type": "fill_blank",
        "answers": ["bad leverage"],
        "other_options": ["good leverage", "regular", "typical"]
    },
    {
        "q": "The Durbin-Watson test detects heteroscedasticity in regression models.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these modeling stages from initial exploration to final validation:",
        "type": "rearrange",
        "words": ["Exploratory Analysis", "Model Specification", "Parameter Estimation", "Diagnostic Checking", "Out-of-Sample Testing"]
    },
    {
        "q": "A researcher finds that after adding an interaction term, the main effect coefficients change direction. This phenomenon is known as:",
        "type": "mcq",
        "o": [
            "Simpson's paradox",
            "Multicollinearity effect",
            "Heteroscedasticity impact",
            "Autocorrelation bias"
        ]
    },
    {
        "q": "The ______ test evaluates whether the variance of residuals changes with the order of data collection in time series.",
        "type": "fill_blank",
        "answers": ["Goldfeld-Quandt"],
        "other_options": ["Breusch-Pagan", "White", "Harvey"]
    },
    {
        "q": "Match the specialized diagnostic plot with its specific purpose in regression analysis:",
        "type": "match",
        "left": ["Tukey-Anscombe Plot", "Williams Graph", "Atkinson Plot", "Mallow's Plot"],
        "right": ["Residuals vs fitted values", "Added variable relationships", "Forward search diagnostics", "Cp statistic visualization"]
    },
    {
        "q": "In ______ regression diagnostics, the focus is on identifying clusters of observations that jointly influence results.",
        "type": "fill_blank",
        "answers": ["group influence"],
        "other_options": ["individual influence", "residual analysis", "leverage detection"]
    },
    {
        "q": "The rainbow test specifically checks for non-linearity by comparing residuals from different data segments.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these advanced model comparison approaches from least to most computationally intensive:",
        "type": "rearrange",
        "words": ["Likelihood Ratio Test", "Cross-Validation", "Bootstrap Model Comparison", "Bayesian Model Averaging"]
    },
    {
        "q": "What does a significant non-constant variance score test using weighted residuals indicate?",
        "type": "mcq",
        "o": [
            "Systematic pattern in error variance requiring transformation",
            "Perfect linearity in all predictor relationships",
            "Complete independence of all observations",
            "Optimal model specification with no improvements needed"
        ]
    },
    {
        "q": "The ______ plot displays partial residuals against individual predictors to detect non-linearity while controlling for other variables.",
        "type": "fill_blank",
        "answers": ["component-plus-residual"],
        "other_options": ["added-variable", "influence", "leverage"]
    },
    {
        "q": "Which diagnostic method is most effective for detecting structural changes in regression parameters over different data subsets?",
        "type": "mcq",
        "o": [
            "Chow test for parameter stability",
            "Simple correlation analysis",
            "Variance inflation factor examination",
            "Normal probability plots"
        ]
    },
    {
        "q": "A ______ point has extreme values on multiple predictors creating high multidimensional distance from the centroid.",
        "type": "fill_blank",
        "answers": ["multivariate leverage"],
        "other_options": ["univariate outlier", "influential point", "regular observation"]
    },
    {
        "q": "The Harvey-Collier test uses the sequence of recursive residuals to assess model specification adequacy.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the advanced influence concept with its specific measurement approach:",
        "type": "match",
        "left": ["Generalized Influence", "Likelihood Distance", "Local Influence", "Partial Influence"],
        "right": ["Cook's distance extension", "Change in log-likelihood", "Perturbation sensitivity", "Case effect on parameters"]
    },
    {
        "q": "What pattern in a forward search plot would indicate the presence of multiple influential observation groups?",
        "type": "mcq",
        "o": [
            "Multiple sharp changes in parameter trajectories",
            "Smooth gradual parameter evolution",
            "Constant parameter values throughout",
            "Random parameter fluctuations"
        ]
    },
    {
        "q": "Rearrange these residual analysis techniques from assumption checking to influence detection:",
        "type": "rearrange",
        "words": ["Normality Assessment", "Variance Stability", "Autocorrelation Testing", "Influence Measurement"]
    },
    {
        "q": "The ______ function in diagnostic analysis examines how small data perturbations affect specific parameter estimates.",
        "type": "fill_blank",
        "answers": ["local influence"],
        "other_options": ["global influence", "residual analysis", "goodness-of-fit"]
    },
    {
        "q": "In ______ regression diagnostics, the focus shifts to identifying observations with unusual response patterns given their predictor values.",
        "type": "fill_blank",
        "answers": ["outlier detection"],
        "other_options": ["leverage analysis", "influence assessment", "model comparison"]
    },
    {
        "q": "A perfectly specified regression model with correct functional form will show no systematic patterns in partial residual plots.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which situation makes the use of robust regression estimation particularly valuable?",
        "type": "mcq",
        "o": [
            "When data contain influential outliers violating normality",
            "When all classical assumptions are perfectly satisfied",
            "When predictors are completely uncorrelated",
            "When sample size is extremely large"
        ]
    },
    {
        "q": "Match the specialized diagnostic test with its specific regression application:",
        "type": "match",
        "left": ["Rainbow Test", "RESET Test", "Chow Test", "CUSUM Test"],
        "right": ["Non-linearity detection", "Omitted variable checking", "Structural break identification", "Parameter stability monitoring"]
    },
    {
        "q": "The ______ statistic measures the change in the covariance matrix determinant when subsets of observations are deleted.",
        "type": "fill_blank",
        "answers": ["generalized Cook's"],
        "other_options": ["likelihood displacement", "leverage", "studentized residual"]
    },
    {
        "q": "When predictor variables are strongly correlated with each other, this creates ______ problems in regression analysis.",
        "type": "fill_blank",
        "answers": ["multicollinearity"],
        "other_options": ["heteroscedasticity", "autocorrelation", "non-linearity"]
    },
    {
        "q": "Rearrange these regression diagnostic procedures from basic examination to advanced validation:",
        "type": "rearrange",
        "words": ["Residual Analysis", "Influence Checking", "Assumption Testing", "Cross-Validation"]
    },
    {
        "q": "A data point with extreme predictor values that doesn't distort the regression relationship is a ______ point.",
        "type": "fill_blank",
        "answers": ["good leverage"],
        "other_options": ["bad leverage", "influential", "outlier"]
    },
    {
        "q": "The ______ function maps probabilities to the log-odds scale in binary classification models.",
        "type": "fill_blank",
        "answers": ["logit"],
        "other_options": ["probit", "loglog", "cloglog"]
    },
    {
        "q": "All observations with high leverage will significantly change regression coefficients when removed.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the regression problem with its characteristic diagnostic pattern:",
        "type": "match",
        "left": ["Heteroscedasticity", "Autocorrelation", "Non-linearity", "Multicollinearity"],
        "right": ["Fan-shaped residual spread", "Runs of same-sign residuals", "Systematic curved pattern", "Large coefficient standard errors"]
    },
    {
        "q": "A tolerance value near zero indicates serious ______ issues among predictors.",
        "type": "fill_blank",
        "answers": ["multicollinearity"],
        "other_options": ["heteroscedasticity", "autocorrelation", "non-linearity"]
    },
    {
        "q": "The ______ plot helps identify observations that have disproportionate impact on regression estimates.",
        "type": "fill_blank",
        "answers": ["influence"],
        "other_options": ["residual", "partial", "component"]
    },
    {
        "q": "In logistic regression, the ______ test assesses how well predicted probabilities match observed outcomes.",
        "type": "fill_blank",
        "answers": ["Hosmer-Lemeshow"],
        "other_options": ["Breusch-Pagan", "Shapiro-Wilk", "Jarque-Bera"]
    },
    {
        "q": "A perfectly horizontal line in a scale-location plot confirms constant error variance.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these model selection criteria from least to most conservative about complexity:",
        "type": "rearrange",
        "words": ["R-squared", "Adjusted R-squared", "AIC", "BIC"]
    },
    {
        "q": "The ______ residual incorporates uncertainty in estimating the error standard deviation.",
        "type": "fill_blank",
        "answers": ["studentized"],
        "other_options": ["standardized", "Pearson", "deviance"]
    },
    {
        "q": "When errors from consecutive observations are correlated, this violates the ______ assumption.",
        "type": "fill_blank",
        "answers": ["independence"],
        "other_options": ["linearity", "homoscedasticity", "normality"]
    },
    {
        "q": "The ______ statistic measures how much a coefficient changes when each observation is deleted.",
        "type": "fill_blank",
        "answers": ["DFBETAS"],
        "other_options": ["DFFITS", "leverage", "covratio"]
    },
    {
        "q": "A U-shaped pattern in a residuals vs fitted plot suggests potential non-linearity.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the goodness-of-fit measure with its regression context:",
        "type": "match",
        "left": ["R-squared", "AUC", "Pseudo R-squared", "MSE"],
        "right": ["Linear regression", "Logistic regression", "GLMs", "Prediction error"]
    },
    {
        "q": "The ______ assumption requires that the expected value of errors is zero.",
        "type": "fill_blank",
        "answers": ["zero mean"],
        "other_options": ["constant variance", "normal distribution", "independence"]
    },
    {
        "q": "A point with both high leverage and large residual is called a ______ point.",
        "type": "fill_blank",
        "answers": ["bad leverage"],
        "other_options": ["good leverage", "regular", "typical"]
    },
    {
        "q": "The Breusch-Pagan test detects autocorrelation in time series data.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these regression analysis steps in proper sequence:",
        "type": "rearrange",
        "words": ["Data Preparation", "Model Specification", "Parameter Estimation", "Diagnostic Checking"]
    }
]
[
    {
        "q": "What is the primary objective of Principal Component Analysis (PCA)?",
        "type": "mcq",
        "o": [
            "Maximize the variance of the projected data",
            "Minimize the variance of the projected data",
            "Maximize the correlation between all variables",
            "Classify data into predefined groups"
        ]
    },
    {
        "q": "In K-Means clustering, the algorithm iteratively updates the ______ of each cluster to minimize the ______ sum of squares.",
        "type": "fill_blank",
        "answers": [
            "centroids",
            "within-cluster"
        ],
        "other_options": [
            "variances",
            "between-cluster",
            "means",
            "total",
            "outliers",
            "parameters"
        ]
    },
    {
        "q": "Match the multivariate technique with its primary category:",
        "type": "match",
        "left": [
            "Principal Component Analysis",
            "Linear Discriminant Analysis",
            "K-Means Clustering",
            "Factor Analysis"
        ],
        "right": [
            "Unsupervised Dimensionality Reduction",
            "Supervised Classification",
            "Unsupervised Grouping",
            "Latent Variable Modeling"
        ]
    },
    {
        "q": "Linear Discriminant Analysis (LDA) is a supervised learning technique because it uses class labels.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output shape of `X_new` in the following code, assuming `X` has shape (100, 10)?",
        "type": "mcq",
        "c": "from sklearn.decomposition import PCA\n\npca = PCA(n_components=3)\nX_new = pca.fit_transform(X)\nprint(X_new.shape)",
        "o": [
            "(100, 3)",
            "(100, 10)",
            "(3, 100)",
            "(10, 3)"
        ]
    },
    {
        "q": "Order the standard steps for performing Principal Component Analysis:",
        "type": "rearrange",
        "words": [
            "Standardize Data",
            "Compute Covariance",
            "Calculate Eigenvectors",
            "Select Components"
        ]
    },
    {
        "q": "A ______ plot is commonly used in PCA to visualize the explained variance and determine the optimal number of components.",
        "type": "fill_blank",
        "answers": [
            "scree"
        ],
        "other_options": [
            "box",
            "scatter",
            "line",
            "histogram"
        ]
    },
    {
        "q": "Hierarchical clustering requires the user to specify the number of clusters (k) before running the algorithm.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "In Factor Analysis, what does 'communality' represent?",
        "type": "mcq",
        "o": [
            "The proportion of a variable's variance explained by common factors",
            "The variance unique to a specific variable",
            "The correlation between two observed variables",
            "The total error variance in the model"
        ]
    },
    {
        "q": "Match the hierarchical clustering linkage method with its definition:",
        "type": "match",
        "left": [
            "Single Linkage",
            "Complete Linkage",
            "Ward's Method"
        ],
        "right": [
            "Minimum distance between clusters",
            "Maximum distance between clusters",
            "Minimum increase in variance"
        ]
    },
    {
        "q": "Arrange the steps of the K-Means algorithm:",
        "type": "rearrange",
        "words": [
            "Initialize Centroids",
            "Assign Points",
            "Update Centroids",
            "Check Convergence"
        ]
    },
    {
        "q": "LDA assumes that the data in each class is normally distributed and shares a common ______ matrix.",
        "type": "fill_blank",
        "answers": [
            "covariance"
        ],
        "other_options": [
            "correlation",
            "distance",
            "identity",
            "projection"
        ]
    },
    {
        "q": "Which code snippet correctly initializes a K-Means model with 5 clusters?",
        "type": "mcq",
        "o": [
            "kmeans = KMeans(n_clusters=5)",
            "kmeans = KMeans(clusters=5)",
            "kmeans = KMeans(k=5)",
            "kmeans = KMeans(n_groups=5)"
        ],
        "c": "from sklearn.cluster import KMeans"
    },
    {
        "q": "Principal Component Analysis is sensitive to the scale of the features, so data should usually be standardized first.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "The ______ coefficient, also known as the silhouette width, measures how similar an object is to its own cluster compared to other clusters.",
        "type": "fill_blank",
        "answers": [
            "silhouette"
        ],
        "other_options": [
            "correlation",
            "cluster",
            "distance",
            "separation"
        ]
    },
    {
        "q": "Match the term to its description in PCA:",
        "type": "match",
        "left": [
            "Eigenvalue",
            "Eigenvector",
            "Loading",
            "Score"
        ],
        "right": [
            "Amount of variance explained",
            "Direction of the component",
            "Correlation with original variable",
            "Transformed data value"
        ]
    },
    {
        "q": "Which method is best suited for clustering data with arbitrary shapes and handling noise?",
        "type": "mcq",
        "o": [
            "DBSCAN",
            "K-Means",
            "Linear Regression",
            "Factor Analysis"
        ]
    },
    {
        "q": "In Discriminant Analysis, the dependent variable must be continuous.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Arrange the phases of a generic multivariate analysis workflow:",
        "type": "rearrange",
        "words": [
            "Data Cleaning",
            "Check Assumptions",
            "Run Model",
            "Interpret Results"
        ]
    },
    {
        "q": "The purpose of factor ______ (e.g., Varimax) is to simplify the factor structure for better interpretability.",
        "type": "fill_blank",
        "answers": [
            "rotation"
        ],
        "other_options": [
            "extraction",
            "loading",
            "scoring",
            "analysis"
        ]
    },
    {
        "q": "In Principal Component Analysis, the resulting principal components are always mathematically ______ to each other.",
        "type": "fill_blank",
        "answers": [
            "orthogonal"
        ],
        "other_options": [
            "parallel",
            "identical",
            "linear",
            "inverse",
            "proportional"
        ]
    },
    {
        "q": "Which visualization is specifically used to display the arrangement of clusters produced by hierarchical clustering?",
        "type": "mcq",
        "o": [
            "Dendrogram",
            "Histogram",
            "Scatterplot Matrix",
            "Boxplot"
        ]
    },
    {
        "q": "What is the key difference between Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA)?",
        "type": "mcq",
        "o": [
            "QDA allows each class to have its own covariance matrix",
            "LDA works on categorical data only",
            "QDA is a linear transformation technique",
            "LDA does not require labeled data"
        ]
    },
    {
        "q": "Match the distance metric to its characteristic:",
        "type": "match",
        "left": [
            "Euclidean Distance",
            "Manhattan Distance",
            "Mahalanobis Distance",
            "Cosine Similarity"
        ],
        "right": [
            "Straight line distance",
            "Sum of absolute differences",
            "Accounts for data correlation",
            "Measures angle between vectors"
        ]
    },
    {
        "q": "The 'Elbow Method' in K-Means helps identify the optimal number of clusters by observing where the reduction in ______ slows down.",
        "type": "fill_blank",
        "answers": [
            "inertia"
        ],
        "other_options": [
            "accuracy",
            "entropy",
            "precision",
            "bias",
            "correlation"
        ]
    },
    {
        "q": "The Kaiser criterion suggests retaining principal components with eigenvalues greater than 1.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Arrange the steps of Agglomerative Hierarchical Clustering:",
        "type": "rearrange",
        "words": [
            "Treat points as clusters",
            "Compute distance matrix",
            "Merge closest pair",
            "Update distance matrix"
        ]
    },
    {
        "q": "What attribute of the fitted KMeans object contains the cluster index for each sample?",
        "type": "mcq",
        "c": "from sklearn.cluster import KMeans\nmodel = KMeans(n_clusters=3)\nmodel.fit(X)\n# Which attribute holds the results?",
        "o": [
            "model.labels_",
            "model.clusters_",
            "model.results_",
            "model.indices_"
        ]
    },
    {
        "q": "Factor Analysis assumes that observed variables are linear combinations of potential factors plus an error term.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In the context of the Eigen equation 'A v = lambda v', what does 'lambda' represent?",
        "type": "mcq",
        "o": [
            "The eigenvalue",
            "The eigenvector",
            "The covariance matrix",
            "The identity matrix"
        ]
    },
    {
        "q": "Using ______ initialization in K-Means selects initial cluster centers that are far apart to speed up convergence.",
        "type": "fill_blank",
        "answers": [
            "k-means++"
        ],
        "other_options": [
            "random",
            "uniform",
            "zero",
            "linear"
        ]
    },
    {
        "q": "Bartlett's Test of Sphericity is used to check if the observed variables are uncorrelated, which would make them unsuitable for Factor Analysis.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the PCA components to their definitions:",
        "type": "match",
        "left": [
            "First Component",
            "Second Component",
            "Explained Variance",
            "Transformation"
        ],
        "right": [
            "Captures maximum variance",
            "Orthogonal to first component",
            "Information retained",
            "Projecting to new space"
        ]
    },
    {
        "q": "What will be the mean and standard deviation of the data after running this code?",
        "type": "mcq",
        "c": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)",
        "o": [
            "Mean approx 0, Std approx 1",
            "Mean approx 1, Std approx 0",
            "Mean and Std remain unchanged",
            "Mean approx 0.5, Std approx 0.5"
        ]
    },
    {
        "q": "Rearrange the typical order of operations for a classification task using LDA:",
        "type": "rearrange",
        "words": [
            "Split Train/Test",
            "Scale Features",
            "Fit LDA Model",
            "Predict Classes"
        ]
    },
    {
        "q": "In Factor Analysis, the correlation between an observed variable and a factor is called a factor ______.",
        "type": "fill_blank",
        "answers": [
            "loading"
        ],
        "other_options": [
            "score",
            "weight",
            "bias",
            "intercept"
        ]
    },
    {
        "q": "K-Means is a deterministic algorithm, meaning it always produces the exact same clusters regardless of initialization.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which code calculates the percentage of variance explained by each selected component?",
        "type": "mcq",
        "c": "pca = PCA(n_components=2)\npca.fit(X)",
        "o": [
            "pca.explained_variance_ratio_",
            "pca.variance_percent_",
            "pca.components_",
            "pca.get_variance()"
        ]
    },
    {
        "q": "What is the primary difference between Principal Component Analysis (PCA) and Factor Analysis (FA)?",
        "type": "mcq",
        "o": [
            "PCA analyzes total variance; FA analyzes common variance",
            "PCA assumes a normal distribution; FA does not",
            "PCA is supervised; FA is unsupervised",
            "PCA increases dimensionality; FA reduces it"
        ]
    },
    {
        "q": "In K-Means clustering, the algorithm is very sensitive to ______, which can significantly skew the position of centroids.",
        "type": "fill_blank",
        "answers": [
            "outliers"
        ],
        "other_options": [
            "features",
            "iterations",
            "integers",
            "labels"
        ]
    },
    {
        "q": "Match the linkage method in Hierarchical Clustering to its potential drawback:",
        "type": "match",
        "left": [
            "Single Linkage",
            "Complete Linkage",
            "Average Linkage"
        ],
        "right": [
            "Chaining effect (long stringy clusters)",
            "Sensitive to outliers",
            "Bias towards globular clusters"
        ]
    },
    {
        "q": "The maximum number of linear discriminants (axes) typically generated in LDA for a classification problem with 'C' classes and 'p' features is:",
        "type": "mcq",
        "o": [
            "min(C - 1, p)",
            "C + p",
            "C * p",
            "p - 1"
        ]
    },
    {
        "q": "What method is used to combine the fitting and predicting steps into a single call for clustering algorithms?",
        "type": "mcq",
        "c": "from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=3)\n# Which method fits and returns labels immediately?\nlabels = kmeans.______",
        "o": [
            "fit_predict(X)",
            "fit_transform(X)",
            "predict_fit(X)",
            "get_labels(X)"
        ]
    },
    {
        "q": "A Biplot in PCA visualizes both the observations (samples) and the variables (vectors) in the same space.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the logical flow of interpreting a Factor Analysis output:",
        "type": "rearrange",
        "words": [
            "Check KMO Statistic",
            "Extract Factors",
            "Rotate Matrix",
            "Name Factors"
        ]
    },
    {
        "q": "The ______ Test of Sphericity checks the null hypothesis that the variables are uncorrelated (identity matrix).",
        "type": "fill_blank",
        "answers": [
            "Bartlett"
        ],
        "other_options": [
            "Tukey",
            "Fisher",
            "Pearson",
            "Anova"
        ]
    },
    {
        "q": "In Linear Discriminant Analysis, the objective is to maximize the ______ variance while minimizing the within-class variance.",
        "type": "fill_blank",
        "answers": [
            "between-class"
        ],
        "other_options": [
            "total",
            "error",
            "residual",
            "latent"
        ]
    },
    {
        "q": "Which metric is used to evaluate the quality of clustering when ground truth labels are NOT known?",
        "type": "mcq",
        "o": [
            "Silhouette Score",
            "Accuracy Score",
            "Confusion Matrix",
            "F1 Score"
        ]
    },
    {
        "q": "Match the variable requirements to the analysis type:",
        "type": "match",
        "left": [
            "Discriminant Analysis (Dependent Var)",
            "Discriminant Analysis (Independent Var)",
            "Cluster Analysis",
            "Factor Analysis"
        ],
        "right": [
            "Categorical",
            "Metric / Continuous",
            "Metric (for distance calc)",
            "Metric / Correlated"
        ]
    },
    {
        "q": "If you cut a dendrogram horizontally at a specific height, the number of vertical lines intersected represents the number of clusters.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of `pca.n_components_` in this snippet?",
        "type": "mcq",
        "c": "pca = PCA(n_components=0.95)\npca.fit(X_std)\n# Assume X_std has 10 features\n# 0.95 means retain 95% variance",
        "o": [
            "The integer number of components needed",
            "0.95",
            "10",
            "The variance array"
        ]
    },
    {
        "q": "The K-Medoids algorithm (e.g., PAM) is generally more robust to outliers than K-Means because it uses actual data points as centers.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the steps to perform a manual dimensionality reduction selection:",
        "type": "rearrange",
        "words": [
            "Run PCA",
            "Plot Cumulative Variance",
            "Identify Elbow Point",
            "Discard Noise Components"
        ]
    },
    {
        "q": "In Factor Analysis, ______ values indicate the correlation between the original variable and the underlying factor.",
        "type": "fill_blank",
        "answers": [
            "loading"
        ],
        "other_options": [
            "score",
            "eigen",
            "intercept",
            "gamma"
        ]
    },
    {
        "q": "Which assumption is NOT required for standard K-Means clustering?",
        "type": "mcq",
        "o": [
            "Clusters must be of similar density",
            "Clusters are spherical (globular)",
            "Variables should be on the same scale",
            "Data must be normally distributed"
        ]
    },
    {
        "q": "Identify the code that calculates the Mahalanobis distance implies using the covariance matrix inverse.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Gaussian Mixture Models (GMM) differ from K-Means because GMM allows for ______ assignment, meaning a data point belongs to clusters with a certain probability.",
        "type": "fill_blank",
        "answers": [
            "soft"
        ],
        "other_options": [
            "hard",
            "rigid",
            "discrete",
            "binary"
        ]
    },
    {
        "q": "Match the Factor Analysis rotation method to its type:",
        "type": "match",
        "left": [
            "Varimax Rotation",
            "Promax Rotation",
            "Quartimax Rotation",
            "No Rotation"
        ],
        "right": [
            "Orthogonal (axes remain perpendicular)",
            "Oblique (axes can be correlated)",
            "Orthogonal (focus on rows)",
            "Structure is difficult to interpret"
        ]
    },
    {
        "q": "In DBSCAN clustering, the parameter 'eps' (epsilon) represents the:",
        "type": "mcq",
        "o": [
            "Maximum distance between two samples to be neighbors",
            "Minimum number of samples in a neighborhood",
            "Number of clusters to find",
            "The random seed for initialization"
        ]
    },
    {
        "q": "Which Python code snippet correctly imports the dendrogram plotting function?",
        "type": "mcq",
        "c": "# Select the correct import statement",
        "o": [
            "from scipy.cluster.hierarchy import dendrogram",
            "from sklearn.cluster import dendrogram",
            "from matplotlib.pyplot import dendrogram",
            "from pandas.plotting import dendrogram"
        ]
    },
    {
        "q": "Principal Component Analysis completely eliminates multicollinearity in the transformed dataset.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the typical order of a pipeline including dimensionality reduction and classification:",
        "type": "rearrange",
        "words": [
            "StandardScaler",
            "PCA",
            "Classifier (e.g., LogisticReg)",
            "Cross-Validation"
        ]
    },
    {
        "q": "The ______ statistic is used in Discriminant Analysis to test the significance of the discriminant function (i.e., do means differ across groups).",
        "type": "fill_blank",
        "answers": [
            "Wilks' Lambda"
        ],
        "other_options": [
            "R-Squared",
            "F-Score",
            "Gini Coefficient",
            "Pearson r"
        ]
    },
    {
        "q": "Match the algorithm to the Scikit-Learn module it belongs to:",
        "type": "match",
        "left": [
            "PCA",
            "KMeans",
            "LinearDiscriminantAnalysis",
            "FactorAnalysis"
        ],
        "right": [
            "sklearn.decomposition",
            "sklearn.cluster",
            "sklearn.discriminant_analysis",
            "sklearn.decomposition"
        ]
    },
    {
        "q": "K-Means clustering is capable of finding clusters that are non-convex (e.g., ring-shaped or crescent-shaped).",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What attribute represents the coordinates of the cluster centers in a fitted Scikit-Learn KMeans object?",
        "type": "mcq",
        "c": "kmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\n# What stores the centers?",
        "o": [
            "kmeans.cluster_centers_",
            "kmeans.centroids_",
            "kmeans.means_",
            "kmeans.centers_"
        ]
    },
    {
        "q": "In Hierarchical Clustering, ______ clustering starts with one giant cluster containing all points and splits them recursively.",
        "type": "fill_blank",
        "answers": [
            "divisive"
        ],
        "other_options": [
            "agglomerative",
            "density",
            "centroid",
            "partitioning"
        ]
    },
    {
        "q": "If you perform PCA on a correlation matrix of 10 variables, the sum of all eigenvalues will equal:",
        "type": "mcq",
        "o": [
            "10",
            "1",
            "0",
            "The number of samples"
        ]
    },
    {
        "q": "Kernel PCA is a variation of PCA used to perform linear dimensionality reduction on linearly separable data.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Arrange the logic of the DBSCAN algorithm for a specific point:",
        "type": "rearrange",
        "words": [
            "Find neighbors within epsilon",
            "Count neighbors",
            "Check against min_samples",
            "Mark as Core or Noise"
        ]
    },
    {
        "q": "Match the cluster validity index to its goal:",
        "type": "match",
        "left": [
            "Davies-Bouldin Index",
            "Dunn Index",
            "Calinski-Harabasz Index"
        ],
        "right": [
            "Lower is better (similarity between clusters)",
            "Higher is better (compactness vs separation)",
            "Higher is better (variance ratio)"
        ]
    },
    {
        "q": "In Linear Discriminant Analysis, 'priors' refers to the ______ probability of each class before observing the data.",
        "type": "fill_blank",
        "answers": [
            "prior"
        ],
        "other_options": [
            "posterior",
            "conditional",
            "marginal",
            "joint"
        ]
    },
    {
        "q": "What is the output of this PCA inverse transform operation?",
        "type": "mcq",
        "c": "pca = PCA(n_components=2)\nX_reduced = pca.fit_transform(X)\nX_original_approx = pca.inverse_transform(X_reduced)\n# X_original_approx shape is:",
        "o": [
            "Same as original X",
            "Same as X_reduced",
            "(n_samples, 2)",
            "(2, n_features)"
        ]
    },
    {
        "q": "Which clustering approach is often referred to as a 'bottom-up' strategy?",
        "type": "mcq",
        "o": [
            "Agglomerative Hierarchical Clustering",
            "Divisive Hierarchical Clustering",
            "K-Means Clustering",
            "Gaussian Mixture Models"
        ]
    },
    {
        "q": "In Principal Component Analysis, the sum of all eigenvalues of the covariance matrix is equal to the ______ variance of the original dataset.",
        "type": "fill_blank",
        "answers": [
            "total"
        ],
        "other_options": [
            "explained",
            "mean",
            "residual",
            "partial"
        ]
    },
    {
        "q": "Match the type of variance in Factor Analysis to its definition:",
        "type": "match",
        "left": [
            "Common Variance",
            "Specific Variance",
            "Error Variance",
            "Total Variance"
        ],
        "right": [
            "Shared by all variables",
            "Unique to a specific variable",
            "Due to measurement unreliability",
            "Sum of common, specific, and error"
        ]
    },
    {
        "q": "Unlike PCA which creates new variables, Linear Discriminant Analysis tries to find axes that maximize the separation between ______.",
        "type": "fill_blank",
        "answers": [
            "classes"
        ],
        "other_options": [
            "features",
            "outliers",
            "variances",
            "errors"
        ]
    },
    {
        "q": "What is the shape of the `components_` attribute in a fitted PCA model with 'n' components and 'p' features?",
        "type": "mcq",
        "c": "pca = PCA(n_components=n)\npca.fit(X) # X has p features\n# shape of pca.components_",
        "o": [
            "(n, p)",
            "(p, n)",
            "(n, n)",
            "(p, p)"
        ]
    },
    {
        "q": "K-Means guarantees finding the global minimum of the Within-Cluster Sum of Squares (WCSS) every time.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange the standard internal logic of one K-Means iteration:",
        "type": "rearrange",
        "words": [
            "Calculate distance to centroids",
            "Assign points to nearest cluster",
            "Calculate new mean of points",
            "Update centroid positions"
        ]
    },
    {
        "q": "The ______ distance is a generalization that includes both Euclidean and Manhattan distance as special cases.",
        "type": "fill_blank",
        "answers": [
            "Minkowski"
        ],
        "other_options": [
            "Mahalanobis",
            "Hamming",
            "Chebyshev",
            "Canberra"
        ]
    },
    {
        "q": "Match the clustering algorithm to its underlying method:",
        "type": "match",
        "left": [
            "K-Means",
            "DBSCAN",
            "Agglomerative",
            "Gaussian Mixture"
        ],
        "right": [
            "Partitioning",
            "Density-based",
            "Hierarchical",
            "Probabilistic / Model-based"
        ]
    },
    {
        "q": "PCA is strictly a feature selection technique because it drops variables from the dataset.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which Scipy function is specifically used to generate the linkage matrix for hierarchical clustering?",
        "type": "mcq",
        "c": "from scipy.cluster.hierarchy import linkage, fcluster\n# Which function creates the matrix Z?",
        "o": [
            "linkage",
            "fcluster",
            "pdist",
            "dendrogram"
        ]
    },
    {
        "q": "In Ward's method of hierarchical clustering, the distance between two clusters is defined by the increase in ______ that results from merging them.",
        "type": "fill_blank",
        "answers": [
            "variance"
        ],
        "other_options": [
            "correlation",
            "covariance",
            "density",
            "entropy"
        ]
    },
    {
        "q": "If two variables are perfectly correlated (correlation = 1), PCA will produce an eigenvalue of 0 for one of the components.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the steps to preprocess data specifically for distance-based clustering:",
        "type": "rearrange",
        "words": [
            "Handle missing values",
            "Encode categorical vars",
            "Standardize (Scale) features",
            "Apply Clustering"
        ]
    },
    {
        "q": "What does the vertical axis (y-axis) of a standard dendrogram typically represent?",
        "type": "mcq",
        "o": [
            "Distance or dissimilarity at which merge occurs",
            "The number of clusters",
            "The number of samples",
            "The probability of class membership"
        ]
    },
    {
        "q": "Match the matrix operation to its concept in Multivariate Analysis:",
        "type": "match",
        "left": [
            "Determinant",
            "Trace",
            "Inverse",
            "Transpose"
        ],
        "right": [
            "Generalized variance",
            "Total variance (sum of diagonal)",
            "Reciprocal (matrix division)",
            "Swapping rows and columns"
        ]
    },
    {
        "q": "In Python's Scikit-Learn, which method would you use to apply an existing PCA model to new, unseen data without re-calibrating the components?",
        "type": "mcq",
        "c": "pca = PCA(n_components=2).fit(X_train)",
        "o": [
            "pca.transform(X_test)",
            "pca.fit_transform(X_test)",
            "pca.predict(X_test)",
            "pca.fit(X_test)"
        ]
    },
    {
        "q": "Factor Analysis is primarily used to uncover ______ variables that explain the pattern of correlations within a set of observed variables.",
        "type": "fill_blank",
        "answers": [
            "latent"
        ],
        "other_options": [
            "categorical",
            "discrete",
            "target",
            "dummy"
        ]
    },
    {
        "q": "What is the primary function of the 'whiten=True' parameter in Scikit-Learn's PCA?",
        "type": "mcq",
        "o": [
            "Scale components to have unit variance",
            "Remove missing values",
            "Force all coefficients to be positive",
            "Rotate the axes by 45 degrees"
        ]
    },
    {
        "q": "In K-Means clustering, the ______ method calculates the mean distance between a data point and all other points in the same cluster.",
        "type": "fill_blank",
        "answers": [
            "cohesion"
        ],
        "other_options": [
            "separation",
            "linkage",
            "entropy",
            "silhouette"
        ]
    },
    {
        "q": "Match the distance metric to the data type it is best suited for:",
        "type": "match",
        "left": [
            "Hamming Distance",
            "Euclidean Distance",
            "Jaccard Similarity"
        ],
        "right": [
            "Binary / Categorical strings",
            "Continuous numeric data",
            "Set overlap / asymmetric binary"
        ]
    },
    {
        "q": "Agglomerative Hierarchical Clustering has a time complexity of approximately O(n cubed) or O(n squared), making it computationally expensive for very large datasets.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which Scikit-Learn attribute helps you identify how much each original feature contributes to a specific Principal Component?",
        "type": "mcq",
        "c": "pca = PCA(n_components=2).fit(X)",
        "o": [
            "pca.components_",
            "pca.explained_variance_",
            "pca.singular_values_",
            "pca.mean_"
        ]
    },
    {
        "q": "Arrange the steps to calculate the Silhouette Score for a single point:",
        "type": "rearrange",
        "words": [
            "Calc mean distance to own cluster (a)",
            "Calc mean distance to nearest neighbor cluster (b)",
            "Subtract a from b",
            "Divide by max(a, b)"
        ]
    },
    {
        "q": "Principal Component Analysis is fundamentally based on the ______ Value Decomposition (SVD) of the data matrix.",
        "type": "fill_blank",
        "answers": [
            "Singular"
        ],
        "other_options": [
            "Eigen",
            "Spectral",
            "Cholesky",
            "QR"
        ]
    },
    {
        "q": "In Linear Discriminant Analysis, if the number of features (p) exceeds the number of samples (n), the covariance matrix becomes singular and cannot be inverted.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the clustering parameter to the algorithm:",
        "type": "match",
        "left": [
            "eps (epsilon)",
            "linkage criterion",
            "n_components",
            "perplexity"
        ],
        "right": [
            "DBSCAN",
            "Hierarchical Clustering",
            "GMM / PCA",
            "t-SNE"
        ]
    },
    {
        "q": "Which clustering algorithm relies on the concept of 'medoids' (actual data points) rather than 'centroids' (averages)?",
        "type": "mcq",
        "o": [
            "PAM (Partitioning Around Medoids)",
            "K-Means",
            "Ward's Method",
            "Gaussian Mixture"
        ]
    },
    {
        "q": "When performing Factor Analysis, ______ rotation allows the factors to be correlated, which is often more realistic in social sciences.",
        "type": "fill_blank",
        "answers": [
            "oblique"
        ],
        "other_options": [
            "orthogonal",
            "varimax",
            "quartimax",
            "identity"
        ]
    },
    {
        "q": "What does the `fit` method do in the context of K-Means compared to `predict`?",
        "type": "mcq",
        "c": "model = KMeans(n_clusters=3)\nmodel.fit(X_train)\nmodel.predict(X_test)",
        "o": [
            "Fit calculates centroids; Predict assigns clusters",
            "Fit assigns clusters; Predict calculates centroids",
            "They do the exact same thing",
            "Fit is for supervised data only"
        ]
    },
    {
        "q": "The Cophenetic Correlation Coefficient is used to evaluate how well a dendrogram preserves the pairwise distances between the original data points.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the typical hierarchy of a 'Divisive' clustering approach:",
        "type": "rearrange",
        "words": [
            "Start with one cluster",
            "Identify cluster with max diameter",
            "Split into sub-clusters",
            "Repeat until singleton"
        ]
    },
    {
        "q": "Match the PCA variant to its use case:",
        "type": "match",
        "left": [
            "Standard PCA",
            "Kernel PCA",
            "Sparse PCA",
            "Incremental PCA"
        ],
        "right": [
            "Linear dimensionality reduction",
            "Non-linear dimensionality reduction",
            "Components with many zero coefficients",
            "Datasets that don't fit in memory"
        ]
    },
    {
        "q": "In the context of outlier detection using PCA, samples with a high ______ error (distance from the principal subspace) are considered anomalies.",
        "type": "fill_blank",
        "answers": [
            "reconstruction"
        ],
        "other_options": [
            "classification",
            "standard",
            "marginal",
            "absolute"
        ]
    },
    {
        "q": "Which code snippet correctly instantiates a Factor Analysis model?",
        "type": "mcq",
        "c": "# Choose the correct sklearn class",
        "o": [
            "from sklearn.decomposition import FactorAnalysis",
            "from sklearn.cluster import FactorAnalysis",
            "from sklearn.discriminant_analysis import FactorAnalysis",
            "from sklearn.manifold import FactorAnalysis"
        ]
    },
    {
        "q": "The 'Curse of Dimensionality' suggests that as the number of dimensions increases, the distance between any two points tends to become equal, making distance-based clustering less effective.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In K-Means clustering, the 'n_init' parameter in Scikit-Learn controls:",
        "type": "mcq",
        "o": [
            "Number of times the algorithm runs with different centroid seeds",
            "Number of clusters to generate",
            "Number of iterations for a single run",
            "The tolerance for convergence"
        ]
    },
    {
        "q": "Principal Component Analysis is sensitive to outliers because it relies on the ______ matrix, which involves squaring deviations from the mean.",
        "type": "fill_blank",
        "answers": [
            "covariance"
        ],
        "other_options": [
            "identity",
            "correlation",
            "projection",
            "distance"
        ]
    },
    {
        "q": "Match the concept to the appropriate Multivariate Analysis technique:",
        "type": "match",
        "left": [
            "Fisher's Linear Discriminant",
            "Scree Plot",
            "Linkage Matrix",
            "Latent Constructs"
        ],
        "right": [
            "Discriminant Analysis",
            "PCA / Factor Analysis",
            "Hierarchical Clustering",
            "Factor Analysis"
        ]
    },
    {
        "q": "If you set 'n_components' equal to the number of features in PCA, the amount of variance explained will be 100%.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which Scikit-Learn class allows for partial computations of PCA, useful for datasets that do not fit in memory?",
        "type": "mcq",
        "c": "from sklearn.decomposition import ______",
        "o": [
            "IncrementalPCA",
            "BatchPCA",
            "MiniBatchPCA",
            "SparsePCA"
        ]
    },
    {
        "q": "Rearrange the sequence of operations for Agglomerative Clustering using a distance matrix:",
        "type": "rearrange",
        "words": [
            "Compute pairwise distances",
            "Find min distance",
            "Merge clusters",
            "Recalculate distances"
        ]
    },
    {
        "q": "In Factor Analysis, the term ______ refers to the portion of variance in a variable that is NOT shared with other variables (Specific Variance + Error Variance).",
        "type": "fill_blank",
        "answers": [
            "uniqueness"
        ],
        "other_options": [
            "communality",
            "eigenvalue",
            "loading",
            "complexity"
        ]
    },
    {
        "q": "Which code snippet correctly retrieves the mean of each feature from a fitted PCA object?",
        "type": "mcq",
        "c": "pca = PCA().fit(X)\n# Retrieve feature means",
        "o": [
            "pca.mean_",
            "pca.means_",
            "pca.feature_means_",
            "pca.center_"
        ]
    },
    {
        "q": "LDA is a generative model, meaning it models the distribution of the data for each class.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the clustering algorithm to its specific geometric shape preference:",
        "type": "match",
        "left": [
            "K-Means",
            "DBSCAN",
            "Gaussian Mixture"
        ],
        "right": [
            "Spherical / Globular",
            "Arbitrary / Irregular",
            "Ellipsoidal"
        ]
    },
    {
        "q": "The ______ Criterion is a stopping rule for Hierarchical Clustering that suggests cutting the dendrogram where the gap between merger levels is largest.",
        "type": "fill_blank",
        "answers": [
            "Gap"
        ],
        "other_options": [
            "Elbow",
            "Silhouette",
            "Distance",
            "Ratio"
        ]
    },
    {
        "q": "What is the result of applying LDA to a dataset with 2 classes?",
        "type": "mcq",
        "o": [
            "1 Discriminant Axis",
            "2 Discriminant Axes",
            "0 Discriminant Axes",
            "Depends on feature count"
        ]
    },
    {
        "q": "Exploratory Factor Analysis (EFA) requires the user to pre-specify exactly which variables belong to which factors.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange the lifecycle of a K-Means model in Scikit-Learn:",
        "type": "rearrange",
        "words": [
            "Import KMeans",
            "Instantiate with k",
            "Call fit()",
            "Access labels_"
        ]
    },
    {
        "q": "In the context of Hierarchical Clustering, the cophenetic correlation coefficient is used to validate how faithfully the ______ represents the original pairwise distances.",
        "type": "fill_blank",
        "answers": [
            "dendrogram"
        ],
        "other_options": [
            "centroid",
            "eigenvector",
            "projection",
            "scatter"
        ]
    },
    {
        "q": "Which mathematical technique is primarily used to solve the optimization problem in PCA?",
        "type": "mcq",
        "o": [
            "Lagrange Multipliers",
            "Gradient Descent",
            "Genetic Algorithms",
            "Bayesian Inference"
        ]
    },
    {
        "q": "Match the input data format to the analysis requirement:",
        "type": "match",
        "left": [
            "K-Means Input",
            "Hierarchical Input",
            "LDA Input"
        ],
        "right": [
            "Raw Features (usually scaled)",
            "Distance Matrix OR Raw Features",
            "Features + Class Labels"
        ]
    },
    {
        "q": "If the variables in a dataset are completely independent (uncorrelated), PCA is highly effective at reducing dimensionality.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What is the shape of the transformed data `X_lda` in this code?",
        "type": "mcq",
        "c": "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n# X has 100 samples, 5 features\n# y has 3 unique classes\nlda = LinearDiscriminantAnalysis()\nX_lda = lda.fit_transform(X, y)",
        "o": [
            "(100, 2)",
            "(100, 3)",
            "(100, 5)",
            "(100, 1)"
        ]
    },
    {
        "q": "Which variation of K-Means is optimized to handle very large datasets by using small, random batches of data in each iteration?",
        "type": "mcq",
        "o": [
            "MiniBatchKMeans",
            "IncrementalKMeans",
            "SparseKMeans",
            "BatchKMeans"
        ]
    },
    {
        "q": "In Scikit-Learn, the ______ Index is a metric used to measure the similarity between two clusterings, adjusted for chance.",
        "type": "fill_blank",
        "answers": [
            "Adjusted Rand"
        ],
        "other_options": [
            "Silhouette",
            "Jaccard",
            "F1",
            "Mutual Info"
        ]
    },
    {
        "q": "Match the clustering algorithm to its scalability characteristic:",
        "type": "match",
        "left": [
            "K-Means",
            "Hierarchical (Agglomerative)",
            "DBSCAN",
            "MiniBatchKMeans"
        ],
        "right": [
            "Scale linearly with N (fast)",
            "Scales quadratically/cubically with N (slow)",
            "Good for spatial databases",
            "Fastest for massive data"
        ]
    },
    {
        "q": "Confirmatory Factor Analysis (CFA) differs from Exploratory Factor Analysis (EFA) because in CFA, the researcher specifies the model structure beforehand.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the primary role of the `svd_solver='randomized'` parameter in PCA?",
        "type": "mcq",
        "o": [
            "To approximate the first few components quickly",
            "To remove random noise from data",
            "To shuffle the dataset before fitting",
            "To select random features"
        ]
    },
    {
        "q": "Rearrange the steps of Spectral Clustering:",
        "type": "rearrange",
        "words": [
            "Construct Affinity Matrix",
            "Compute Laplacian",
            "Compute Eigenvectors",
            "Run K-Means on vectors"
        ]
    },
    {
        "q": "The ______ coefficient ranges from -1 to 1, where values near +1 indicate that the sample is far away from the neighboring clusters.",
        "type": "fill_blank",
        "answers": [
            "Silhouette"
        ],
        "other_options": [
            "Pearson",
            "Spearman",
            "Rand",
            "Gini"
        ]
    },
    {
        "q": "Which Scikit-Learn method allows you to transform data using a fitted LDA model?",
        "type": "mcq",
        "c": "lda = LinearDiscriminantAnalysis().fit(X, y)\n# Apply transformation",
        "o": [
            "lda.transform(X)",
            "lda.predict_proba(X)",
            "lda.decision_function(X)",
            "lda.score(X, y)"
        ]
    },
    {
        "q": "Mean Shift clustering requires the user to specify the number of clusters (k) in advance.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the metric to what it measures in cluster evaluation (Ground Truth available):",
        "type": "match",
        "left": [
            "Homogeneity",
            "Completeness",
            "V-Measure"
        ],
        "right": [
            "Each cluster contains only members of a single class",
            "All members of a given class are assigned to the same cluster",
            "Harmonic mean of Homogeneity and Completeness"
        ]
    },
    {
        "q": "In Principal Component Analysis, if the original variables are measured in different units (e.g., kg vs meters), the result will be biased towards variables with ______ variance.",
        "type": "fill_blank",
        "answers": [
            "higher"
        ],
        "other_options": [
            "lower",
            "zero",
            "equal",
            "negative"
        ]
    },
    {
        "q": "What is the output of `model.n_iter_` in K-Means?",
        "type": "mcq",
        "c": "model = KMeans(max_iter=300)\nmodel.fit(X)\nprint(model.n_iter_)",
        "o": [
            "Number of iterations run to converge",
            "300",
            "Number of clusters",
            "Number of samples"
        ]
    },
    {
        "q": "Quadratic Discriminant Analysis (QDA) is generally more flexible than LDA but requires estimating more parameters, making it prone to overfitting on small datasets.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the typical steps for choosing 'k' using the Silhouette Method:",
        "type": "rearrange",
        "words": [
            "Loop through range of k",
            "Fit KMeans for each k",
            "Calculate avg silhouette score",
            "Pick k with max score"
        ]
    },
    {
        "q": "The ______ matrix is used in Hierarchical Clustering to store the distance between every pair of observations.",
        "type": "fill_blank",
        "answers": [
            "dissimilarity"
        ],
        "other_options": [
            "covariance",
            "confusion",
            "identity",
            "projection"
        ]
    },
    {
        "q": "In Factor Analysis, if the KMO (Kaiser-Meyer-Olkin) test yields a value below 0.5, it indicates that:",
        "type": "mcq",
        "o": [
            "The data is not suitable for Factor Analysis",
            "The data is perfect for Factor Analysis",
            "You should use PCA instead",
            "There are too many outliers"
        ]
    },
    {
        "q": "Match the term to its definition in Discriminant Analysis:",
        "type": "match",
        "left": [
            "Centroid",
            "Discriminant Score",
            "Cutoff Value"
        ],
        "right": [
            "Mean vector of a group",
            "Value resulting from the discriminant function",
            "Threshold for classifying cases"
        ]
    },
    {
        "q": "Which attribute in a fitted PCA object tells you how much information (variance) is lost by dropping components?",
        "type": "mcq",
        "c": "pca = PCA(n_components=0.9)\npca.fit(X)",
        "o": [
            "1.0 - sum(pca.explained_variance_ratio_)",
            "pca.noise_variance_",
            "pca.singular_values_",
            "pca.loss_"
        ]
    },
    {
        "q": "Single linkage (nearest neighbor) in hierarchical clustering tends to produce compact, spherical clusters.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which Scikit-Learn PCA parameter allows you to use a specific algorithm like 'randomized' or 'arpack' for decomposition?",
        "type": "mcq",
        "o": [
            "svd_solver",
            "solver",
            "algorithm",
            "kernel"
        ]
    },
    {
        "q": "In Hierarchical Clustering, the ______ matrix contains the pairwise distances between all samples and is the primary input for generating the dendrogram.",
        "type": "fill_blank",
        "answers": [
            "linkage"
        ],
        "other_options": [
            "affinity",
            "covariance",
            "identity",
            "projection"
        ]
    },
    {
        "q": "Match the clustering algorithm to its main advantage:",
        "type": "match",
        "left": [
            "Affinity Propagation",
            "Spectral Clustering",
            "MiniBatchKMeans",
            "OPTICS"
        ],
        "right": [
            "Does not require number of clusters (k)",
            "Handles non-linear cluster shapes",
            "Speed on large datasets",
            "Handles varying density better than DBSCAN"
        ]
    },
    {
        "q": "Linear Discriminant Analysis can be used for dimensionality reduction even if the final goal is not classification.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the expected output of this code regarding the number of features?",
        "type": "mcq",
        "c": "from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import PCA\n# Assume sparse input\npca = PCA(n_components=2)\n# Sparse data usually requires:",
        "o": [
            "TruncatedSVD instead of PCA",
            "PCA with dense=True",
            "Standard PCA works fine",
            "KernelPCA only"
        ]
    },
    {
        "q": "Rearrange the steps to interpret a Biplot:",
        "type": "rearrange",
        "words": [
            "Observe points (scores)",
            "Observe vectors (loadings)",
            "Check angle between vectors",
            "Assess point-vector projection"
        ]
    },
    {
        "q": "In K-Means, the sum of squared distances of samples to their closest cluster center is often called the ______.",
        "type": "fill_blank",
        "answers": [
            "inertia"
        ],
        "other_options": [
            "entropy",
            "variance",
            "bias",
            "residual"
        ]
    },
    {
        "q": "Which Factor Analysis method is most appropriate when the data is not normally distributed?",
        "type": "mcq",
        "o": [
            "Principal Axis Factoring",
            "Maximum Likelihood",
            "Least Squares",
            "Image Factoring"
        ]
    },
    {
        "q": "The 'shrinkage' parameter in Linear Discriminant Analysis is useful when the number of samples is small compared to the number of features.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the matrix shape to the PCA attribute (where n=samples, p=features, k=components):",
        "type": "match",
        "left": [
            "components_",
            "explained_variance_",
            "mean_",
            "transformed data"
        ],
        "right": [
            "(k, p)",
            "(k,)",
            "(p,)",
            "(n, k)"
        ]
    },
    {
        "q": "DBSCAN is a ______-based clustering algorithm, meaning it groups points that are closely packed together.",
        "type": "fill_blank",
        "answers": [
            "density"
        ],
        "other_options": [
            "centroid",
            "distance",
            "grid",
            "hierarchy"
        ]
    },
    {
        "q": "Which code calculates the inverse of the covariance matrix, often used in Mahalanobis distance?",
        "type": "mcq",
        "c": "import numpy as np\ncov_matrix = np.cov(X.T)\n# Calculate inverse",
        "o": [
            "np.linalg.inv(cov_matrix)",
            "np.linalg.eig(cov_matrix)",
            "np.linalg.det(cov_matrix)",
            "np.transpose(cov_matrix)"
        ]
    },
    {
        "q": "In Factor Analysis, variables with high 'cross-loadings' (high correlation with multiple factors) are generally considered desirable for clear interpretation.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange the general workflow of the 'Mean Shift' clustering algorithm:",
        "type": "rearrange",
        "words": [
            "Place window on data",
            "Compute mean of points in window",
            "Shift window to mean",
            "Repeat until convergence"
        ]
    },
    {
        "q": "A coefficient of ______ closer to 1 implies that the clustering is appropriate and the clusters are well separated.",
        "type": "fill_blank",
        "answers": [
            "silhouette"
        ],
        "other_options": [
            "correlation",
            "determination",
            "variation",
            "skewness"
        ]
    },
    {
        "q": "Which technique is strictly supervised?",
        "type": "mcq",
        "o": [
            "Linear Discriminant Analysis",
            "Principal Component Analysis",
            "K-Means Clustering",
            "Hierarchical Clustering"
        ]
    },
    {
        "q": "Match the acronym to the full name:",
        "type": "match",
        "left": [
            "PCA",
            "LDA",
            "GMM",
            "SVD"
        ],
        "right": [
            "Principal Component Analysis",
            "Linear Discriminant Analysis",
            "Gaussian Mixture Model",
            "Singular Value Decomposition"
        ]
    },
    {
        "q": "Ward's method in hierarchical clustering minimizes the sum of squared differences within all clusters.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In Scikit-Learn's DBSCAN implementation, which specific integer label is assigned to 'noise' points that do not belong to any cluster?",
        "type": "mcq",
        "o": [
            "-1",
            "0",
            "None",
            "999"
        ]
    },
    {
        "q": "Unlike PCA which focuses on preserving global structure, t-SNE is a non-linear technique primarily designed to preserve ______ structure.",
        "type": "fill_blank",
        "answers": [
            "local"
        ],
        "other_options": [
            "global",
            "linear",
            "total",
            "latent"
        ]
    },
    {
        "q": "Match the Scikit-Learn parameter to the algorithm it belongs to:",
        "type": "match",
        "left": [
            "bandwidth",
            "linkage",
            "min_samples",
            "n_init"
        ],
        "right": [
            "Mean Shift",
            "Agglomerative Clustering",
            "DBSCAN",
            "K-Means"
        ]
    },
    {
        "q": "Standard K-Means clustering performs poorly on datasets containing concentric circles (like a target) because it assumes clusters are convex.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What happens if you set `distance_threshold=0` in Agglomerative Clustering (with n_clusters=None)?",
        "type": "mcq",
        "c": "model = AgglomerativeClustering(n_clusters=None, distance_threshold=0)\nmodel.fit(X)",
        "o": [
            "It builds a full tree down to single points",
            "It creates one single giant cluster",
            "It throws an error",
            "It automatically finds the best k"
        ]
    },
    {
        "q": "Rearrange the sequence of operations for calculating Linear Discriminant Analysis:",
        "type": "rearrange",
        "words": [
            "Compute Class Means",
            "Compute Within-Class Scatter",
            "Compute Between-Class Scatter",
            "Compute Eigenvectors"
        ]
    },
    {
        "q": "In Factor Analysis, the rule of thumb often cited for sample size is that there should be at least ______ times as many observations as there are variables.",
        "type": "fill_blank",
        "answers": [
            "10"
        ],
        "other_options": [
            "2",
            "100",
            "1",
            "0.5"
        ]
    },
    {
        "q": "Which attribute of a fitted KMeans model produces the distance of each sample to the cluster centers?",
        "type": "mcq",
        "c": "kmeans = KMeans(n_clusters=3).fit(X)\n# Code to get distances to centers",
        "o": [
            "kmeans.transform(X)",
            "kmeans.predict(X)",
            "kmeans.score(X)",
            "kmeans.distances_"
        ]
    },
    {
        "q": "LDA explicitly attempts to model the difference between the classes of data, whereas PCA does not look at class labels at all.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the real-world application to the most appropriate Multivariate technique:",
        "type": "match",
        "left": [
            "Market Segmentation (Customer Groups)",
            "Eigenfaces (Image Compression)",
            "Psychometric Survey Validation",
            "Bankruptcy Prediction (Solvent vs Insolvent)"
        ],
        "right": [
            "Cluster Analysis",
            "Principal Component Analysis",
            "Factor Analysis",
            "Discriminant Analysis"
        ]
    },
    {
        "q": "The ______ method in Hierarchical Clustering calculates the distance between two clusters as the average distance between all pairs of objects in the two clusters.",
        "type": "fill_blank",
        "answers": [
            "average"
        ],
        "other_options": [
            "single",
            "complete",
            "ward",
            "centroid"
        ]
    },
    {
        "q": "What is the primary output of the `fit_transform` method in PCA?",
        "type": "mcq",
        "o": [
            "The Principal Scores (coordinates in new space)",
            "The Eigenvectors (rotation matrix)",
            "The Eigenvalues (variance)",
            "The Covariance Matrix"
        ]
    },
    {
        "q": "If the 'between-group' variance is significantly larger than the 'within-group' variance, it suggests that the groups are well-separated.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the standard workflow for a Data Science clustering project:",
        "type": "rearrange",
        "words": [
            "Feature Scaling",
            "Run Clustering Algo",
            "Calculate Silhouette Score",
            "Profile Cluster Characteristics"
        ]
    },
    {
        "q": "Which algorithm is a 'divisive' (top-down) approach to clustering?",
        "type": "mcq",
        "o": [
            "DIANA (Divisive Analysis)",
            "AGNES (Agglomerative Nesting)",
            "K-Means",
            "DBSCAN"
        ]
    },
    {
        "q": "In the context of PCA, the matrix 'W' usually refers to the ______ matrix that projects the original data onto the principal components.",
        "type": "fill_blank",
        "answers": [
            "weight"
        ],
        "other_options": [
            "covariance",
            "identity",
            "scatter",
            "distance"
        ]
    },
    {
        "q": "Match the K-Means limitation to its description:",
        "type": "match",
        "left": [
            "Local Minima",
            "Scalability",
            "Spherical Assumption",
            "Fixed K"
        ],
        "right": [
            "Result depends on initialization",
            "Slow with distance calculations for many points",
            "Fails on elongated clusters",
            "Must know number of groups beforehand"
        ]
    },
    {
        "q": "The trace of the covariance matrix is equal to the sum of the eigenvalues in PCA.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In 'Fuzzy C-Means' clustering, unlike standard K-Means, each data point has a ______ of belonging to each cluster rather than a single hard assignment.",
        "type": "fill_blank",
        "answers": [
            "probability"
        ],
        "other_options": [
            "distance",
            "variance",
            "centroid",
            "label"
        ]
    },
    {
        "q": "Which PCA variant introduces an L1 regularization term (penalty) to ensure that many principal component loadings are exactly zero?",
        "type": "mcq",
        "o": [
            "Sparse PCA",
            "Kernel PCA",
            "Incremental PCA",
            "Randomized PCA"
        ]
    },
    {
        "q": "Match the specialized clustering algorithm to its unique characteristic:",
        "type": "match",
        "left": [
            "BIRCH",
            "Affinity Propagation",
            "OPTICS",
            "Spectral Clustering"
        ],
        "right": [
            "Builds a Clustering Feature Tree (CFT) for large data",
            "Passes messages between points (availability/responsibility)",
            "Produces a Reachability Plot instead of explicit clusters",
            "Uses the eigenvalues of the Laplacian matrix"
        ]
    },
    {
        "q": "Linear Discriminant Analysis assumes that the covariance matrices of the different classes are identical (homoscedasticity).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What method would you call on a fitted PCA object to retrieve the estimated covariance matrix of the data?",
        "type": "mcq",
        "c": "pca = PCA()\npca.fit(X)\n# Retrieve covariance",
        "o": [
            "pca.get_covariance()",
            "pca.covariance_",
            "pca.cov_matrix_",
            "pca.calc_covariance()"
        ]
    },
    {
        "q": "Rearrange the logical steps of the 'Elbow Method' visualization:",
        "type": "rearrange",
        "words": [
            "Run K-Means for k=1 to 10",
            "Record SSE for each k",
            "Plot k vs SSE",
            "Locate the bend"
        ]
    },
    {
        "q": "In Factor Analysis, a 'Heywood case' refers to a situation where an estimated variance parameter (uniqueness) is ______ than zero, which is theoretically impossible.",
        "type": "fill_blank",
        "answers": [
            "less"
        ],
        "other_options": [
            "greater",
            "equal",
            "more",
            "approx"
        ]
    },
    {
        "q": "Which distance metric is defined as the maximum absolute difference between any single dimension of the data vectors (L-infinity norm)?",
        "type": "mcq",
        "o": [
            "Chebyshev Distance",
            "Manhattan Distance",
            "Euclidean Distance",
            "Minkowski Distance"
        ]
    },
    {
        "q": "Unlike Linear Regression, Linear Discriminant Analysis handles multicollinearity (highly correlated independent variables) well without needing regularization.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the clustering evaluation metric to its interpretation:",
        "type": "match",
        "left": [
            "Davies-Bouldin Index",
            "Calinski-Harabasz Index",
            "Silhouette Score"
        ],
        "right": [
            "Lower values indicate better clustering",
            "Higher values indicate better clustering",
            "Values near +1 indicate good separation"
        ]
    },
    {
        "q": "In the OPTICS algorithm, the ______ distance represents the minimum radius required to classify a point as a core point.",
        "type": "fill_blank",
        "answers": [
            "core"
        ],
        "other_options": [
            "reachability",
            "manhattan",
            "linkage",
            "euclidean"
        ]
    },
    {
        "q": "Which Scipy parameter controls how deep the dendrogram is displayed, truncating the tree to show only the last 'p' merged clusters?",
        "type": "mcq",
        "c": "dendrogram(linkage_matrix, truncate_mode='lastp', p=10)",
        "o": [
            "p",
            "truncate_mode",
            "depth",
            "levels"
        ]
    },
    {
        "q": "Affinity Propagation requires the user to set the number of clusters (k) manually before training.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange the interpretation of a Factor Analysis Loading Matrix:",
        "type": "rearrange",
        "words": [
            "Identify high loadings",
            "Ignore low loadings",
            "Check signs (+/-)",
            "Label the Factor"
        ]
    },
    {
        "q": "The ______ factor in Affinity Propagation controls the extent to which the current value is maintained relative to incoming messages (to avoid oscillations).",
        "type": "fill_blank",
        "answers": [
            "damping"
        ],
        "other_options": [
            "smoothing",
            "alpha",
            "learning",
            "gamma"
        ]
    },
    {
        "q": "Which code snippet correctly fits a model and returns the cluster labels in one step?",
        "type": "mcq",
        "c": "from sklearn.cluster import DBSCAN\ndb = DBSCAN(eps=0.3)\n# Get labels directly",
        "o": [
            "labels = db.fit_predict(X)",
            "labels = db.predict(X)",
            "labels = db.transform(X)",
            "labels = db.fit_transform(X)"
        ]
    },
    {
        "q": "Multidimensional Scaling (MDS) is a technique often grouped with PCA that attempts to preserve the ______ distances between points when mapping them to a lower dimension.",
        "type": "fill_blank",
        "answers": [
            "pairwise"
        ],
        "other_options": [
            "variance",
            "class",
            "absolute",
            "linear"
        ]
    },
    {
        "q": "Canberra distance is a weighted version of Manhattan distance that is very sensitive to data values close to zero.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which method in Scikit-Learn allows K-Means to be updated incrementally with new data batches (online learning)?",
        "type": "mcq",
        "c": "from sklearn.cluster import MiniBatchKMeans\nmodel = MiniBatchKMeans()\n# Which method updates the model?",
        "o": [
            "model.partial_fit(new_data)",
            "model.update(new_data)",
            "model.fit_increment(new_data)",
            "model.add_data(new_data)"
        ]
    },
    {
        "q": "In Principal Component Analysis, the eigenvector associated with the largest eigenvalue represents the direction of ______ variance.",
        "type": "fill_blank",
        "answers": [
            "maximum"
        ],
        "other_options": [
            "minimum",
            "zero",
            "average",
            "residual"
        ]
    },
    {
        "q": "Match the Hierarchical Clustering parameter to its effect:",
        "type": "match",
        "left": [
            "metric='euclidean'",
            "metric='precomputed'",
            "linkage='ward'",
            "linkage='complete'"
        ],
        "right": [
            "Standard geometric distance",
            "Input is a distance matrix",
            "Minimizes variance (Euclidean only)",
            "Uses maximum distance between sets"
        ]
    },
    {
        "q": "Linear Discriminant Analysis fails if a variable has zero variance (is constant) across all samples.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the specific purpose of the 'connectivity' parameter in Agglomerative Clustering?",
        "type": "mcq",
        "o": [
            "Restricts merging to spatially adjacent clusters",
            "Connects the dendrogram to the internet",
            "Defines the number of threads to use",
            "Links the cluster labels to true classes"
        ]
    },
    {
        "q": "Rearrange the steps to visually determine the number of factors using a Scree Plot:",
        "type": "rearrange",
        "words": [
            "Calculate Eigenvalues",
            "Plot Eigenvalues vs Index",
            "Find the 'Elbow'",
            "Retain factors above elbow"
        ]
    },
    {
        "q": "In Spectral Clustering, the data is projected into a lower-dimensional space defined by the eigenvectors of the ______ matrix before clustering.",
        "type": "fill_blank",
        "answers": [
            "Laplacian"
        ],
        "other_options": [
            "Covariance",
            "Confusion",
            "Identity",
            "Hessian"
        ]
    },
    {
        "q": "Which attribute holds the class priors (percentage of each class) in a fitted LDA model?",
        "type": "mcq",
        "c": "lda = LinearDiscriminantAnalysis().fit(X, y)\n# Check class proportions",
        "o": [
            "lda.priors_",
            "lda.class_weight_",
            "lda.proportions_",
            "lda.weights_"
        ]
    },
    {
        "q": "The 'Bisecting K-Means' algorithm is a divisive hierarchical approach that repeatedly splits the cluster with the largest error.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the evaluation metric to its requirement:",
        "type": "match",
        "left": [
            "Adjusted Rand Index",
            "Silhouette Coefficient",
            "Homogeneity Score",
            "Inertia"
        ],
        "right": [
            "Requires Ground Truth",
            "No Ground Truth needed",
            "Requires Ground Truth",
            "No Ground Truth (internal metric)"
        ]
    },
    {
        "q": "When using PCA for image compression, the 'reconstructed' image is obtained by performing the ______ transformation on the reduced data.",
        "type": "fill_blank",
        "answers": [
            "inverse"
        ],
        "other_options": [
            "forward",
            "linear",
            "logistic",
            "fourier"
        ]
    },
    {
        "q": "What is the output of `pca.noise_variance_` useful for?",
        "type": "mcq",
        "o": [
            "Estimating the variance of the noise in Probabilistic PCA",
            "Measuring the total error of the model",
            "Determining the number of outliers",
            "Calculating the standard deviation of the input"
        ]
    },
    {
        "q": "In Cluster Analysis, 'Hard' clustering assigns each point to exactly one cluster, whereas 'Soft' (Fuzzy) clustering assigns degrees of membership.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the components of the K-Means objective function formula (Sum of Squares):",
        "type": "rearrange",
        "words": [
            "Sum over all clusters",
            "Sum over points in cluster",
            "Square of distance",
            "Point minus Centroid"
        ]
    },
    {
        "q": "The ______ correlation coefficient is a measure used in Factor Analysis to assess the adequacy of the correlation matrix for factorization (KMO test).",
        "type": "fill_blank",
        "answers": [
            "partial"
        ],
        "other_options": [
            "pearson",
            "spearman",
            "total",
            "rank"
        ]
    },
    {
        "q": "Which distance metric is most appropriate for clustering data representing latitude and longitude coordinates on a sphere?",
        "type": "mcq",
        "o": [
            "Haversine Distance",
            "Euclidean Distance",
            "Manhattan Distance",
            "Hamming Distance"
        ]
    },
    {
        "q": "Match the PCA parameter to its function:",
        "type": "match",
        "left": [
            "n_components=0.95",
            "n_components=5",
            "whiten=True",
            "svd_solver='arpack'"
        ],
        "right": [
            "Select components by variance ratio",
            "Select exact number of components",
            "Normalize components to unit variance",
            "Use truncated SVD (efficient for k << p)"
        ]
    },
    {
        "q": "If the number of features is very large compared to the number of samples, LDA is likely to overfit.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which PCA variation uses the 'kernel trick' to project data into a higher-dimensional space to separate non-linearly separable data?",
        "type": "mcq",
        "o": [
            "Kernel PCA",
            "Sparse PCA",
            "Incremental PCA",
            "Probabilistic PCA"
        ]
    },
    {
        "q": "In K-Means clustering, the geometric boundaries that separate the regions assigned to different clusters are known as ______ cells.",
        "type": "fill_blank",
        "answers": [
            "Voronoi"
        ],
        "other_options": [
            "Gaussian",
            "Euclidean",
            "Manhattan",
            "Tesla"
        ]
    },
    {
        "q": "Match the kernel function used in Kernel PCA to its mathematical characteristic:",
        "type": "match",
        "left": [
            "Linear Kernel",
            "RBF (Radial Basis Function)",
            "Polynomial Kernel",
            "Sigmoid Kernel"
        ],
        "right": [
            "Standard PCA equivalent",
            "Infinite dimensional projection (Gaussian)",
            "Degree 'd' interactions",
            "Hyperbolic tangent shape"
        ]
    },
    {
        "q": "Standard DBSCAN in Scikit-Learn includes a 'predict' method to classify new, unseen data points after training.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What is the result of `model.labels_` in Agglomerative Clustering?",
        "type": "mcq",
        "c": "from sklearn.cluster import AgglomerativeClustering\nmodel = AgglomerativeClustering(n_clusters=3)\nmodel.fit(X)\n# model.labels_ contains:",
        "o": [
            "The cluster index for each training sample",
            "The cluster centers",
            "The linkage matrix",
            "The distance matrix"
        ]
    },
    {
        "q": "Rearrange the logical flow of 'Feature Extraction' using PCA:",
        "type": "rearrange",
        "words": [
            "Center the data (Mean=0)",
            "Compute SVD or Eigendecomposition",
            "Sort eigenvalues descending",
            "Project data to top k vectors"
        ]
    },
    {
        "q": "In Discriminant Analysis, the ratio of ______ variance to ______ variance is maximized to achieve the best separation.",
        "type": "fill_blank",
        "answers": [
            "between-class",
            "within-class"
        ],
        "other_options": [
            "total",
            "residual",
            "error",
            "latent"
        ]
    },
    {
        "q": "Which Scikit-Learn cluster validity metric requires the computation of a contingency table between predicted labels and true labels?",
        "type": "mcq",
        "o": [
            "V-Measure",
            "Silhouette Score",
            "Inertia",
            "Davies-Bouldin Index"
        ]
    },
    {
        "q": "Factor Analysis is conceptually distinct from PCA because it includes an error term for each variable, whereas PCA assumes measurements are error-free.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the technique to its primary limitation:",
        "type": "match",
        "left": [
            "K-Means",
            "DBSCAN",
            "Hierarchical",
            "PCA"
        ],
        "right": [
            "Assume spherical clusters",
            "Struggles with varying densities",
            "High memory usage (distance matrix)",
            "Linear transformation only (basic version)"
        ]
    },
    {
        "q": "The ______ metric is often used in clustering binary data (0/1) to measure dissimilarity based on the proportion of mismatches.",
        "type": "fill_blank",
        "answers": [
            "Hamming"
        ],
        "other_options": [
            "Euclidean",
            "Cosine",
            "Pearson",
            "Ward"
        ]
    },
    {
        "q": "What is the shape of the output covariance matrix from `EmpiricalCovariance` fit on data with 5 features?",
        "type": "mcq",
        "c": "from sklearn.covariance import EmpiricalCovariance\ncov = EmpiricalCovariance().fit(X) # X.shape = (100, 5)",
        "o": [
            "(5, 5)",
            "(100, 5)",
            "(100, 100)",
            "(5, 1)"
        ]
    },
    {
        "q": "In Cluster Analysis, a 'singleton' refers to a cluster that contains exactly one data point.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the steps to determine cluster stability:",
        "type": "rearrange",
        "words": [
            "Subsample the dataset",
            "Run clustering algorithm",
            "Repeat multiple times",
            "Compare cluster memberships"
        ]
    },
    {
        "q": "If you use `n_components='mle'` in Scikit-Learn's PCA, it attempts to guess the dimension using ______'s Automatic Choice.",
        "type": "fill_blank",
        "answers": [
            "Minka"
        ],
        "other_options": [
            "Kaiser",
            "Pearson",
            "Fisher",
            "Gauss"
        ]
    },
    {
        "q": "Which plot is specifically used in Hierarchical Clustering to visualize the arrangement of clusters and the distance at which they merge?",
        "type": "mcq",
        "o": [
            "Dendrogram",
            "Scree Plot",
            "Biplot",
            "Scatter Matrix"
        ]
    },
    {
        "q": "Match the Linear Algebra concept to its role in Multivariate Analysis:",
        "type": "match",
        "left": [
            "Orthogonal Matrix",
            "Identity Matrix",
            "Diagonal Matrix",
            "Singular Matrix"
        ],
        "right": [
            "Rows/Cols are uncorrelated vectors",
            "Represents uncorrelated variables with unit variance",
            "Values only on diagonal (scaling)",
            "Determinant is zero (not invertible)"
        ]
    },
    {
        "q": "LDA is less prone to overfitting than QDA because LDA estimates fewer parameters (one shared covariance matrix vs one per class).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which clustering algorithm is specifically designed to handle categorical data by using modes instead of means?",
        "type": "mcq",
        "o": [
            "K-Modes",
            "K-Means",
            "DBSCAN",
            "Gaussian Mixture"
        ]
    },
    {
        "q": "In Principal Component Analysis, if the data is not centered (mean subtracted) before analysis, the first principal component will often correspond to the ______ of the data.",
        "type": "fill_blank",
        "answers": [
            "mean"
        ],
        "other_options": [
            "variance",
            "median",
            "noise",
            "outlier"
        ]
    },
    {
        "q": "Match the distance metric to its typical use case:",
        "type": "match",
        "left": [
            "Gower Distance",
            "Bray-Curtis Dissimilarity",
            "Canberra Distance"
        ],
        "right": [
            "Mixed data types (numeric + categorical)",
            "Ecological data / Counts",
            "Data sensitive to values near zero"
        ]
    },
    {
        "q": "Linear Discriminant Analysis natively supports multi-class classification without needing 'One-vs-Rest' strategies.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the maximum number of discriminant components LDA can produce for a dataset with 4 classes and 10 features?",
        "type": "mcq",
        "o": [
            "3",
            "4",
            "10",
            "9"
        ]
    },
    {
        "q": "Rearrange the steps to compute the cumulative explained variance in PCA:",
        "type": "rearrange",
        "words": [
            "Get explained_variance_ratio_",
            "Apply np.cumsum()",
            "Plot step function",
            "Determine threshold (e.g., 95%)"
        ]
    },
    {
        "q": "In Hierarchical Clustering, the height of the vertical lines in a dendrogram represents the ______ between the merged clusters.",
        "type": "fill_blank",
        "answers": [
            "distance"
        ],
        "other_options": [
            "density",
            "probability",
            "covariance",
            "entropy"
        ]
    },
    {
        "q": "Which code snippet correctly standardizes data before applying PCA?",
        "type": "mcq",
        "c": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n# Fit and transform",
        "o": [
            "X_scaled = scaler.fit_transform(X)",
            "X_scaled = scaler.predict(X)",
            "X_scaled = scaler.standardize(X)",
            "X_scaled = scaler.normalize(X)"
        ]
    },
    {
        "q": "K-Means always partitions the data into convex (Voronoi) cells.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the statistical test to its purpose in Multivariate Analysis:",
        "type": "match",
        "left": [
            "Box's M Test",
            "Bartlett's Test",
            "KMO Test"
        ],
        "right": [
            "Equality of covariance matrices (LDA assumption)",
            "Homogeneity of variances or Sphericity",
            "Sampling adequacy for Factor Analysis"
        ]
    },
    {
        "q": "Feature ______ is a technique that uses clustering (like Ward's method) to group similar features together to reduce dimensionality.",
        "type": "fill_blank",
        "answers": [
            "agglomeration"
        ],
        "other_options": [
            "selection",
            "extraction",
            "rotation",
            "decomposition"
        ]
    },
    {
        "q": "What does the `cluster_std` parameter control in `make_blobs` when generating synthetic data for clustering?",
        "type": "mcq",
        "c": "from sklearn.datasets import make_blobs\nX, y = make_blobs(n_samples=100, centers=3, cluster_std=1.0)",
        "o": [
            "The standard deviation (spread) of the clusters",
            "The number of clusters",
            "The distance between clusters",
            "The number of features"
        ]
    },
    {
        "q": "In Factor Analysis, 'Orthogonal' rotation assumes factors are uncorrelated, while 'Oblique' rotation allows factors to be correlated.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the components of a 'Confusion Matrix' for a binary classification problem (flattened order usually used in descriptions):",
        "type": "rearrange",
        "words": [
            "True Negatives (TN)",
            "False Positives (FP)",
            "False Negatives (FN)",
            "True Positives (TP)"
        ]
    },
    {
        "q": "The ______ coefficient determines the degree to which a cluster is distinct from other clusters (separation) vs how compact it is (cohesion).",
        "type": "fill_blank",
        "answers": [
            "silhouette"
        ],
        "other_options": [
            "correlation",
            "regression",
            "beta",
            "loading"
        ]
    },
    {
        "q": "Which Scikit-Learn PCA attribute allows you to access the singular values directly?",
        "type": "mcq",
        "c": "pca = PCA().fit(X)\n# Access singular values",
        "o": [
            "pca.singular_values_",
            "pca.eigenvalues_",
            "pca.values_",
            "pca.s_"
        ]
    },
    {
        "q": "Regularized Discriminant Analysis (RDA) is a compromise between LDA and QDA that shrinks the separate covariances of QDA toward a common covariance.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the term to the algorithm that uses it:",
        "type": "match",
        "left": [
            "Lambda (Eigenvalue)",
            "Epsilon (Radius)",
            "Centroid (Mean)",
            "Prior (Probability)"
        ],
        "right": [
            "PCA / Factor Analysis",
            "DBSCAN",
            "K-Means",
            "LDA / QDA"
        ]
    },
    {
        "q": "Which method in Scikit-Learn's KMeans class transforms the data X into a new cluster-distance space (distance to each centroid)?",
        "type": "mcq",
        "c": "kmeans = KMeans(n_clusters=3).fit(X)\n# Returns shape (n_samples, n_clusters)",
        "o": [
            "kmeans.transform(X)",
            "kmeans.predict(X)",
            "kmeans.fit_predict(X)",
            "kmeans.score(X)"
        ]
    },
    {
        "q": "In Factor Analysis, the concept of ______ Structure (Thurstone) refers to a rotation solution where each variable loads highly on only one factor.",
        "type": "fill_blank",
        "answers": [
            "Simple"
        ],
        "other_options": [
            "Complex",
            "Latent",
            "Eigen",
            "Orthogonal"
        ]
    },
    {
        "q": "Match the algorithm to its time complexity class (approximate for n samples):",
        "type": "match",
        "left": [
            "K-Means (Lloyd's)",
            "Agglomerative Clustering",
            "DBSCAN (with indexing)",
            "Spectral Clustering"
        ],
        "right": [
            "O(n) - Linear (usually)",
            "O(n squared) or O(n cubed)",
            "O(n log n)",
            "O(n cubed) - Eigen decomposition"
        ]
    },
    {
        "q": "Principal Component Analysis implies a causal relationship between the principal components and the original variables.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "In the context of Discriminant Analysis, which attribute contains the weight vector(s) used to separate the classes?",
        "type": "mcq",
        "c": "lda = LinearDiscriminantAnalysis().fit(X, y)",
        "o": [
            "lda.coef_",
            "lda.weights_",
            "lda.vectors_",
            "lda.loadings_"
        ]
    },
    {
        "q": "Rearrange the steps for selecting 'k' using the Cross-Validation approach:",
        "type": "rearrange",
        "words": [
            "Split data into folds",
            "Train with k clusters",
            "Evaluate on validation set",
            "Average the scores"
        ]
    },
    {
        "q": "The ______ linkage method in Hierarchical Clustering uses the distance between the centroids (means) of the two clusters being merged.",
        "type": "fill_blank",
        "answers": [
            "centroid"
        ],
        "other_options": [
            "average",
            "ward",
            "single",
            "complete"
        ]
    },
    {
        "q": "DBSCAN is fully deterministic: it always produces the exact same cluster assignments for 'border points' regardless of data processing order.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which function creates a connectivity graph (sparse matrix) often used to constrain Hierarchical Clustering to neighboring points?",
        "type": "mcq",
        "c": "from sklearn.neighbors import ______",
        "o": [
            "kneighbors_graph",
            "connectivity_matrix",
            "sparse_graph",
            "linkage_graph"
        ]
    },
    {
        "q": "Match the PCA result to its dimensions (where n=samples, p=features):",
        "type": "match",
        "left": [
            "Covariance Matrix",
            "Eigenvector Matrix (V)",
            "Score Matrix (Z)",
            "Eigenvalues (Lambda)"
        ],
        "right": [
            "p x p (Symmetric)",
            "p x p (Rotation)",
            "n x p (Transformed)",
            "1 x p (Variance scalar)"
        ]
    },
    {
        "q": "The 'Scree Plot' used in PCA and Factor Analysis gets its name from the geological term for ______ accumulating at the base of a cliff.",
        "type": "fill_blank",
        "answers": [
            "rubble"
        ],
        "other_options": [
            "water",
            "sand",
            "snow",
            "trees"
        ]
    },
    {
        "q": "If you perform PCA on the Correlation Matrix rather than the Covariance Matrix, it is equivalent to performing PCA on standardized data (z-scores).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which clustering algorithm is based on the idea of 'exemplars' (representative points) found by passing messages between data points?",
        "type": "mcq",
        "o": [
            "Affinity Propagation",
            "K-Means",
            "DBSCAN",
            "Mean Shift"
        ]
    },
    {
        "q": "Rearrange the calculation of the Covariance Matrix for data X (centered):",
        "type": "rearrange",
        "words": [
            "Transpose X (X.T)",
            "Multiply X.T by X",
            "Divide by (n-1)",
            "Result is Matrix C"
        ]
    },
    {
        "q": "In Scikit-Learn, `FeatureAgglomeration` uses ______ clustering to group features (columns) rather than samples (rows).",
        "type": "fill_blank",
        "answers": [
            "hierarchical"
        ],
        "other_options": [
            "kmeans",
            "spectral",
            "dbscan",
            "linear"
        ]
    },
    {
        "q": "Which condition renders Linear Discriminant Analysis (LDA) impossible to calculate directly (Singular Matrix error)?",
        "type": "mcq",
        "o": [
            "Perfect multicollinearity among features",
            "Having more samples than features",
            "Classes having unequal sizes",
            "Data being normally distributed"
        ]
    },
    {
        "q": "Match the 'Solver' in LDA to its characteristic:",
        "type": "match",
        "left": [
            "svd (Singular Value Decomposition)",
            "lsqr (Least Squares)",
            "eigen (Eigenvalue Decomposition)"
        ],
        "right": [
            "Default, does not compute covariance matrix",
            "Supports shrinkage, fast",
            "Supports shrinkage, slower"
        ]
    },
    {
        "q": "Running K-Means with `n_clusters=1` will result in an inertia (within-cluster sum of squares) equal to the total variance of the dataset multiplied by n_samples.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "The Hopkins Statistic is a statistical test used to assess the ______ tendency of a dataset (i.e., whether the data contains meaningful clusters or is just random noise).",
        "type": "fill_blank",
        "answers": [
            "clustering"
        ],
        "other_options": [
            "variance",
            "normal",
            "linear",
            "regression"
        ]
    },
    {
        "q": "Which Scipy function flattens a dendrogram into specific cluster labels based on a threshold 't'?",
        "type": "mcq",
        "c": "from scipy.cluster.hierarchy import linkage, ______\nZ = linkage(X)\nlabels = ______(Z, t=1.5, criterion='distance')",
        "o": [
            "fcluster",
            "cut_tree",
            "flatten",
            "get_labels"
        ]
    },
    {
        "q": "Match the advanced clustering algorithm to its mechanism:",
        "type": "match",
        "left": [
            "BIRCH",
            "Mean Shift",
            "Spectral",
            "Agglomerative"
        ],
        "right": [
            "Clustering Feature Tree (CF Tree)",
            "Kernel Density Estimation (Mode seeking)",
            "Graph Laplacian Eigenvectors",
            "Recursive Merging"
        ]
    },
    {
        "q": "In Probabilistic PCA, the model assumes that the data is generated from a latent variable model with Gaussian noise.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the relationship between Cosine Similarity (S) and Cosine Distance (D)?",
        "type": "mcq",
        "o": [
            "D = 1 - S",
            "D = 1 + S",
            "D = 1 / S",
            "D = S squared"
        ]
    },
    {
        "q": "Rearrange the steps to compute the 'F-statistic' in the context of clustering (ANOVA approach):",
        "type": "rearrange",
        "words": [
            "Calc Between-Cluster Variance",
            "Calc Within-Cluster Variance",
            "Divide Between by Within",
            "Compare to F-distribution"
        ]
    },
    {
        "q": "If the 'priors' parameter in LDA is set to None (default), the class probabilities are inferred from the ______ frequencies of the classes in the training data.",
        "type": "fill_blank",
        "answers": [
            "relative"
        ],
        "other_options": [
            "equal",
            "inverse",
            "random",
            "uniform"
        ]
    },
    {
        "q": "Which attribute of the `AgolmerativeClustering` object describes the structure of the tree (parents and children)?",
        "type": "mcq",
        "c": "model = AgglomerativeClustering().fit(X)\n# The array defining the tree merge steps",
        "o": [
            "model.children_",
            "model.tree_",
            "model.parents_",
            "model.hierarchy_"
        ]
    },
    {
        "q": "The 'Fowlkes-Mallows Index' is an external evaluation metric that measures the geometric mean of precision and recall between two clusterings.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the parameter to the Scikit-Learn function:",
        "type": "match",
        "left": [
            "whiten (PCA)",
            "store_covariance (LDA)",
            "linkage (Agglomerative)",
            "leaf_size (DBSCAN/BallTree)"
        ],
        "right": [
            "Normalize components",
            "Compute covariance matrix",
            "Define merge criterion",
            "Optimize neighbor search"
        ]
    },
    {
        "q": "In Biplots, if two variable vectors are close to 180 degrees apart (opposite directions), it indicates a strong ______ correlation.",
        "type": "fill_blank",
        "answers": [
            "negative"
        ],
        "other_options": [
            "positive",
            "zero",
            "spurious",
            "partial"
        ]
    },
    {
        "q": "Which matrix property must hold true for the Covariance Matrix in order to perform Cholesky decomposition (often used in simulations)?",
        "type": "mcq",
        "o": [
            "Positive Definite",
            "Singular",
            "Orthogonal",
            "Skew-Symmetric"
        ]
    },
    {
        "q": "K-Means clustering is invariant to affine transformations (like scaling or rotation) of the dataset.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange the lifecycle of applying a 'Pipeline' with clustering:",
        "type": "rearrange",
        "words": [
            "Define steps list",
            "Create Pipeline object",
            "Fit pipeline to data",
            "Predict cluster labels"
        ]
    },
    {
        "q": "The ______ Information Criterion (BIC) is often used in Model-Based Clustering (like GMM) to select the optimal number of clusters by penalizing model complexity.",
        "type": "fill_blank",
        "answers": [
            "Bayesian"
        ],
        "other_options": [
            "Akaike",
            "Deviance",
            "Entropy",
            "Fisher"
        ]
    },
    {
        "q": "Which code snippet correctly calculates the 'Pairwise Distances' between all rows in matrix X?",
        "type": "mcq",
        "c": "from sklearn.metrics import pairwise_distances\n# X shape (n_samples, n_features)",
        "o": [
            "D = pairwise_distances(X)",
            "D = pairwise_distances(X, axis=0)",
            "D = pairwise_distances(X.T)",
            "D = pairwise_distances(X, metric='none')"
        ]
    },
    {
        "q": "Match the Clustering Tendency visual to its description:",
        "type": "match",
        "left": [
            "Visual Assessment of Tendency (VAT)",
            "Dendrogram",
            "Silhouette Plot",
            "Elbow Plot"
        ],
        "right": [
            "Heatmap of ordered dissimilarity matrix",
            "Tree diagram of merges",
            "Bar chart of cohesion vs separation",
            "Line chart of Sum of Squared Errors"
        ]
    },
    {
        "q": "In a contingency matrix used for cluster evaluation, the entry C_ij represents the number of samples belonging to true class 'i' that are assigned to cluster 'j'.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which Scikit-Learn attribute is used to access the number of features seen during the fit method?",
        "type": "mcq",
        "c": "pca = PCA(n_components=2)\npca.fit(X_train)\n# Check input feature count",
        "o": [
            "pca.n_features_in_",
            "pca.n_features_",
            "pca.features_",
            "pca.n_columns_"
        ]
    },
    {
        "q": "In K-Means clustering, if you set 'k' equal to the number of data points, the 'inertia' (sum of squared errors) will effectively become ______.",
        "type": "fill_blank",
        "answers": [
            "zero"
        ],
        "other_options": [
            "infinite",
            "one",
            "maximum",
            "negative"
        ]
    },
    {
        "q": "Match the factor extraction method to its description:",
        "type": "match",
        "left": [
            "Principal Component Method",
            "Maximum Likelihood",
            "Principal Axis Factoring"
        ],
        "right": [
            "Uses total variance (1s on diagonal)",
            "Assumes multivariate normality, goodness-of-fit available",
            "Uses shared variance (communalities on diagonal)"
        ]
    },
    {
        "q": "The 'average' linkage method in hierarchical clustering is generally less sensitive to outliers than the 'single' linkage method.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the primary output of the 'decision_function' in Linear Discriminant Analysis for a binary classification?",
        "type": "mcq",
        "o": [
            "The signed distance to the separating hyperplane",
            "The probability of the positive class",
            "The class label (0 or 1)",
            "The covariance matrix"
        ]
    },
    {
        "q": "Rearrange the steps of K-Means++ initialization:",
        "type": "rearrange",
        "words": [
            "Pick 1st centroid randomly",
            "Calc distance squared for all points",
            "Pick next centroid via probability",
            "Repeat until k centroids chosen"
        ]
    },
    {
        "q": "In Cluster Analysis, ______ refers to the degree to which data points in the same cluster are similar to each other.",
        "type": "fill_blank",
        "answers": [
            "cohesion"
        ],
        "other_options": [
            "separation",
            "correlation",
            "covariance",
            "adhesion"
        ]
    },
    {
        "q": "Which code correctly standardizes data to have a mean of 0 and standard deviation of 1?",
        "type": "mcq",
        "c": "from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()",
        "o": [
            "X_new = (X - X.mean()) / X.std()",
            "X_new = (X - X.min()) / (X.max() - X.min())",
            "X_new = (X - X.median()) / X.var()",
            "X_new = X / X.max()"
        ]
    },
    {
        "q": "A high 'condition number' in the covariance matrix suggests that the matrix is close to being singular (ill-conditioned), which can cause issues in LDA.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the visual plot to its axis labels:",
        "type": "match",
        "left": [
            "Scree Plot (X-axis)",
            "Scree Plot (Y-axis)",
            "Dendrogram (Y-axis)",
            "Biplot (Arrows)"
        ],
        "right": [
            "Component Number",
            "Eigenvalue / Variance",
            "Distance / Dissimilarity",
            "Variable Loadings"
        ]
    },
    {
        "q": "In PCA, the 'scores' represent the coordinates of the original samples in the new ______ coordinate system.",
        "type": "fill_blank",
        "answers": [
            "orthogonal"
        ],
        "other_options": [
            "parallel",
            "categorical",
            "infinite",
            "random"
        ]
    },
    {
        "q": "Which statement about 'MiniBatchKMeans' is correct compared to standard 'KMeans'?",
        "type": "mcq",
        "o": [
            "It is faster but may result in slightly lower cluster quality",
            "It is slower but produces better clusters",
            "It requires the data to be loaded into memory all at once",
            "It cannot handle sparse data"
        ]
    },
    {
        "q": "If the variables have very different variances and are not scaled, PCA will be dominated by the variables with the smallest variance.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange the typical components of a Factor Analysis result table:",
        "type": "rearrange",
        "words": [
            "Variable Names",
            "Factor Loadings",
            "Communality Estimates",
            "Eigenvalues"
        ]
    },
    {
        "q": "The ______ matrix in LDA represents the scatter of class means around the overall global mean.",
        "type": "fill_blank",
        "answers": [
            "between-class"
        ],
        "other_options": [
            "within-class",
            "total",
            "identity",
            "singular"
        ]
    },
    {
        "q": "What does `pca.components_[0]` represent?",
        "type": "mcq",
        "c": "pca = PCA(n_components=2).fit(X)",
        "o": [
            "The eigenvector with the highest variance",
            "The first sample in the projected space",
            "The mean of the first feature",
            "The singular value of the first component"
        ]
    },
    {
        "q": "Match the concept to the technique that primarily utilizes it:",
        "type": "match",
        "left": [
            "Maximize Variance",
            "Maximize Class Separation",
            "Minimize Within-Cluster Variance",
            "Model Latent Constructs"
        ],
        "right": [
            "PCA",
            "LDA",
            "K-Means",
            "Factor Analysis"
        ]
    },
    {
        "q": "The 'Mahalanobis' distance is essentially the Euclidean distance calculated after transforming the data to remove correlations and scale differences.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which criterion in Factor Analysis suggests retaining all factors with an eigenvalue greater than 1.0?",
        "type": "mcq",
        "o": [
            "Kaiser Criterion",
            "Elbow Criterion",
            "Silhouette Criterion",
            "Bartlett Criterion"
        ]
    },
    {
        "q": "In K-Means clustering, the algorithm minimizes the ______ variance, which effectively minimizes the squared distance of points to their respective centroids.",
        "type": "fill_blank",
        "answers": [
            "intra-cluster"
        ],
        "other_options": [
            "inter-cluster",
            "total",
            "covariance",
            "explained"
        ]
    },
    {
        "q": "Match the Scikit-Learn clustering model to its correct 'predict' capability:",
        "type": "match",
        "left": [
            "KMeans",
            "DBSCAN",
            "SpectralClustering",
            "AgglomerativeClustering"
        ],
        "right": [
            "Has predict() method for new data",
            "No predict() method (transductive)",
            "No predict() method (transductive)",
            "No predict() method (transductive)"
        ]
    },
    {
        "q": "Principal Component Analysis is strictly a linear transformation technique and cannot natively capture non-linear relationships without kernel tricks.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does the `fit` method calculate in Scikit-Learn's `StandardScaler`?",
        "type": "mcq",
        "c": "scaler = StandardScaler()\nscaler.fit(X)",
        "o": [
            "Mean and Standard Deviation of X",
            "Min and Max of X",
            "The median of X",
            "The covariance matrix of X"
        ]
    },
    {
        "q": "Rearrange the steps to interpret a Cluster Heatmap:",
        "type": "rearrange",
        "words": [
            "Observe dendrogram on axis",
            "Identify color intensity blocks",
            "Relate blocks to row labels",
            "Relate blocks to col labels"
        ]
    },
    {
        "q": "In Linear Discriminant Analysis, the projection vector 'w' is chosen to maximize the difference between class ______ while keeping the class scatter small.",
        "type": "fill_blank",
        "answers": [
            "means"
        ],
        "other_options": [
            "variances",
            "sizes",
            "priors",
            "errors"
        ]
    },
    {
        "q": "Which metric is the ratio of the sum of within-cluster dispersion to the between-cluster separation (lower is better)?",
        "type": "mcq",
        "o": [
            "Davies-Bouldin Index",
            "Dunn Index",
            "Silhouette Score",
            "Calinski-Harabasz Index"
        ]
    },
    {
        "q": "Factor Analysis is widely used for 'Construct Validity' in psychology to ensure a test measures the theoretical trait it is supposed to measure.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the term to the matrix definition:",
        "type": "match",
        "left": [
            "S (Covariance Matrix)",
            "R (Correlation Matrix)",
            "I (Identity Matrix)",
            "L (Loading Matrix)"
        ],
        "right": [
            "Unstandardized dispersion",
            "Standardized dispersion (diagonals=1)",
            "Diagonals=1, Off-diagonals=0",
            "Correlations between vars and factors"
        ]
    },
    {
        "q": "The ______ coefficient is a measure of internal consistency or reliability of a set of scale items, often used alongside Factor Analysis.",
        "type": "fill_blank",
        "answers": [
            "Cronbach's Alpha"
        ],
        "other_options": [
            "Pearson's Beta",
            "Spearman's Rho",
            "Kendall's Tau",
            "Gini"
        ]
    },
    {
        "q": "Which Scikit-Learn attribute provides the coordinates of the cluster centers?",
        "type": "mcq",
        "c": "model = KMeans(n_clusters=3).fit(X)",
        "o": [
            "model.cluster_centers_",
            "model.centroids_",
            "model.means_",
            "model.centers_"
        ]
    },
    {
        "q": "Hierarchical clustering is computationally more expensive than K-Means, typically having a time complexity of O(n cubed) or O(n squared log n).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the order of variance types in Factor Analysis from specific to general:",
        "type": "rearrange",
        "words": [
            "Error Variance",
            "Specific Variance",
            "Unique Variance",
            "Common Variance"
        ]
    },
    {
        "q": "In the context of 'Sparse PCA', the sparsity is controlled by a regularization parameter (alpha) that forces many ______ to be zero.",
        "type": "fill_blank",
        "answers": [
            "loadings"
        ],
        "other_options": [
            "samples",
            "means",
            "classes",
            "eigenvalues"
        ]
    },
    {
        "q": "Which method is best suited for identifying 'anomalies' or 'outliers' based on low-density regions?",
        "type": "mcq",
        "o": [
            "DBSCAN",
            "K-Means",
            "Linear Regression",
            "Factor Analysis"
        ]
    },
    {
        "q": "Match the visualization tool to the analysis:",
        "type": "match",
        "left": [
            "Scree Plot",
            "Dendrogram",
            "Voronoi Diagram",
            "Scatter Matrix"
        ],
        "right": [
            "Eigenvalue Drop-off",
            "Hierarchical Merges",
            "Partition Boundaries",
            "Pairwise Relationships"
        ]
    },
    {
        "q": "If `whiten=True` in PCA, the transformed components will be uncorrelated and have a variance of 1.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In Scikit-Learn's PCA, if you set 'n_components' to a float between 0 and 1 (e.g., 0.95), what does the algorithm do?",
        "type": "mcq",
        "o": [
            "Selects the number of components to explain 95% of variance",
            "Selects exactly 95 components",
            "Sets the tolerance for convergence to 0.95",
            "Multiplies all data by 0.95"
        ]
    },
    {
        "q": "The ______ distance is a distance metric between two probability distributions, often used in complex clustering or t-SNE (Kullback-Leibler).",
        "type": "fill_blank",
        "answers": [
            "divergence"
        ],
        "other_options": [
            "convergence",
            "correlation",
            "covariance",
            "residue"
        ]
    },
    {
        "q": "Match the clustering algorithm to the geometry it tends to find:",
        "type": "match",
        "left": [
            "K-Means",
            "DBSCAN",
            "Gaussian Mixture Models",
            "Single Linkage Hierarchical"
        ],
        "right": [
            "Spherical / Globular",
            "Arbitrary shapes (density-connected)",
            "Ellipsoidal",
            "Long, chain-like shapes"
        ]
    },
    {
        "q": "Linear Discriminant Analysis requires that the number of samples in the smallest class must be greater than the number of features to avoid singular matrix issues.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which Scikit-Learn clustering method does NOT require the 'n_clusters' parameter to be set at all?",
        "type": "mcq",
        "o": [
            "DBSCAN",
            "K-Means",
            "Spectral Clustering",
            "MiniBatchKMeans"
        ]
    },
    {
        "q": "Rearrange the typical steps to interpret Principal Components:",
        "type": "rearrange",
        "words": [
            "Examine explained variance ratio",
            "Examine component loadings",
            "Identify top features per component",
            "Assign business label to component"
        ]
    },
    {
        "q": "In K-Means, the algorithm stops when the centroids do not move significantly, or when the ______ number of iterations is reached.",
        "type": "fill_blank",
        "answers": [
            "maximum"
        ],
        "other_options": [
            "minimum",
            "optimal",
            "random",
            "average"
        ]
    },
    {
        "q": "What is the shape of the linkage matrix 'Z' returned by `scipy.cluster.hierarchy.linkage` given 'n' samples?",
        "type": "mcq",
        "c": "from scipy.cluster.hierarchy import linkage\nZ = linkage(X)",
        "o": [
            "(n - 1, 4)",
            "(n, n)",
            "(n, 4)",
            "(n - 1, n)"
        ]
    },
    {
        "q": "LDA is considered a parametric method because it assumes the data comes from a multivariate normal distribution.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the term to the PCA matrix algebra definition:",
        "type": "match",
        "left": [
            "Orthogonal",
            "Orthonormal",
            "Diagonalize",
            "Project"
        ],
        "right": [
            "Dot product is zero",
            "Dot product is zero AND length is 1",
            "Make non-diagonal elements zero",
            "Map data to new basis"
        ]
    },
    {
        "q": "The ______ coefficient in Cluster Analysis measures the reduction in dispersion obtained by merging two clusters (used in the Elbow method context).",
        "type": "fill_blank",
        "answers": [
            "determination"
        ],
        "other_options": [
            "correlation",
            "variation",
            "separation",
            "jaccard"
        ]
    },
    {
        "q": "Which code snippet correctly instantiates a Gaussian Mixture Model?",
        "type": "mcq",
        "c": "# Select the correct import",
        "o": [
            "from sklearn.mixture import GaussianMixture",
            "from sklearn.cluster import GaussianMixture",
            "from sklearn.decomposition import GaussianMixture",
            "from sklearn.ensemble import GaussianMixture"
        ]
    },
    {
        "q": "In Hierarchical Clustering, 'inversion' occurs when a child cluster merges at a higher distance value than its parent cluster, which is theoretically impossible in standard ultrametric trees.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the steps for performing K-Fold Cross Validation on an LDA model:",
        "type": "rearrange",
        "words": [
            "Shuffle dataset",
            "Split into K groups",
            "Train on K-1 groups",
            "Test on remaining group"
        ]
    },
    {
        "q": "In Factor Analysis, the square of the factor loading represents the ______ of the variable explained by that factor.",
        "type": "fill_blank",
        "answers": [
            "variance"
        ],
        "other_options": [
            "mean",
            "error",
            "correlation",
            "skewness"
        ]
    },
    {
        "q": "Which PCA parameter in Scikit-Learn is used to subtract the mean from each feature automatically?",
        "type": "mcq",
        "c": "# PCA() handles centering automatically",
        "o": [
            "It is always done by default",
            "center=True",
            "scale=True",
            "normalize=True"
        ]
    },
    {
        "q": "Match the specific algorithm to the broader family:",
        "type": "match",
        "left": [
            "Ward's Method",
            "OPTICS",
            "K-Medoids",
            "Trilinear Decomposition"
        ],
        "right": [
            "Hierarchical",
            "Density-Based",
            "Partitioning",
            "Factor/Component Analysis"
        ]
    },
    {
        "q": "If the dot product of two principal component vectors is zero, it proves they are uncorrelated.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Unlike Linear Regression which minimizes the vertical distance between points and the line, Principal Component Analysis minimizes the ______ distance.",
        "type": "fill_blank",
        "answers": [
            "orthogonal"
        ],
        "other_options": [
            "horizontal",
            "euclidean",
            "squared",
            "absolute"
        ]
    },
    {
        "q": "Which linkage method in Hierarchical Clustering uses the distance between the 'centroids' (means) of the two clusters to determine the merge cost?",
        "type": "mcq",
        "o": [
            "Centroid Linkage",
            "Single Linkage",
            "Complete Linkage",
            "Ward's Method"
        ]
    },
    {
        "q": "Match the term to the variance type in PCA/Factor Analysis:",
        "type": "match",
        "left": [
            "Eigenvalue > 1",
            "Communality = 1",
            "Factor Loading near 0",
            "Scree Plot Elbow"
        ],
        "right": [
            "Component explains more variance than 1 variable",
            "Variable is fully explained by factors",
            "Variable has no correlation with factor",
            "Cut-off point for number of factors"
        ]
    },
    {
        "q": "In K-Means clustering, the decision boundaries formed between clusters are always linear (straight lines or planes).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the specific purpose of the `n_init` parameter in Scikit-Learn's `KMeans` class?",
        "type": "mcq",
        "c": "kmeans = KMeans(n_init=10)",
        "o": [
            "Run the algorithm 10 times with different seeds and pick the best",
            "Run 10 iterations of the centroid update loop",
            "Initialize 10 clusters",
            "Use 10 parallel threads"
        ]
    },
    {
        "q": "Rearrange the sequence of computing Factor Scores (Regression Method):",
        "type": "rearrange",
        "words": [
            "Compute Correlation Matrix",
            "Invert Correlation Matrix",
            "Multiply by Loadings",
            "Multiply by Standardized Data"
        ]
    },
    {
        "q": "The ______ Ratio Criterion (Calinski-Harabasz Index) measures the ratio of the sum of between-clusters dispersion and of within-cluster dispersion.",
        "type": "fill_blank",
        "answers": [
            "Variance"
        ],
        "other_options": [
            "Likelihood",
            "Distance",
            "Entropy",
            "Silhouette"
        ]
    },
    {
        "q": "Which Scikit-Learn module contains the `spectral_clustering` function?",
        "type": "mcq",
        "c": "from sklearn.______ import spectral_clustering",
        "o": [
            "cluster",
            "decomposition",
            "manifold",
            "neighbors"
        ]
    },
    {
        "q": "Quadratic Discriminant Analysis (QDA) creates curved decision boundaries because it calculates a separate covariance matrix for each class.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the algorithm to its 'Cluster Model' type:",
        "type": "match",
        "left": [
            "K-Means",
            "DBSCAN",
            "Gaussian Mixture",
            "Hierarchical"
        ],
        "right": [
            "Centroid-based models",
            "Density-based models",
            "Distribution-based models",
            "Connectivity-based models"
        ]
    },
    {
        "q": "In PCA, if you want to reconstruct the original data from the reduced components, you apply the ______ of the transformation matrix.",
        "type": "fill_blank",
        "answers": [
            "transpose"
        ],
        "other_options": [
            "inverse",
            "determinant",
            "trace",
            "square"
        ]
    },
    {
        "q": "What is the output shape of `pca.transform(X)` if you select 2 components for a dataset with 100 samples?",
        "type": "mcq",
        "c": "pca = PCA(n_components=2).fit(X) # X shape (100, 50)",
        "o": [
            "(100, 2)",
            "(2, 100)",
            "(100, 50)",
            "(50, 2)"
        ]
    },
    {
        "q": "The 'Bisecting K-Means' algorithm is more prone to getting stuck in local minima than standard K-Means.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange the hierarchy of concepts in Model-Based Clustering (GMM):",
        "type": "rearrange",
        "words": [
            "Data points",
            "Gaussian Distributions",
            "Mixture Weights",
            "Likelihood Maximization"
        ]
    },
    {
        "q": "Which distance metric is defined as the sum of the absolute differences of their coordinates (also known as City Block)?",
        "type": "mcq",
        "o": [
            "Manhattan Distance",
            "Euclidean Distance",
            "Mahalanobis Distance",
            "Cosine Distance"
        ]
    },
    {
        "q": "In Discriminant Analysis, the 'Confusion Matrix' is used to evaluate the ______ of the classification model.",
        "type": "fill_blank",
        "answers": [
            "accuracy"
        ],
        "other_options": [
            "variance",
            "linearity",
            "covariance",
            "complexity"
        ]
    },
    {
        "q": "Match the PCA variant to its specific benefit:",
        "type": "match",
        "left": [
            "Incremental PCA",
            "Randomized PCA",
            "Sparse PCA",
            "Kernel PCA"
        ],
        "right": [
            "Low memory usage (partial_fit)",
            "Fast approximation for large data",
            "Interpretable components (zeros)",
            "Non-linear dimensionality reduction"
        ]
    },
    {
        "q": "A 'Dendrogram' can be cut at different heights to produce different numbers of flat clusters.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the primary goal of Principal Component Analysis?",
        "type": "mcq",
        "o": [
            "To reduce dimensionality while preserving maximum variance",
            "To identify latent factors that explain correlations",
            "To group similar observations together",
            "To classify observations into predefined groups"
        ]
    },
    {
        "q": "In Factor Analysis, the ______ represent unobserved latent variables that explain the correlations among observed variables.",
        "type": "fill_blank",
        "answers": ["factors"],
        "other_options": ["components", "clusters", "discriminants"]
    },
    {
        "q": "Match the multivariate technique with its primary purpose:",
        "type": "match",
        "left": ["Principal Component Analysis", "Factor Analysis", "Cluster Analysis", "Discriminant Analysis"],
        "right": ["Dimensionality reduction", "Identify latent constructs", "Group similar observations", "Classify into predefined groups"]
    },
    {
        "q": "The first principal component is the direction that captures the ______ variance in the data.",
        "type": "fill_blank",
        "answers": ["maximum"],
        "other_options": ["minimum", "average", "median"]
    },
    {
        "q": "K-means clustering requires specifying the number of clusters before analysis.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the typical steps in performing PCA:",
        "type": "rearrange",
        "words": ["Standardize data", "Compute covariance matrix", "Calculate eigenvectors", "Select principal components"]
    },
    {
        "q": "Which multivariate technique is most appropriate for identifying customer segments based on purchasing behavior?",
        "type": "mcq",
        "o": [
            "Cluster Analysis",
            "Discriminant Analysis",
            "Factor Analysis",
            "Principal Component Analysis"
        ]
    },
    {
        "q": "In Discriminant Analysis, the ______ function maximizes separation between groups while minimizing within-group variance.",
        "type": "fill_blank",
        "answers": ["discriminant"],
        "other_options": ["classification", "clustering", "factor"]
    },
    {
        "q": "Factor loadings represent the correlation between observed variables and ______.",
        "type": "mcq",
        "o": [
            "Latent factors",
            "Principal components",
            "Cluster centers",
            "Discriminant functions"
        ]
    },
    {
        "q": "Hierarchical clustering creates a dendrogram that shows the relationships between observations.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "The proportion of variance explained by each principal component can be determined from the ______.",
        "type": "fill_blank",
        "answers": ["eigenvalues"],
        "other_options": ["eigenvectors", "loadings", "scores"]
    },
    {
        "q": "Match the clustering algorithm with its characteristic:",
        "type": "match",
        "left": ["K-means", "Hierarchical", "DBSCAN", "Gaussian Mixture"],
        "right": ["Requires pre-specified K", "Creates tree structure", "Density-based clusters", "Probabilistic assignment"]
    },
    {
        "q": "In Factor Analysis, the ______ matrix contains the correlations between variables and factors.",
        "type": "fill_blank",
        "answers": ["loading"],
        "other_options": ["rotation", "component", "pattern"]
    },
    {
        "q": "Which technique would be most appropriate for determining which variables best distinguish between successful and unsuccessful marketing campaigns?",
        "type": "mcq",
        "o": [
            "Discriminant Analysis",
            "Principal Component Analysis",
            "Factor Analysis",
            "Cluster Analysis"
        ]
    },
    {
        "q": "Rearrange the steps in typical cluster analysis:",
        "type": "rearrange",
        "words": ["Choose distance metric", "Select algorithm", "Determine number of clusters", "Validate results"]
    },
    {
        "q": "The scree plot in PCA helps determine the optimal number of components to retain by showing the ______ of each component.",
        "type": "fill_blank",
        "answers": ["eigenvalue"],
        "other_options": ["variance", "loading", "score"]
    },
    {
        "q": "In Discriminant Analysis, the prior probability represents the ______ probability of group membership before observing the data.",
        "type": "fill_blank",
        "answers": ["prior"],
        "other_options": ["posterior", "conditional", "marginal"]
    },
    {
        "q": "Factor rotation helps achieve simpler structure by making the factor loadings more interpretable.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which distance metric is most commonly used in K-means clustering?",
        "type": "mcq",
        "o": [
            "Euclidean distance",
            "Manhattan distance",
            "Mahalanobis distance",
            "Cosine similarity"
        ]
    },
    {
        "q": "The ______ in Factor Analysis represents the proportion of variance in each variable explained by the common factors.",
        "type": "fill_blank",
        "answers": ["communality"],
        "other_options": ["uniqueness", "specificity", "loading"]
    },
    {
        "q": "What does the Kaiser-Guttman criterion suggest for selecting principal components?",
        "type": "mcq",
        "o": [
            "Retain components with eigenvalues greater than 1",
            "Retain components that explain 95% of variance",
            "Retain the first three components regardless",
            "Retain components with negative eigenvalues"
        ]
    },
    {
        "q": "In cluster analysis, the ______ index measures how similar an object is to its own cluster compared to other clusters.",
        "type": "fill_blank",
        "answers": ["silhouette"],
        "other_options": ["calinski", "davies", "rand"]
    },
    {
        "q": "Match the rotation method in Factor Analysis with its characteristic:",
        "type": "match",
        "left": ["Varimax", "Quartimax", "Oblimin", "Promax"],
        "right": ["Orthogonal, simplifies columns", "Orthogonal, simplifies rows", "Oblique, allows correlation", "Oblique, power transformation"]
    },
    {
        "q": "Linear Discriminant Analysis assumes that all classes share the same covariance matrix.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "The total variance explained in PCA equals the sum of all ______ of the covariance matrix.",
        "type": "fill_blank",
        "answers": ["eigenvalues"],
        "other_options": ["eigenvectors", "loadings", "components"]
    },
    {
        "q": "Which assumption is NOT required for Factor Analysis?",
        "type": "mcq",
        "o": [
            "Variables are normally distributed",
            "There are no outliers in the data",
            "Sample size should be large enough",
            "Variables are linearly related"
        ]
    },
    {
        "q": "Rearrange the hierarchical clustering methods from most to least sensitive to outliers:",
        "type": "rearrange",
        "words": ["Complete linkage", "Average linkage", "Single linkage", "Ward's method"]
    },
    {
        "q": "In PCA, the principal component scores are calculated as linear combinations of the ______ variables.",
        "type": "fill_blank",
        "answers": ["original"],
        "other_options": ["standardized", "transformed", "reduced"]
    },
    {
        "q": "The Bartlett's test of sphericity in Factor Analysis tests whether the correlation matrix is an identity matrix.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which distance measure is most appropriate for binary data in cluster analysis?",
        "type": "mcq",
        "o": [
            "Jaccard distance",
            "Euclidean distance",
            "Manhattan distance",
            "Mahalanobis distance"
        ]
    },
    {
        "q": "In Discriminant Analysis, the ______ probability is calculated after observing the predictor variables.",
        "type": "fill_blank",
        "answers": ["posterior"],
        "other_options": ["prior", "conditional", "marginal"]
    },
    {
        "q": "The KMO measure in Factor Analysis indicates the ______ of partial correlations among variables.",
        "type": "fill_blank",
        "answers": ["sampling adequacy"],
        "other_options": ["factorability", "reliability", "validity"]
    },
    {
        "q": "Match the clustering validation measure with what it assesses:",
        "type": "match",
        "left": ["Silhouette coefficient", "Calinski-Harabasz", "Davies-Bouldin", "Adjusted Rand"],
        "right": ["Cohesion vs separation", "Between vs within cluster variance", "Average similarity ratio", "Agreement with true labels"]
    },
    {
        "q": "PCA is sensitive to the scaling of variables, so standardization is often recommended.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In Factor Analysis, the ______ variance is the portion of variance unique to each variable and not shared with others.",
        "type": "fill_blank",
        "answers": ["unique"],
        "other_options": ["common", "specific", "error"]
    },
    {
        "q": "Which technique would be most appropriate for reducing 50 correlated variables to a smaller set of uncorrelated components?",
        "type": "mcq",
        "o": [
            "Principal Component Analysis",
            "Factor Analysis",
            "Cluster Analysis",
            "Discriminant Analysis"
        ]
    },
    {
        "q": "Rearrange the steps in performing Factor Analysis:",
        "type": "rearrange",
        "words": ["Test assumptions", "Extract factors", "Determine number of factors", "Rotate factors", "Interpret loadings"]
    },
    {
        "q": "The Wilks' lambda statistic in Discriminant Analysis tests the ______ between groups.",
        "type": "fill_blank",
        "answers": ["overall significance"],
        "other_options": ["pairwise differences", "variable importance", "classification accuracy"]
    },
    {
        "q": "DBSCAN clustering can identify clusters of arbitrary shapes and handle outliers effectively.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In PCA, the proportion of variance explained by component i is calculated as eigenvalue_i divided by ______.",
        "type": "fill_blank",
        "answers": ["sum of all eigenvalues"],
        "other_options": ["number of variables", "total variance", "trace of matrix"]
    },
    {
        "q": "What is the main difference between PCA and Factor Analysis?",
        "type": "mcq",
        "o": [
            "PCA focuses on total variance, FA on shared variance",
            "PCA uses rotation, FA does not",
            "FA creates orthogonal components, PCA does not",
            "PCA requires normally distributed data, FA does not"
        ]
    },
    {
        "q": "In cluster analysis, the elbow method helps determine the optimal number of ______.",
        "type": "fill_blank",
        "answers": ["clusters"],
        "other_options": ["factors", "components", "dimensions"]
    },
    {
        "q": "Match the discriminant analysis type with its description:",
        "type": "match",
        "left": ["LDA", "QDA", "MANOVA", "Fisher's LDA"],
        "right": ["Linear, equal covariance", "Quadratic, different covariance", "Multivariate ANOVA", "Maximizes class separation"]
    },
    {
        "q": "Factor scores in Factor Analysis represent estimates of each observation's position on the latent factors.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "The first step in most multivariate analyses is data ______ to ensure variables are comparable.",
        "type": "fill_blank",
        "answers": ["standardization"],
        "other_options": ["transformation", "cleaning", "reduction"]
    },
    {
        "q": "Which clustering algorithm does NOT require specifying the number of clusters in advance?",
        "type": "mcq",
        "o": [
            "DBSCAN",
            "K-means",
            "Gaussian Mixture Models",
            "Fuzzy C-means"
        ]
    },
    {
        "q": "Rearrange the process of conducting discriminant analysis:",
        "type": "rearrange",
        "words": ["Check assumptions", "Compute discriminant functions", "Test function significance", "Classify cases", "Validate results"]
    },
    {
        "q": "In PCA, when variables are standardized, the covariance matrix becomes the ______ matrix.",
        "type": "fill_blank",
        "answers": ["correlation"],
        "other_options": ["identity", "variance", "loading"]
    },
    {
        "q": "Oblique rotation in Factor Analysis allows factors to be correlated with each other.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "The communality in Factor Analysis ranges from ______ to ______.",
        "type": "fill_blank",
        "answers": ["0", "1"],
        "other_options": ["-1", "1", "0", "100", "-infinity", "infinity"]
    },
    {
        "q": "Which measure is used to assess clustering quality without known true labels?",
        "type": "mcq",
        "o": [
            "Silhouette score",
            "Adjusted Rand index",
            "F-measure",
            "Accuracy"
        ]
    },
    {
        "q": "In discriminant analysis, the ______ rule classifies observations based on highest posterior probability.",
        "type": "fill_blank",
        "answers": ["Bayes"],
        "other_options": ["Fisher", "Euclidean", "Mahalanobis"]
    },
    {
        "q": "Match the PCA concept with its definition:",
        "type": "match",
        "left": ["Eigenvalues", "Eigenvectors", "Loadings", "Scores"],
        "right": ["Variance of components", "Direction of components", "Correlations with components", "Component values for observations"]
    },
    {
        "q": "Factor Analysis assumes that observed variables are linear combinations of underlying factors plus error.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "The scree test in PCA involves plotting ______ against component number.",
        "type": "fill_blank",
        "answers": ["eigenvalues"],
        "other_options": ["variance", "loadings", "correlations"]
    },
    {
        "q": "Which technique is most appropriate for identifying underlying constructs that explain patterns in survey data?",
        "type": "mcq",
        "o": [
            "Factor Analysis",
            "Principal Component Analysis",
            "Cluster Analysis",
            "Discriminant Analysis"
        ]
    },
    {
        "q": "In hierarchical clustering, the ______ method minimizes the maximum distance between clusters.",
        "type": "fill_blank",
        "answers": ["complete linkage"],
        "other_options": ["single linkage", "average linkage", "ward linkage"]
    },
    {
        "q": "Rearrange the types of multivariate analysis from unsupervised to supervised:",
        "type": "rearrange",
        "words": ["Principal Component Analysis", "Factor Analysis", "Cluster Analysis", "Discriminant Analysis"]
    },
    {
        "q": "The parallel analysis method in Factor Analysis compares actual eigenvalues with those from ______ data.",
        "type": "fill_blank",
        "answers": ["random"],
        "other_options": ["standardized", "transformed", "reduced"]
    },
    {
        "q": "QDA (Quadratic Discriminant Analysis) allows each class to have its own covariance matrix.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "The process of varimax rotation in Factor Analysis aims to maximize the ______ of squared loadings.",
        "type": "fill_blank",
        "answers": ["variance"],
        "other_options": ["sum", "average", "range"]
    },
    {
        "q": "Rearrange the steps for validating a cluster solution:",
        "type": "rearrange",
        "words": ["Internal validation", "Stability assessment", "External validation", "Interpretability check"]
    },
    {
        "q": "In Discriminant Analysis, the cost of misclassification can be incorporated through the use of ______.",
        "type": "fill_blank",
        "answers": ["prior probabilities"],
        "other_options": ["distance metrics", "loading matrices", "eigenvalues"]
    },
    {
        "q": "Agglomerative hierarchical clustering begins with each observation as its own cluster.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "The determinant of the within-group covariance matrix is important in which discriminant analysis context?",
        "type": "mcq",
        "o": [
            "Checking the homogeneity of covariance assumption",
            "Calculating factor scores",
            "Determining the number of principal components",
            "Selecting the number of clusters"
        ]
    },
    {
        "q": "Match the multivariate outlier detection method with its approach:",
        "type": "match",
        "left": ["Mahalanobis distance", "Robust PCA", "DBSCAN", "MCD estimator"],
        "right": ["Distance from center accounting for covariance", "PCA resistant to outliers", "Identifies outliers as noise points", "Robust covariance estimation"]
    },
    {
        "q": "The Kaiser-Meyer-Olkin measure indicates whether the data are suitable for ______.",
        "type": "fill_blank",
        "answers": ["Factor Analysis"],
        "other_options": ["Cluster Analysis", "PCA", "Discriminant Analysis"]
    },
    {
        "q": "In PCA, the sum of all eigenvalues equals the total ______ in the data.",
        "type": "fill_blank",
        "answers": ["variance"],
        "other_options": ["covariance", "correlation", "information"]
    },
    {
        "q": "Fuzzy clustering allows observations to belong to multiple clusters simultaneously.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which assumption is critical for Linear Discriminant Analysis but relaxed in Quadratic Discriminant Analysis?",
        "type": "mcq",
        "o": [
            "Homogeneity of covariance matrices",
            "Normality of predictors",
            "Independence of observations",
            "Linearity of relationships"
        ]
    },
    {
        "q": "The proportion of trace statistic in Discriminant Analysis indicates the relative importance of each ______.",
        "type": "fill_blank",
        "answers": ["discriminant function"],
        "other_options": ["predictor variable", "cluster", "factor"]
    },
    {
        "q": "Rearrange the methods for determining factors in Factor Analysis from most to least conservative:",
        "type": "rearrange",
        "words": ["Parallel analysis", "Kaiser criterion", "Scree test", "Variance explained"]
    },
    {
        "q": "The Gower distance is particularly useful for clustering datasets with ______ variable types.",
        "type": "fill_blank",
        "answers": ["mixed"],
        "other_options": ["continuous", "categorical", "binary"]
    },
    {
        "q": "In orthogonal rotation methods like varimax, the factors remain uncorrelated with each other.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which technique would be most appropriate for identifying distinct patient subgroups based on symptom patterns without predefined diagnoses?",
        "type": "mcq",
        "o": [
            "Cluster Analysis",
            "Discriminant Analysis",
            "Principal Component Analysis",
            "Factor Analysis"
        ]
    },
    {
        "q": "The concept of 'simple structure' in Factor Analysis was originally proposed by ______.",
        "type": "fill_blank",
        "answers": ["Thurstone"],
        "other_options": ["Pearson", "Fisher", "Kaiser"]
    },
    {
        "q": "Match the clustering algorithm with its primary advantage:",
        "type": "match",
        "left": ["PAM", "Hierarchical DBSCAN", "Spectral clustering", "Gaussian Mixture"],
        "right": ["Less sensitive to outliers than k-means", "Handles arbitrary shaped clusters", "Works well with non-convex clusters", "Provides probabilistic cluster membership"]
    },
    {
        "q": "In canonical discriminant analysis, the first canonical variable maximizes separation between ______.",
        "type": "fill_blank",
        "answers": ["groups"],
        "other_options": ["variables", "factors", "components"]
    },
    {
        "q": "The determinant of a covariance matrix is related to the overall ______ among variables.",
        "type": "fill_blank",
        "answers": ["multicollinearity"],
        "other_options": ["variance", "correlation", "covariance"]
    },
    {
        "q": "Ward's method in hierarchical clustering minimizes the within-cluster ______.",
        "type": "fill_blank",
        "answers": ["variance"],
        "other_options": ["distance", "correlation", "covariance"]
    },
    {
        "q": "What is the primary purpose of performing a sensitivity analysis after cluster analysis?",
        "type": "mcq",
        "o": [
            "To assess stability of clusters under different conditions",
            "To determine the number of principal components",
            "To calculate factor loadings",
            "To validate discriminant functions"
        ]
    },
    {
        "q": "The ______ index measures agreement between two different cluster solutions on the same data.",
        "type": "fill_blank",
        "answers": ["adjusted rand"],
        "other_options": ["silhouette", "calinski", "davies"]
    },
    {
        "q": "In Factor Analysis, the residual matrix contains the differences between observed correlations and ______ correlations.",
        "type": "fill_blank",
        "answers": ["reproduced"],
        "other_options": ["expected", "theoretical", "estimated"]
    },
    {
        "q": "Principal Component Analysis can be used as a preprocessing step before Cluster Analysis to reduce noise.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the cross-validation technique with its use in multivariate analysis:",
        "type": "match",
        "left": ["Leave-one-out", "k-fold", "Stratified", "Bootstrap"],
        "right": ["Assess classification accuracy", "General model validation", "Maintain class proportions", "Estimate sampling distribution"]
    },
    {
        "q": "Which characteristic distinguishes Factor Analysis from Principal Component Analysis in terms of error treatment?",
        "type": "mcq",
        "o": [
            "FA includes unique variance not explained by common factors",
            "PCA accounts for measurement error in variables",
            "FA assumes no error in variable measurement",
            "PCA separates common and unique variance"
        ]
    },
    {
        "q": "The concept of 'factor indeterminacy' refers to the fact that factor scores are not ______ determined.",
        "type": "fill_blank",
        "answers": ["uniquely"],
        "other_options": ["statistically", "mathematically", "empirically"]
    },
    {
        "q": "Rearrange the hierarchy of multivariate techniques from data reduction to classification:",
        "type": "rearrange",
        "words": ["Principal Component Analysis", "Factor Analysis", "Cluster Analysis", "Discriminant Analysis"]
    },
    {
        "q": "In Discriminant Analysis, the classification functions are derived from ______ probabilities.",
        "type": "fill_blank",
        "answers": ["posterior"],
        "other_options": ["prior", "conditional", "marginal"]
    },
    {
        "q": "The gap statistic method compares within-cluster dispersion with that expected under an appropriate ______ distribution.",
        "type": "fill_blank",
        "answers": ["reference"],
        "other_options": ["normal", "uniform", "random"]
    },
    {
        "q": "Oblique rotation methods allow for correlation among factors, which can be more realistic for social science data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which method is specifically designed for clustering categorical data?",
        "type": "mcq",
        "o": [
            "k-modes",
            "k-means",
            "DBSCAN",
            "Hierarchical clustering"
        ]
    },
    {
        "q": "The ______ plot in Discriminant Analysis shows group separation along the discriminant functions.",
        "type": "fill_blank",
        "answers": ["territorial map"],
        "other_options": ["scree", "loading", "biplot"]
    },
    {
        "q": "In PCA, the biplot simultaneously displays both variable ______ and observation scores.",
        "type": "fill_blank",
        "answers": ["loadings"],
        "other_options": ["weights", "vectors", "directions"]
    },
    {
        "q": "Match the multivariate assumption with its verification method:",
        "type": "match",
        "left": ["Multivariate normality", "Homoscedasticity", "Linearity", "Multicollinearity"],
        "right": ["Mardia's test", "Box's M test", "Scatterplot matrix", "Variance Inflation Factor"]
    },
    {
        "q": "The proportion of correctly classified cases in the training data is called the ______ accuracy.",
        "type": "fill_blank",
        "answers": ["apparent"],
        "other_options": ["actual", "estimated", "cross-validated"]
    },
    {
        "q": "Factor Analysis typically requires a larger sample size than Principal Component Analysis for stable results.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which technique would be most appropriate for identifying the key variables that differentiate between high-performing and low-performing stores?",
        "type": "mcq",
        "o": [
            "Discriminant Analysis",
            "Principal Component Analysis",
            "Factor Analysis",
            "Cluster Analysis"
        ]
    },
    {
        "q": "The ______ method in hierarchical clustering joins clusters that minimize the increase in within-cluster variance.",
        "type": "fill_blank",
        "answers": ["Ward"],
        "other_options": ["single", "complete", "average"]
    },
    {
        "q": "In confirmatory factor analysis, researchers specify the factor structure ______ conducting the analysis.",
        "type": "fill_blank",
        "answers": ["before"],
        "other_options": ["after", "during", "without"]
    },
    {
        "q": "What does a high value on the Davies-Bouldin index indicate about cluster quality?",
        "type": "mcq",
        "o": [
            "Poor separation between clusters",
            "Excellent cluster cohesion",
            "Optimal number of clusters",
            "High variance explained"
        ]
    },
    {
        "q": "The process of ______ involves assigning cluster labels based on the closest centroid in k-means clustering.",
        "type": "fill_blank",
        "answers": ["assignment"],
        "other_options": ["initialization", "optimization", "convergence"]
    },
    {
        "q": "In Discriminant Analysis, the ______ vector contains the coefficients that maximize group separation.",
        "type": "fill_blank",
        "answers": ["eigen"],
        "other_options": ["loading", "factor", "component"]
    },
    {
        "q": "The promax rotation method applies a power transformation to varimax results to achieve oblique simple structure.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the multivariate technique with its typical output visualization:",
        "type": "match",
        "left": ["PCA", "Factor Analysis", "Cluster Analysis", "Discriminant Analysis"],
        "right": ["Scree plot", "Loading plot", "Dendrogram", "Territorial map"]
    },
    {
        "q": "Which measure assesses how well a clustering solution generalizes to new data?",
        "type": "mcq",
        "o": [
            "Stability measures",
            "Silhouette width",
            "Within-cluster sum of squares",
            "Calinski-Harabasz index"
        ]
    },
    {
        "q": "The concept of 'simple structure' in Factor Analysis requires that each variable has ______ on multiple factors.",
        "type": "fill_blank",
        "answers": ["high loadings"],
        "other_options": ["low loadings", "zero loadings", "negative loadings"]
    },
    {
        "q": "Rearrange the typical workflow for market segmentation analysis:",
        "type": "rearrange",
        "words": ["Data collection", "Variable selection", "Cluster analysis", "Profile segments", "Validate segments"]
    },
    {
        "q": "In PCA, the ______ represents the correlation between the original variables and the principal components.",
        "type": "fill_blank",
        "answers": ["loading"],
        "other_options": ["score", "weight", "coefficient"]
    },
    {
        "q": "The Mann-Whitney test can be used to validate cluster solutions by testing variable differences between clusters.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which approach helps determine if two variables load significantly on the same factor?",
        "type": "mcq",
        "o": [
            "Testing the difference between factor loadings",
            "Calculating Euclidean distance",
            "Computing Mahalanobis distance",
            "Examining scree plot eigenvalues"
        ]
    },
    {
        "q": "The ______ criterion in hierarchical clustering determines when to stop merging clusters.",
        "type": "fill_blank",
        "answers": ["stopping"],
        "other_options": ["merging", "splitting", "linking"]
    },
    {
        "q": "In Discriminant Analysis, the classification matrix is also known as the ______ matrix.",
        "type": "fill_blank",
        "answers": ["confusion"],
        "other_options": ["correlation", "covariance", "loading"]
    },
    {
        "q": "Factor score indeterminacy means different methods can produce different scores for the same factor pattern.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which technique is most appropriate for reducing 30 highly correlated psychological test items into underlying personality traits?",
        "type": "mcq",
        "o": [
            "Factor Analysis",
            "Principal Component Analysis",
            "Cluster Analysis",
            "Discriminant Analysis"
        ]
    },
    {
        "q": "The ______ method in cluster analysis begins with all observations in one cluster and successively splits them.",
        "type": "fill_blank",
        "answers": ["divisive"],
        "other_options": ["agglomerative", "partitional", "hierarchical"]
    },
    {
        "q": "In PCA, the proportion of variance explained is calculated as eigenvalue divided by ______.",
        "type": "fill_blank",
        "answers": ["sum of eigenvalues"],
        "other_options": ["number of variables", "total variance", "trace of matrix"]
    },
    {
        "q": "The Tucker-Lewis index is used to assess ______ in Factor Analysis.",
        "type": "fill_blank",
        "answers": ["model fit"],
        "other_options": ["factor loadings", "sample size", "rotation quality"]
    },
    {
        "q": "Regularized Discriminant Analysis combines features of both LDA and QDA to handle small sample sizes.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which clustering algorithm is most robust to outliers and can identify noise points?",
        "type": "mcq",
        "o": [
            "DBSCAN",
            "K-means",
            "Hierarchical clustering",
            "Gaussian mixture models"
        ]
    },
    {
        "q": "What is the primary purpose of using a dendrogram in hierarchical clustering?",
        "type": "mcq",
        "o": [
            "To visualize the nested cluster structure and similarity levels",
            "To determine the optimal number of principal components",
            "To display factor loadings and correlations",
            "To show classification boundaries between groups"
        ]
    },
    {
        "q": "The ______ rotation method in Factor Analysis aims to simplify the columns of the factor loading matrix.",
        "type": "fill_blank",
        "answers": ["varimax"],
        "other_options": ["quartimax", "oblimin", "promax"]
    },
    {
        "q": "In Discriminant Analysis, the ______ function provides the probability of group membership given the predictor values.",
        "type": "fill_blank",
        "answers": ["discriminant"],
        "other_options": ["classification", "probability", "membership"]
    },
    {
        "q": "The Calinski-Harabasz index is also known as the Variance Ratio Criterion.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the clustering validation approach with its description:",
        "type": "match",
        "left": ["Internal validation", "External validation", "Relative validation", "Stability validation"],
        "right": ["Uses intrinsic information only", "Compares to known true labels", "Compares multiple clusterings", "Assesses consistency across samples"]
    },
    {
        "q": "Which characteristic makes Factor Analysis particularly suitable for psychological test development?",
        "type": "mcq",
        "o": [
            "Ability to identify latent constructs from observed variables",
            "Capacity to reduce dimensionality without losing information",
            "Effectiveness in classifying individuals into groups",
            "Efficiency in handling large numbers of variables"
        ]
    },
    {
        "q": "The process of ______ in k-means clustering recalculates centroids based on current cluster assignments.",
        "type": "fill_blank",
        "answers": ["updating"],
        "other_options": ["initializing", "converging", "optimizing"]
    },
    {
        "q": "Rearrange the steps for conducting a robust multivariate analysis:",
        "type": "rearrange",
        "words": ["Check assumptions", "Handle outliers", "Transform variables", "Apply analysis", "Validate results"]
    },
    {
        "q": "In PCA, the ______ matrix contains the correlations between variables and components.",
        "type": "fill_blank",
        "answers": ["loading"],
        "other_options": ["component", "score", "eigenvector"]
    },
    {
        "q": "The Jaccard coefficient measures similarity between clusters by considering only positive matches.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which technique would be most appropriate for identifying the underlying dimensions that explain correlations among customer satisfaction survey items?",
        "type": "mcq",
        "o": [
            "Factor Analysis",
            "Cluster Analysis",
            "Discriminant Analysis",
            "Principal Component Analysis"
        ]
    },
    {
        "q": "The ______ method in hierarchical clustering joins the two clusters with the smallest maximum pairwise distance.",
        "type": "fill_blank",
        "answers": ["complete linkage"],
        "other_options": ["single linkage", "average linkage", "ward linkage"]
    },
    {
        "q": "In Discriminant Analysis, the ______ probability represents the likelihood of group membership before observing the data.",
        "type": "fill_blank",
        "answers": ["prior"],
        "other_options": ["posterior", "conditional", "marginal"]
    },
    {
        "q": "The concept of 'eigenvalue greater than 1' as a factor retention criterion is known as the ______ criterion.",
        "type": "fill_blank",
        "answers": ["Kaiser"],
        "other_options": ["scree", "parallel", "variance"]
    },
    {
        "q": "Multicollinearity among predictor variables can improve the performance of Discriminant Analysis.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which clustering algorithm is particularly effective for identifying clusters of arbitrary shapes and sizes?",
        "type": "mcq",
        "o": [
            "DBSCAN",
            "K-means",
            "Gaussian Mixture Models",
            "Hierarchical clustering"
        ]
    },
    {
        "q": "The ______ plot in PCA displays both variable loadings and observation scores simultaneously.",
        "type": "fill_blank",
        "answers": ["bi"],
        "other_options": ["scree", "loading", "score"]
    },
    {
        "q": "In Factor Analysis, the ______ matrix shows the correlations among factors after oblique rotation.",
        "type": "fill_blank",
        "answers": ["factor correlation"],
        "other_options": ["loading", "pattern", "structure"]
    },
    {
        "q": "The Hartigan-Wong algorithm is an efficient implementation of ______ clustering.",
        "type": "fill_blank",
        "answers": ["k-means"],
        "other_options": ["hierarchical", "density-based", "model-based"]
    },
    {
        "q": "Regularization in Discriminant Analysis helps address issues with small sample sizes and high-dimensional data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the main advantage of using Ward's method in hierarchical clustering?",
        "type": "mcq",
        "o": [
            "It tends to create clusters of relatively equal size",
            "It is the fastest computational method available",
            "It works best with categorical variables only",
            "It requires no distance matrix calculation"
        ]
    },
    {
        "q": "The ______ rotation method in Factor Analysis simplifies the rows of the factor loading matrix.",
        "type": "fill_blank",
        "answers": ["quartimax"],
        "other_options": ["varimax", "oblimin", "promax"]
    },
    {
        "q": "In Discriminant Analysis, the ______ distance accounts for the covariance structure when measuring separation.",
        "type": "fill_blank",
        "answers": ["Mahalanobis"],
        "other_options": ["Euclidean", "Manhattan", "Minkowski"]
    },
    {
        "q": "The silhouette width ranges from -1 to +1, where values close to +1 indicate poor clustering.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the multivariate analysis technique with its data requirement:",
        "type": "match",
        "left": ["PCA", "Factor Analysis", "Cluster Analysis", "Discriminant Analysis"],
        "right": ["No group labels needed", "No group labels needed", "No group labels needed", "Predefined groups required"]
    },
    {
        "q": "Which method is specifically designed to handle the 'curse of dimensionality' in cluster analysis?",
        "type": "mcq",
        "o": [
            "Subspace clustering",
            "K-means clustering",
            "Hierarchical clustering",
            "Density-based clustering"
        ]
    },
    {
        "q": "The process of ______ in Factor Analysis involves estimating the communalities before factor extraction.",
        "type": "fill_blank",
        "answers": ["initialization"],
        "other_options": ["rotation", "extraction", "validation"]
    },
    {
        "q": "Rearrange the typical sequence for model-based clustering:",
        "type": "rearrange",
        "words": ["Select model type", "Estimate parameters", "Determine clusters", "Validate model fit"]
    },
    {
        "q": "In PCA, the ______ represents the coordinates of observations in the new component space.",
        "type": "fill_blank",
        "answers": ["scores"],
        "other_options": ["loadings", "weights", "coefficients"]
    },
    {
        "q": "The adjusted Rand index accounts for chance agreement when comparing cluster solutions.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which technique is most appropriate for reducing a large set of correlated financial ratios while preserving most variance?",
        "type": "mcq",
        "o": [
            "Principal Component Analysis",
            "Factor Analysis",
            "Cluster Analysis",
            "Discriminant Analysis"
        ]
    },
    {
        "q": "The ______ clustering approach uses probability distributions to model cluster membership.",
        "type": "fill_blank",
        "answers": ["model-based"],
        "other_options": ["partitional", "hierarchical", "density-based"]
    },
    {
        "q": "In Discriminant Analysis, the ______ function is linear when covariance matrices are assumed equal across groups.",
        "type": "fill_blank",
        "answers": ["discriminant"],
        "other_options": ["classification", "probability", "distance"]
    },
    {
        "q": "The ______ criterion uses a broken-stick model to determine significant principal components.",
        "type": "fill_blank",
        "answers": ["scree"],
        "other_options": ["Kaiser", "parallel", "variance"]
    },
    {
        "q": "Oblique rotation in Factor Analysis always produces uncorrelated factors.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which validation method involves repeatedly sampling from the data and examining cluster consistency?",
        "type": "mcq",
        "o": [
            "Bootstrap validation",
            "Cross-validation",
            "External validation",
            "Internal validation"
        ]
    },
    {
        "q": "The ______ method in hierarchical clustering joins clusters based on average pairwise distances.",
        "type": "fill_blank",
        "answers": ["average linkage"],
        "other_options": ["single linkage", "complete linkage", "ward linkage"]
    },
    {
        "q": "In Factor Analysis, the ______ matrix contains the unique variances for each variable.",
        "type": "fill_blank",
        "answers": ["specificity"],
        "other_options": ["communality", "loading", "correlation"]
    },
    {
        "q": "The ______ index measures cluster compactness and separation using within and between cluster sums of squares.",
        "type": "fill_blank",
        "answers": ["Calinski-Harabasz"],
        "other_options": ["silhouette", "Davies-Bouldin", "adjusted Rand"]
    },
    {
        "q": "Quadratic Discriminant Analysis can model more complex decision boundaries than Linear Discriminant Analysis.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What distinguishes the objective of Principal Component Analysis from that of Factor Analysis?",
        "type": "mcq",
        "o": [
            "PCA explains total variance while FA explains shared variance",
            "PCA requires normally distributed data while FA does not",
            "FA produces orthogonal components while PCA does not",
            "PCA is used for classification while FA is for clustering"
        ]
    },
    {
        "q": "The ______ method in cluster analysis begins with each observation as a separate cluster and merges the most similar pairs.",
        "type": "fill_blank",
        "answers": ["agglomerative"],
        "other_options": ["divisive", "partitional", "density-based"]
    },
    {
        "q": "In Discriminant Analysis, the ______ rule minimizes the total probability of misclassification.",
        "type": "fill_blank",
        "answers": ["Bayes"],
        "other_options": ["Fisher", "Euclidean", "Maximum Likelihood"]
    },
    {
        "q": "The Gower coefficient can handle mixed data types including continuous, binary, and categorical variables.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the factor extraction method with its key characteristic:",
        "type": "match",
        "left": ["Principal Axis Factoring", "Maximum Likelihood", "Principal Components", "Alpha Factoring"],
        "right": ["Estimates communalities iteratively", "Provides goodness-of-fit test", "Uses 1s in diagonal of correlation matrix", "Maximizes factor reliability"]
    },
    {
        "q": "Which technique is most suitable for identifying homogeneous subgroups in customer data without predefined categories?",
        "type": "mcq",
        "o": [
            "Cluster Analysis",
            "Discriminant Analysis",
            "Principal Component Analysis",
            "Factor Analysis"
        ]
    },
    {
        "q": "The ______ plot displays the cumulative proportion of variance explained by successive principal components.",
        "type": "fill_blank",
        "answers": ["variance explained"],
        "other_options": ["scree", "loading", "biplot"]
    },
    {
        "q": "Rearrange the steps for performing a two-step cluster analysis:",
        "type": "rearrange",
        "words": ["Pre-cluster with BIRCH", "Cluster pre-clusters", "Determine optimal clusters", "Profile final clusters"]
    },
    {
        "q": "In Factor Analysis, the ______ represents the proportion of variance in a variable explained by the common factors.",
        "type": "fill_blank",
        "answers": ["communality"],
        "other_options": ["uniqueness", "specificity", "loading"]
    },
    {
        "q": "The Dunn index identifies well-separated clusters by comparing minimum inter-cluster distance to maximum intra-cluster distance.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which assumption is critical for both Linear Discriminant Analysis and Quadratic Discriminant Analysis?",
        "type": "mcq",
        "o": [
            "Multivariate normality of predictors",
            "Equal covariance matrices across groups",
            "Independence of observations",
            "Linear relationships among predictors"
        ]
    },
    {
        "q": "The ______ method in hierarchical clustering joins the two clusters with the smallest minimum pairwise distance.",
        "type": "fill_blank",
        "answers": ["single linkage"],
        "other_options": ["complete linkage", "average linkage", "ward linkage"]
    },
    {
        "q": "In PCA, the sum of squared loadings for a component equals its ______.",
        "type": "fill_blank",
        "answers": ["eigenvalue"],
        "other_options": ["variance", "correlation", "weight"]
    },
    {
        "q": "The ______ rotation method allows factors to be correlated and uses a parameter to control the degree of obliqueness.",
        "type": "fill_blank",
        "answers": ["oblimin"],
        "other_options": ["varimax", "quartimax", "promax"]
    },
    {
        "q": "K-means clustering always converges to the global optimum solution.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which technique would be most appropriate for determining which variables best distinguish between different species of plants?",
        "type": "mcq",
        "o": [
            "Discriminant Analysis",
            "Principal Component Analysis",
            "Factor Analysis",
            "Cluster Analysis"
        ]
    },
    {
        "q": "The ______ index measures cluster quality by comparing average within-cluster distance to nearest-cluster distance.",
        "type": "fill_blank",
        "answers": ["silhouette"],
        "other_options": ["calinski", "davies", "dunn"]
    },
    {
        "q": "In Discriminant Analysis, the ______ matrix contains the coefficients that define the discriminant functions.",
        "type": "fill_blank",
        "answers": ["canonical"],
        "other_options": ["loading", "correlation", "covariance"]
    },
    {
        "q": "The ______ criterion retains factors that account for a predetermined percentage of total variance.",
        "type": "fill_blank",
        "answers": ["variance explained"],
        "other_options": ["Kaiser", "scree", "parallel"]
    },
    {
        "q": "Factor scores are exact values that can be uniquely determined from the factor loadings and observed variables.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What is the primary limitation of using single linkage in hierarchical clustering?",
        "type": "mcq",
        "o": [
            "It is sensitive to outliers and can create elongated chains",
            "It requires specifying the number of clusters in advance",
            "It cannot handle categorical variables effectively",
            "It assumes spherical clusters of equal size"
        ]
    },
    {
        "q": "The ______ method in Factor Analysis provides statistical tests for the number of factors to retain.",
        "type": "fill_blank",
        "answers": ["maximum likelihood"],
        "other_options": ["principal axis", "alpha factoring", "image factoring"]
    },
    {
        "q": "In Discriminant Analysis, the ______ matrix shows how well the classification functions perform on the training data.",
        "type": "fill_blank",
        "answers": ["confusion"],
        "other_options": ["correlation", "covariance", "loading"]
    },
    {
        "q": "The Hopkins statistic measures the clustering tendency of a dataset by comparing it to a uniform random distribution.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the multivariate technique with its primary mathematical foundation:",
        "type": "match",
        "left": ["PCA", "Factor Analysis", "K-means", "LDA"],
        "right": ["Eigen decomposition", "Common factor model", "Centroid optimization", "Between-group variance maximization"]
    },
    {
        "q": "Which characteristic makes DBSCAN particularly useful for real-world datasets?",
        "type": "mcq",
        "o": [
            "It can identify noise points and clusters of arbitrary shapes",
            "It always produces spherical clusters of equal density",
            "It requires no distance metric calculations",
            "It works best with normally distributed data"
        ]
    },
    {
        "q": "The ______ rotation in Factor Analysis applies a power transformation to achieve near-orthogonal simple structure.",
        "type": "fill_blank",
        "answers": ["promax"],
        "other_options": ["varimax", "quartimax", "oblimin"]
    },
    {
        "q": "Rearrange the process for conducting a comprehensive cluster analysis:",
        "type": "rearrange",
        "words": ["Data preparation", "Distance calculation", "Cluster formation", "Validation", "Interpretation"]
    },
    {
        "q": "In PCA, the ______ of the covariance matrix determine the variance explained by each component.",
        "type": "fill_blank",
        "answers": ["eigenvalues"],
        "other_options": ["eigenvectors", "loadings", "coefficients"]
    },
    {
        "q": "The structure matrix in Factor Analysis shows the correlations between variables and factors after rotation.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which technique is most appropriate for identifying the underlying psychological constructs measured by a personality inventory?",
        "type": "mcq",
        "o": [
            "Factor Analysis",
            "Principal Component Analysis",
            "Cluster Analysis",
            "Discriminant Analysis"
        ]
    },
    {
        "q": "The ______ method in hierarchical clustering minimizes the total within-cluster variance when merging clusters.",
        "type": "fill_blank",
        "answers": ["Ward"],
        "other_options": ["single", "complete", "average"]
    },
    {
        "q": "In Discriminant Analysis, the ______ function provides the linear combination of predictors that best separates groups.",
        "type": "fill_blank",
        "answers": ["canonical"],
        "other_options": ["classification", "probability", "discriminant"]
    },
    {
        "q": "The ______ index measures cluster validity by comparing similarity within and between clusters.",
        "type": "fill_blank",
        "answers": ["Davies-Bouldin"],
        "other_options": ["silhouette", "calinski", "dunn"]
    },
    {
        "q": "Principal Component Analysis assumes that all variance in the variables is common variance.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which method helps determine if the data are suitable for factor analysis by testing the correlation matrix?",
        "type": "mcq",
        "o": [
            "Bartlett's test of sphericity",
            "Kaiser-Meyer-Olkin measure",
            "Scree test",
            "Parallel analysis"
        ]
    },
    {
        "q": "The ______ clustering algorithm uses a density-based approach to identify core points and border points.",
        "type": "fill_blank",
        "answers": ["DBSCAN"],
        "other_options": ["k-means", "hierarchical", "gaussian mixture"]
    },
    {
        "q": "In Factor Analysis, the ______ represents variance unique to each variable and not explained by common factors.",
        "type": "fill_blank",
        "answers": ["uniqueness"],
        "other_options": ["communality", "specificity", "loading"]
    },
    {
        "q": "The ______ criterion compares actual eigenvalues with those from random data to determine factor retention.",
        "type": "fill_blank",
        "answers": ["parallel analysis"],
        "other_options": ["Kaiser", "scree", "variance explained"]
    },
    {
        "q": "Quadratic Discriminant Analysis requires estimating separate covariance matrices for each group.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What problem does the 'curse of dimensionality' pose for Cluster Analysis?",
        "type": "mcq",
        "o": [
            "Distance measures become less meaningful in high-dimensional spaces",
            "Computational time decreases exponentially with more variables",
            "Clusters become more distinct and easier to identify",
            "The need for data standardization is eliminated"
        ]
    },
    {
        "q": "The ______ matrix in Factor Analysis contains the correlations among variables after accounting for the common factors.",
        "type": "fill_blank",
        "answers": ["residual"],
        "other_options": ["reproduced", "pattern", "structure"]
    },
    {
        "q": "In Discriminant Analysis, the ______ approach is used when the group covariance matrices are not equal.",
        "type": "fill_blank",
        "answers": ["quadratic"],
        "other_options": ["linear", "regularized", "canonical"]
    },
    {
        "q": "The cophenetic correlation coefficient measures how well a dendrogram preserves the original pairwise distances.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the clustering algorithm with its key parameter:",
        "type": "match",
        "left": ["K-means", "DBSCAN", "Gaussian Mixture", "Spectral Clustering"],
        "right": ["Number of clusters (k)", "Epsilon distance and min points", "Number of components", "Number of eigenvectors"]
    },
    {
        "q": "Which technique addresses the issue of factor score indeterminacy in Factor Analysis?",
        "type": "mcq",
        "o": [
            "Using multiple estimation methods like regression and Bartlett",
            "Increasing the sample size indefinitely",
            "Applying orthogonal rotation to all factors",
            "Reducing the number of variables in the analysis"
        ]
    },
    {
        "q": "The process of ______ in cluster analysis involves running the algorithm multiple times with different initializations.",
        "type": "fill_blank",
        "answers": ["replication"],
        "other_options": ["validation", "rotation", "extraction"]
    },
    {
        "q": "Rearrange the hierarchy of multivariate techniques from most general to most specific data requirements:",
        "type": "rearrange",
        "words": ["Cluster Analysis", "Factor Analysis", "Principal Component Analysis", "Discriminant Analysis"]
    },
    {
        "q": "In PCA, the ______ represents the contribution of each variable to a principal component.",
        "type": "fill_blank",
        "answers": ["loading"],
        "other_options": ["score", "eigenvalue", "weight"]
    },
    {
        "q": "The silhouette analysis can be performed for individual observations to assess their cluster fit.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which method is particularly useful for determining the optimal number of clusters by comparing within-cluster variation?",
        "type": "mcq",
        "o": [
            "Gap statistic",
            "Adjusted Rand index",
            "Cophenetic correlation",
            "Kaiser criterion"
        ]
    },
    {
        "q": "The ______ rotation method in Factor Analysis seeks to simplify both rows and columns of the loading matrix.",
        "type": "fill_blank",
        "answers": ["equamax"],
        "other_options": ["varimax", "quartimax", "oblimin"]
    },
    {
        "q": "In Discriminant Analysis, the ______ probability is updated after observing the predictor variables.",
        "type": "fill_blank",
        "answers": ["posterior"],
        "other_options": ["prior", "conditional", "joint"]
    },
    {
        "q": "The ______ index identifies well-separated, compact clusters by comparing inter and intra-cluster distances.",
        "type": "fill_blank",
        "answers": ["Dunn"],
        "other_options": ["silhouette", "calinski", "davies"]
    },
    {
        "q": "Factor Analysis typically requires a larger minimum sample size than Principal Component Analysis.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which technique would be most appropriate for identifying market segments based on purchasing behavior patterns?",
        "type": "mcq",
        "o": [
            "Cluster Analysis",
            "Discriminant Analysis",
            "Principal Component Analysis",
            "Factor Analysis"
        ]
    },
    {
        "q": "The ______ method in hierarchical clustering can be sensitive to the order of observations in the dataset.",
        "type": "fill_blank",
        "answers": ["single linkage"],
        "other_options": ["complete linkage", "average linkage", "ward linkage"]
    },
    {
        "q": "In Factor Analysis, the ______ represents the proportion of variance that is not explained by the common factors.",
        "type": "fill_blank",
        "answers": ["uniqueness"],
        "other_options": ["communality", "specificity", "loading"]
    },
    {
        "q": "The ______ criterion uses a broken stick model to determine significant principal components.",
        "type": "fill_blank",
        "answers": ["scree"],
        "other_options": ["Kaiser", "parallel", "variance"]
    },
    {
        "q": "Regularized Discriminant Analysis interpolates between LDA and QDA to handle small sample sizes.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is a key advantage of using model-based clustering over distance-based methods?",
        "type": "mcq",
        "o": [
            "It provides probabilistic cluster assignments and handles overlapping clusters",
            "It requires less computational resources and memory",
            "It always converges to the global optimum solution",
            "It does not require specifying any parameters"
        ]
    },
    {
        "q": "The ______ coefficient measures the agreement between two different hierarchical clustering solutions.",
        "type": "fill_blank",
        "answers": ["cophenetic"],
        "other_options": ["silhouette", "rand", "dunn"]
    },
    {
        "q": "In Discriminant Analysis, the ______ method is used when the number of variables exceeds the number of observations.",
        "type": "fill_blank",
        "answers": ["regularized"],
        "other_options": ["linear", "quadratic", "canonical"]
    },
    {
        "q": "The quartimax rotation method in Factor Analysis simplifies the rows of the factor loading matrix.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the multivariate analysis challenge with its common solution:",
        "type": "match",
        "left": ["High dimensionality", "Missing data", "Outlier sensitivity", "Small sample size"],
        "right": ["PCA preprocessing", "Multiple imputation", "Robust clustering methods", "Regularization techniques"]
    },
    {
        "q": "Which characteristic distinguishes spectral clustering from traditional k-means clustering?",
        "type": "mcq",
        "o": [
            "It can identify non-convex clusters using graph theory",
            "It requires specifying the number of clusters in advance",
            "It uses Euclidean distance as the primary metric",
            "It assumes spherical cluster shapes"
        ]
    },
    {
        "q": "The process of ______ in Factor Analysis involves transforming factors to achieve a more interpretable structure.",
        "type": "fill_blank",
        "answers": ["rotation"],
        "other_options": ["extraction", "validation", "estimation"]
    },
    {
        "q": "Rearrange the steps for performing robust Principal Component Analysis:",
        "type": "rearrange",
        "words": ["Detect outliers", "Apply robust covariance estimation", "Compute robust PCA", "Validate components"]
    },
    {
        "q": "In cluster analysis, the ______ index measures the similarity between different clustering runs on the same data.",
        "type": "fill_blank",
        "answers": ["adjusted rand"],
        "other_options": ["silhouette", "calinski", "davies"]
    },
    {
        "q": "The pattern matrix in Factor Analysis shows the unique relationship between variables and factors after rotation.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which technique is most appropriate for reducing a large set of correlated environmental variables while identifying underlying ecological factors?",
        "type": "mcq",
        "o": [
            "Factor Analysis",
            "Principal Component Analysis",
            "Cluster Analysis",
            "Discriminant Analysis"
        ]
    },
    {
        "q": "The ______ method in hierarchical clustering is particularly sensitive to outliers and can produce elongated chains.",
        "type": "fill_blank",
        "answers": ["single linkage"],
        "other_options": ["complete linkage", "average linkage", "ward linkage"]
    },
    {
        "q": "In Discriminant Analysis, the ______ rule classifies observations based on the smallest Mahalanobis distance to group centroids.",
        "type": "fill_blank",
        "answers": ["minimum distance"],
        "other_options": ["bayes", "fisher", "maximum likelihood"]
    },
    {
        "q": "The ______ statistic tests whether the correlation matrix is significantly different from an identity matrix in Factor Analysis.",
        "type": "fill_blank",
        "answers": ["Bartlett"],
        "other_options": ["Kaiser", "Tucker", "Lewis"]
    },
    {
        "q": "Fuzzy clustering allows observations to have partial membership in multiple clusters simultaneously.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which validation method involves comparing cluster solutions from different subsets of the data?",
        "type": "mcq",
        "o": [
            "Stability analysis",
            "Internal validation",
            "External validation",
            "Hypothesis testing"
        ]
    },
    {
        "q": "The ______ rotation method in Factor Analysis uses a power transformation of an orthogonal solution to achieve oblique simple structure.",
        "type": "fill_blank",
        "answers": ["promax"],
        "other_options": ["varimax", "quartimax", "oblimin"]
    },
    {
        "q": "In PCA, the sum of all eigenvalues equals the trace of the ______ matrix.",
        "type": "fill_blank",
        "answers": ["covariance"],
        "other_options": ["correlation", "loading", "component"]
    },
    {
        "q": "The ______ index measures cluster quality by comparing within-cluster distances to between-cluster distances.",
        "type": "fill_blank",
        "answers": ["Calinski-Harabasz"],
        "other_options": ["silhouette", "davies", "dunn"]
    },
    {
        "q": "Quadratic Discriminant Analysis can model nonlinear decision boundaries between groups.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the primary purpose of using the gap statistic in cluster analysis?",
        "type": "mcq",
        "o": [
            "To compare within-cluster dispersion to expected dispersion under null reference",
            "To measure the agreement between two different clustering solutions",
            "To determine the optimal number of factors in factor analysis",
            "To assess the classification accuracy of discriminant functions"
        ]
    },
    {
        "q": "The ______ matrix in Factor Analysis contains the correlations between factors after oblique rotation.",
        "type": "fill_blank",
        "answers": ["factor correlation"],
        "other_options": ["pattern", "structure", "loading"]
    },
    {
        "q": "In Discriminant Analysis, the ______ approach combines features of both LDA and QDA for better generalization.",
        "type": "fill_blank",
        "answers": ["regularized"],
        "other_options": ["canonical", "fisher", "bayesian"]
    },
    {
        "q": "The Hopkins statistic value close to 0.5 indicates the data are uniformly distributed with no clustering tendency.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the clustering validation type with its primary focus:",
        "type": "match",
        "left": ["Internal", "External", "Relative", "Stability"],
        "right": ["Intrinsic data structure", "Comparison to ground truth", "Comparison of different clusterings", "Consistency across samples"]
    },
    {
        "q": "Which characteristic makes the CLARA algorithm suitable for large datasets?",
        "type": "mcq",
        "o": [
            "It works on samples of the data rather than the entire dataset",
            "It requires no distance matrix computation",
            "It automatically determines the optimal number of clusters",
            "It handles categorical variables without preprocessing"
        ]
    },
    {
        "q": "The process of ______ in hierarchical clustering determines when to stop merging or splitting clusters.",
        "type": "fill_blank",
        "answers": ["cutting"],
        "other_options": ["pruning", "terminating", "finalizing"]
    },
    {
        "q": "Rearrange the steps for performing confirmatory factor analysis:",
        "type": "rearrange",
        "words": ["Specify factor model", "Estimate parameters", "Test model fit", "Modify if needed"]
    },
    {
        "q": "In PCA, the ______ represent the directions of maximum variance in the data.",
        "type": "fill_blank",
        "answers": ["eigenvectors"],
        "other_options": ["eigenvalues", "loadings", "scores"]
    },
    {
        "q": "The adjusted Rand index corrects for chance agreement when comparing cluster partitions.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which technique is most appropriate for identifying the key variables that distinguish between different disease subtypes?",
        "type": "mcq",
        "o": [
            "Discriminant Analysis",
            "Principal Component Analysis",
            "Factor Analysis",
            "Cluster Analysis"
        ]
    },
    {
        "q": "The ______ method in density-based clustering identifies core points, border points, and noise points.",
        "type": "fill_blank",
        "answers": ["DBSCAN"],
        "other_options": ["OPTICS", "HDBSCAN", "Mean Shift"]
    },
    {
        "q": "In Factor Analysis, the ______ represents the variance common to all variables in the analysis.",
        "type": "fill_blank",
        "answers": ["communality"],
        "other_options": ["uniqueness", "specificity", "loading"]
    },
    {
        "q": "The ______ criterion retains principal components that explain more variance than the average component.",
        "type": "fill_blank",
        "answers": ["Kaiser"],
        "other_options": ["scree", "parallel", "broken stick"]
    },
    {
        "q": "Fuzzy C-means clustering allows observations to belong to multiple clusters with varying degrees of membership.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which method is particularly useful for validating cluster stability when true labels are unknown?",
        "type": "mcq",
        "o": [
            "Bootstrap resampling",
            "External validation indices",
            "Hypothesis testing of means",
            "Correlation analysis"
        ]
    },
    {
        "q": "The ______ rotation method in Factor Analysis seeks to simplify the factor structure while allowing factors to correlate.",
        "type": "fill_blank",
        "answers": ["oblimin"],
        "other_options": ["varimax", "quartimax", "equamax"]
    },
    {
        "q": "In Discriminant Analysis, the ______ function provides the probability of group membership given the observed data.",
        "type": "fill_blank",
        "answers": ["posterior"],
        "other_options": ["prior", "likelihood", "discriminant"]
    },
    {
        "q": "The ______ index measures cluster compactness using the average distance between cluster points and their centroid.",
        "type": "fill_blank",
        "answers": ["within-cluster sum of squares"],
        "other_options": ["silhouette", "davies-bouldin", "calinski-harabasz"]
    },
    {
        "q": "Principal Component Analysis can be used as a preprocessing step for Discriminant Analysis to handle multicollinearity.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What problem does the 'masking effect' describe in cluster analysis?",
        "type": "mcq",
        "o": [
            "Small clusters being hidden by larger clusters in certain algorithms",
            "Variables with high variance dominating principal components",
            "Correlated factors obscuring simple structure in rotation",
            "Outliers influencing discriminant function coefficients"
        ]
    },
    {
        "q": "The ______ method in Factor Analysis estimates factors by maximizing the likelihood function under normality assumptions.",
        "type": "fill_blank",
        "answers": ["maximum likelihood"],
        "other_options": ["principal axis", "alpha factoring", "image factoring"]
    },
    {
        "q": "In Discriminant Analysis, the ______ matrix contains the coefficients that define the canonical discriminant functions.",
        "type": "fill_blank",
        "answers": ["canonical coefficient"],
        "other_options": ["structure", "pattern", "loading"]
    },
    {
        "q": "The silhouette analysis provides both global and observation-specific measures of cluster quality.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the multivariate technique with its typical data scaling requirement:",
        "type": "match",
        "left": ["PCA with covariance matrix", "PCA with correlation matrix", "Cluster Analysis", "Factor Analysis"],
        "right": ["Variables on similar scales", "Variables on different scales", "Depends on distance metric", "Typically uses correlation matrix"]
    },
    {
        "q": "Which characteristic makes the PAM algorithm more robust to outliers than k-means?",
        "type": "mcq",
        "o": [
            "It uses medoids which are actual data points as cluster centers",
            "It requires fewer iterations to converge to a solution",
            "It automatically determines the optimal number of clusters",
            "It works with any distance metric without computation"
        ]
    },
    {
        "q": "The process of ______ in hierarchical clustering creates the tree structure that shows cluster relationships.",
        "type": "fill_blank",
        "answers": ["linkage"],
        "other_options": ["merging", "agglomeration", "nesting"]
    },
    {
        "q": "Rearrange the steps for performing multiple discriminant analysis:",
        "type": "rearrange",
        "words": ["Check assumptions", "Compute discriminant functions", "Test function significance", "Interpret structure matrix", "Classify cases"]
    },
    {
        "q": "In PCA, the ______ indicate the amount of variance carried by each principal component.",
        "type": "fill_blank",
        "answers": ["eigenvalues"],
        "other_options": ["eigenvectors", "loadings", "scores"]
    },
    {
        "q": "The pattern matrix and structure matrix are identical when using orthogonal rotation in Factor Analysis.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which technique is most appropriate for identifying homogeneous subgroups in a customer database for targeted marketing?",
        "type": "mcq",
        "o": [
            "Cluster Analysis",
            "Discriminant Analysis",
            "Principal Component Analysis",
            "Factor Analysis"
        ]
    },
    {
        "q": "The ______ method in hierarchical clustering minimizes the increase in total within-cluster variance.",
        "type": "fill_blank",
        "answers": ["Ward"],
        "other_options": ["single", "complete", "average"]
    },
    {
        "q": "In Factor Analysis, the ______ represents the correlation between a variable and a factor after rotation.",
        "type": "fill_blank",
        "answers": ["structure coefficient"],
        "other_options": ["pattern coefficient", "loading", "weight"]
    },
    {
        "q": "The ______ criterion compares actual eigenvalues with those from random data to determine component retention.",
        "type": "fill_blank",
        "answers": ["parallel analysis"],
        "other_options": ["Kaiser", "scree", "variance explained"]
    },
    {
        "q": "Gaussian Mixture Models assume that data within each cluster follow a multivariate normal distribution.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which method helps assess whether a clustering solution is stable across different samples from the same population?",
        "type": "mcq",
        "o": [
            "Bootstrap validation",
            "Silhouette analysis",
            "Gap statistic",
            "Calinski-Harabasz index"
        ]
    },
    {
        "q": "The ______ rotation method in Factor Analysis seeks simple structure by focusing on variable complexity rather than factor complexity.",
        "type": "fill_blank",
        "answers": ["quartimax"],
        "other_options": ["varimax", "oblimin", "promax"]
    },
    {
        "q": "In Discriminant Analysis, the ______ probability represents the baseline likelihood of group membership.",
        "type": "fill_blank",
        "answers": ["prior"],
        "other_options": ["posterior", "conditional", "joint"]
    },
    {
        "q": "The ______ index identifies well-separated clusters by finding the minimum inter-cluster distance divided by maximum intra-cluster distance.",
        "type": "fill_blank",
        "answers": ["Dunn"],
        "other_options": ["silhouette", "davies-bouldin", "calinski-harabasz"]
    },
    {
        "q": "Factor scores are always uniquely determined and have the same values regardless of the estimation method used.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What is the primary challenge addressed by using robust PCA methods?",
        "type": "mcq",
        "o": [
            "Sensitivity to outliers and violations of normality assumptions",
            "Inability to handle categorical variables in the analysis",
            "Requirement for large sample sizes to achieve stable results",
            "Difficulty in interpreting the resulting component loadings"
        ]
    },
    {
        "q": "The ______ index measures clustering quality by computing the average similarity between each point and its cluster compared to other clusters.",
        "type": "fill_blank",
        "answers": ["silhouette"],
        "other_options": ["calinski", "davies", "dunn"]
    },
    {
        "q": "In Discriminant Analysis, the ______ matrix shows the correlations between predictors and discriminant functions.",
        "type": "fill_blank",
        "answers": ["structure"],
        "other_options": ["pattern", "loading", "canonical"]
    },
    {
        "q": "The broken stick criterion is more conservative than the Kaiser criterion for selecting principal components.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the factor extraction method with its key assumption:",
        "type": "match",
        "left": ["Principal Axis Factoring", "Maximum Likelihood", "Unweighted Least Squares", "Generalized Least Squares"],
        "right": ["Iterative estimation of communalities", "Multivariate normality of variables", "Minimizes residual sum of squares", "Accounts for variable correlations"]
    },
    {
        "q": "Which characteristic makes OPTICS superior to DBSCAN for cluster analysis?",
        "type": "mcq",
        "o": [
            "It can identify clusters at varying density levels within the same dataset",
            "It requires fewer parameters to be specified by the user",
            "It automatically determines the optimal number of clusters",
            "It works faster on large high-dimensional datasets"
        ]
    },
    {
        "q": "The process of ______ in Factor Analysis helps achieve Thurstone's simple structure criteria.",
        "type": "fill_blank",
        "answers": ["rotation"],
        "other_options": ["extraction", "validation", "estimation"]
    },
    {
        "q": "Rearrange the typical workflow for mixture model clustering:",
        "type": "rearrange",
        "words": ["Select number of components", "Initialize parameters", "Run EM algorithm", "Assess model fit", "Assign cluster membership"]
    },
    {
        "q": "In PCA, the ______ represent the original variables' contributions to each principal component.",
        "type": "fill_blank",
        "answers": ["loadings"],
        "other_options": ["scores", "eigenvalues", "weights"]
    },
    {
        "q": "The Calinski-Harabasz index is also known as the Variance Ratio Criterion.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which technique is most appropriate for understanding the underlying dimensions that explain correlations among psychological test items?",
        "type": "mcq",
        "o": [
            "Factor Analysis",
            "Principal Component Analysis",
            "Cluster Analysis",
            "Discriminant Analysis"
        ]
    },
    {
        "q": "The ______ method in hierarchical clustering can produce compact, spherical clusters but is sensitive to outliers.",
        "type": "fill_blank",
        "answers": ["Ward"],
        "other_options": ["single", "complete", "average"]
    },
    {
        "q": "In Discriminant Analysis, the ______ function provides the decision boundary for classification between groups.",
        "type": "fill_blank",
        "answers": ["discriminant"],
        "other_options": ["canonical", "classification", "probability"]
    },
    {
        "q": "The ______ criterion retains factors that account for a predetermined percentage of total variance.",
        "type": "fill_blank",
        "answers": ["variance explained"],
        "other_options": ["Kaiser", "scree", "parallel"]
    },
    {
        "q": "Spectral clustering can identify non-convex clusters that traditional k-means cannot detect.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which method is particularly useful for determining if data have natural clustering tendency before analysis?",
        "type": "mcq",
        "o": [
            "Hopkins statistic",
            "Silhouette analysis",
            "Gap statistic",
            "Adjusted Rand index"
        ]
    },
    {
        "q": "The ______ rotation method in Factor Analysis uses a power transformation to achieve near-orthogonal simple structure.",
        "type": "fill_blank",
        "answers": ["promax"],
        "other_options": ["varimax", "quartimax", "oblimin"]
    },
    {
        "q": "In cluster analysis, the ______ distance metric is particularly suitable for binary data.",
        "type": "fill_blank",
        "answers": ["Jaccard"],
        "other_options": ["Euclidean", "Manhattan", "Mahalanobis"]
    },
    {
        "q": "The ______ index measures cluster validity by computing the average similarity ratio of each cluster.",
        "type": "fill_blank",
        "answers": ["Davies-Bouldin"],
        "other_options": ["silhouette", "calinski", "dunn"]
    },
    {
        "q": "Regularized Discriminant Analysis helps address multicollinearity issues in high-dimensional data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What issue does the 'swamping effect' describe in cluster analysis?",
        "type": "mcq",
        "o": [
            "Normal points being incorrectly identified as outliers due to masking",
            "Small clusters merging into larger ones during hierarchical clustering",
            "Variables with low variance being ignored in PCA",
            "Factors becoming too correlated during oblique rotation"
        ]
    },
    {
        "q": "The ______ method in Factor Analysis is particularly useful when the normality assumption is violated.",
        "type": "fill_blank",
        "answers": ["principal axis factoring"],
        "other_options": ["maximum likelihood", "generalized least squares", "image factoring"]
    },
    {
        "q": "In Discriminant Analysis, the ______ approach is preferred when the number of variables exceeds the sample size.",
        "type": "fill_blank",
        "answers": ["penalized"],
        "other_options": ["linear", "quadratic", "canonical"]
    },
    {
        "q": "The mutual information score can be used to validate cluster solutions when true labels are available.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the clustering algorithm with its most suitable data type:",
        "type": "match",
        "left": ["K-modes", "Spectral clustering", "Gaussian Mixture", "BIRCH"],
        "right": ["Categorical data", "Non-convex clusters", "Continuous, normally distributed data", "Very large datasets"]
    },
    {
        "q": "Which problem is specifically addressed by using the Minimum Covariance Determinant in robust PCA?",
        "type": "mcq",
        "o": [
            "Contamination of estimates by multivariate outliers",
            "Inability to handle missing data in the dataset",
            "Slow computation speed with large numbers of variables",
            "Difficulty in interpreting rotated component matrices"
        ]
    },
    {
        "q": "The process of ______ in hierarchical clustering helps determine the optimal cut height for the dendrogram.",
        "type": "fill_blank",
        "answers": ["dynamic tree cutting"],
        "other_options": ["static pruning", "height selection", "branch optimization"]
    },
    {
        "q": "Rearrange the steps for performing sparse PCA:",
        "type": "rearrange",
        "words": ["Standardize data", "Apply sparsity constraint", "Compute sparse components", "Select regularization parameter", "Validate interpretation"]
    },
    {
        "q": "In Factor Analysis, the ______ matrix contains the unique relationships between variables and factors after oblique rotation.",
        "type": "fill_blank",
        "answers": ["pattern"],
        "other_options": ["structure", "loading", "correlation"]
    },
    {
        "q": "The Fowlkes-Mallows index measures the similarity between two cluster solutions.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which technique is most appropriate for identifying subtle subgroups in genetic data where clusters may overlap?",
        "type": "mcq",
        "o": [
            "Fuzzy clustering",
            "K-means clustering",
            "Hierarchical clustering",
            "DBSCAN clustering"
        ]
    },
    {
        "q": "The ______ method in Factor Analysis is based on maximizing the reliability of factors.",
        "type": "fill_blank",
        "answers": ["alpha factoring"],
        "other_options": ["image factoring", "principal axis", "maximum likelihood"]
    },
    {
        "q": "In cluster analysis, the ______ index is particularly useful for comparing cluster solutions with different numbers of clusters.",
        "type": "fill_blank",
        "answers": ["adjusted rand"],
        "other_options": ["silhouette", "calinski", "davies"]
    },
    {
        "q": "The ______ criterion uses Monte Carlo simulation to determine significant principal components.",
        "type": "fill_blank",
        "answers": ["parallel analysis"],
        "other_options": ["Kaiser", "scree", "variance explained"]
    },
    {
        "q": "Non-negative matrix factorization can be used as an alternative to PCA for parts-based representation.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which method helps address the problem of factor score indeterminacy in Factor Analysis?",
        "type": "mcq",
        "o": [
            "Using multiple estimation methods and comparing results",
            "Increasing the number of variables in the analysis",
            "Applying varimax rotation to all factors",
            "Reducing the sample size to eliminate noise"
        ]
    },
    {
        "q": "The ______ clustering algorithm builds a tree structure to handle large datasets efficiently.",
        "type": "fill_blank",
        "answers": ["BIRCH"],
        "other_options": ["DBSCAN", "OPTICS", "PAM"]
    },
    {
        "q": "In Discriminant Analysis, the ______ matrix contains the within-group covariance information.",
        "type": "fill_blank",
        "answers": ["pooled covariance"],
        "other_options": ["total covariance", "between-group covariance", "correlation"]
    },
    {
        "q": "The ______ index measures cluster quality by computing the mean similarity between clusters.",
        "type": "fill_blank",
        "answers": ["Davies-Bouldin"],
        "other_options": ["silhouette", "calinski", "dunn"]
    },
    {
        "q": "Sparse PCA tends to produce more interpretable components by forcing some loadings to zero.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What distinguishes the objective of Multiple Discriminant Analysis from Two-Group Discriminant Analysis?",
        "type": "mcq",
        "o": [
            "MDA finds multiple discriminant functions for more than two groups",
            "MDA requires normally distributed data while two-group does not",
            "Two-group DA uses quadratic functions while MDA uses linear",
            "MDA is unsupervised while two-group DA is supervised"
        ]
    },
    {
        "q": "The ______ coefficient measures how well a dendrogram preserves the original pairwise distances between observations.",
        "type": "fill_blank",
        "answers": ["cophenetic"],
        "other_options": ["silhouette", "calinski", "dunn"]
    },
    {
        "q": "In Factor Analysis, the ______ method extracts factors based on the squared multiple correlations of each variable.",
        "type": "fill_blank",
        "answers": ["image"],
        "other_options": ["alpha", "principal", "maximum"]
    },
    {
        "q": "The variance inflation factor can be used to detect multicollinearity in Discriminant Analysis.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the clustering challenge with its solution approach:",
        "type": "match",
        "left": ["Varying cluster densities", "High-dimensional data", "Mixed data types", "Determining cluster number"],
        "right": ["HDBSCAN", "Subspace clustering", "Gower's distance", "Gap statistic"]
    },
    {
        "q": "Which characteristic makes Factor Analysis preferable to PCA for test development in psychology?",
        "type": "mcq",
        "o": [
            "It distinguishes between common and unique variance components",
            "It requires smaller sample sizes for stable results",
            "It produces orthogonal factors by default",
            "It works better with normally distributed variables"
        ]
    },
    {
        "q": "The process of ______ in cluster analysis involves removing features that do not contribute to cluster separation.",
        "type": "fill_blank",
        "answers": ["feature selection"],
        "other_options": ["dimensionality reduction", "data transformation", "outlier removal"]
    },
    {
        "q": "Rearrange the steps for performing robust cluster analysis:",
        "type": "rearrange",
        "words": ["Detect outliers", "Choose robust algorithm", "Apply clustering", "Validate robustness"]
    },
    {
        "q": "In PCA, the ______ matrix contains the coordinates of observations in the reduced component space.",
        "type": "fill_blank",
        "answers": ["score"],
        "other_options": ["loading", "eigenvector", "component"]
    },
    {
        "q": "The mutual information score can assess agreement between clustering solutions when true labels are unknown.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which technique is most appropriate for identifying the key features that distinguish between multiple customer segments identified through clustering?",
        "type": "mcq",
        "o": [
            "Follow-up Discriminant Analysis",
            "Additional Factor Analysis",
            "Secondary PCA",
            "Nested clustering"
        ]
    },
    {
        "q": "The ______ method in hierarchical clustering can produce elongated chains due to its sensitivity to close points.",
        "type": "fill_blank",
        "answers": ["single linkage"],
        "other_options": ["complete linkage", "average linkage", "ward linkage"]
    },
    {
        "q": "In Factor Analysis, the ______ represents the proportion of variance unique to each variable.",
        "type": "fill_blank",
        "answers": ["uniqueness"],
        "other_options": ["communality", "specificity", "loading"]
    },
    {
        "q": "The ______ criterion retains components that explain more variance than subsequent components in PCA.",
        "type": "fill_blank",
        "answers": ["scree"],
        "other_options": ["Kaiser", "parallel", "variance"]
    },
    {
        "q": "Fuzzy clustering algorithms assign membership values between 0 and 1 for each observation.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which validation approach is most appropriate when true cluster labels are available?",
        "type": "mcq",
        "o": [
            "External validation indices",
            "Internal validation indices",
            "Relative validation indices",
            "Stability measures"
        ]
    },
    {
        "q": "The ______ rotation method in Factor Analysis seeks to simplify both rows and columns of the loading matrix equally.",
        "type": "fill_blank",
        "answers": ["equamax"],
        "other_options": ["varimax", "quartimax", "oblimin"]
    },
    {
        "q": "In Discriminant Analysis, the ______ rule minimizes the total cost of misclassification.",
        "type": "fill_blank",
        "answers": ["Bayes"],
        "other_options": ["Fisher", "Euclidean", "Mahalanobis"]
    },
    {
        "q": "The ______ index measures cluster separation by comparing within-cluster to between-cluster distances.",
        "type": "fill_blank",
        "answers": ["Calinski-Harabasz"],
        "other_options": ["silhouette", "davies", "dunn"]
    },
    {
        "q": "Sparse Discriminant Analysis can improve interpretation by selecting relevant variables.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the primary advantage of using the CLARA algorithm over PAM for large datasets?",
        "type": "mcq",
        "o": [
            "It works on data samples rather than the entire dataset",
            "It automatically determines the optimal number of clusters",
            "It handles categorical variables without preprocessing",
            "It produces more spherical clusters than PAM"
        ]
    },
    {
        "q": "The ______ matrix in Factor Analysis shows the correlations between variables after the common factors have been extracted.",
        "type": "fill_blank",
        "answers": ["residual"],
        "other_options": ["reproduced", "pattern", "structure"]
    },
    {
        "q": "In Discriminant Analysis, the ______ method handles the case where some classes have very few observations.",
        "type": "fill_blank",
        "answers": ["regularized"],
        "other_options": ["quadratic", "linear", "canonical"]
    },
    {
        "q": "The Gower distance can simultaneously handle continuous, ordinal, and nominal variables in cluster analysis.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the multivariate analysis assumption violation with its impact:",
        "type": "match",
        "left": ["Non-normality in DA", "Heteroscedasticity in DA", "Multicollinearity in PCA", "Outliers in clustering"],
        "right": ["Biased classification boundaries", "Reduced classification accuracy", "Unstable component loadings", "Distorted cluster centers"]
    },
    {
        "q": "Which characteristic makes CHAMELEON clustering effective for complex datasets?",
        "type": "mcq",
        "o": [
            "It considers both inter-connectivity and closeness of clusters",
            "It requires no parameters and works automatically",
            "It is specifically designed for high-dimensional data only",
            "It assumes all clusters have equal density and size"
        ]
    },
    {
        "q": "The process of ______ in Factor Analysis involves estimating how well the factor model reproduces the original correlations.",
        "type": "fill_blank",
        "answers": ["goodness-of-fit assessment"],
        "other_options": ["model specification", "factor extraction", "rotation validation"]
    },
    {
        "q": "Rearrange the steps for performing kernel PCA:",
        "type": "rearrange",
        "words": ["Choose kernel function", "Compute kernel matrix", "Center kernel matrix", "Find eigenvectors", "Project data"]
    },
    {
        "q": "In cluster analysis, the ______ index measures the agreement between clustering and known true labels.",
        "type": "fill_blank",
        "answers": ["adjusted rand"],
        "other_options": ["silhouette", "calinski", "davies"]
    },
    {
        "q": "The Tucker-Lewis Index compares the fitted model to a null model in Factor Analysis.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which technique is most appropriate for identifying the underlying factors that explain correlations among environmental variables across different regions?",
        "type": "mcq",
        "o": [
            "Multi-group Factor Analysis",
            "Principal Component Analysis",
            "Discriminant Analysis",
            "Two-step clustering"
        ]
    },
    {
        "q": "The ______ method in hierarchical clustering is least sensitive to outliers but can produce compact clusters.",
        "type": "fill_blank",
        "answers": ["complete linkage"],
        "other_options": ["single linkage", "average linkage", "ward linkage"]
    },
    {
        "q": "In Discriminant Analysis, the ______ function provides the linear combination that maximizes group separation.",
        "type": "fill_blank",
        "answers": ["canonical"],
        "other_options": ["classification", "probability", "discriminant"]
    },
    {
        "q": "The ______ criterion in Factor Analysis uses hypothesis testing to determine the number of factors.",
        "type": "fill_blank",
        "answers": ["likelihood ratio test"],
        "other_options": ["Kaiser", "scree", "parallel"]
    },
    {
        "q": "Spectral clustering can capture complex cluster structures that are non-linearly separable.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which method helps identify the most representative variables for each principal component?",
        "type": "mcq",
        "o": [
            "Examining component loadings above a threshold",
            "Calculating the correlation between components",
            "Performing parallel analysis on loadings",
            "Applying varimax rotation to components"
        ]
    },
    {
        "q": "The ______ clustering approach uses probability distributions to model each cluster.",
        "type": "fill_blank",
        "answers": ["model-based"],
        "other_options": ["partitional", "hierarchical", "density-based"]
    },
    {
        "q": "In Factor Analysis, the ______ represents the variance explained by the common factors for each variable.",
        "type": "fill_blank",
        "answers": ["communality"],
        "other_options": ["uniqueness", "specificity", "loading"]
    },
    {
        "q": "The ______ index measures cluster quality by computing the average similarity to the closest cluster.",
        "type": "fill_blank",
        "answers": ["silhouette"],
        "other_options": ["calinski", "davies", "dunn"]
    },
    {
        "q": "Quadratic Discriminant Analysis requires more parameters to estimate than Linear Discriminant Analysis.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What problem does the 'empty cluster' issue describe in k-means clustering?",
        "type": "mcq",
        "o": [
            "A cluster centroid having no data points assigned to it",
            "Clusters overlapping completely with no separation",
            "The algorithm converging to too few clusters",
            "Outliers forming their own single-point clusters"
        ]
    },
    {
        "q": "The ______ method in Factor Analysis is based on the concept of image analysis from psychometrics.",
        "type": "fill_blank",
        "answers": ["image factoring"],
        "other_options": ["alpha factoring", "principal axis", "maximum likelihood"]
    },
    {
        "q": "In Discriminant Analysis, the ______ matrix contains the total variability in the predictor variables.",
        "type": "fill_blank",
        "answers": ["total covariance"],
        "other_options": ["within-group covariance", "between-group covariance", "pooled covariance"]
    },
    {
        "q": "The gamma index can be used to compare the similarity between two hierarchical clusterings.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the multivariate analysis preprocessing step with its purpose:",
        "type": "match",
        "left": ["Data standardization", "Missing value imputation", "Outlier treatment", "Collinearity check"],
        "right": ["Equal variable contribution", "Complete cases for analysis", "Reduce extreme value influence", "Detect redundant variables"]
    },
    {
        "q": "Which characteristic makes BIRCH clustering suitable for very large datasets?",
        "type": "mcq",
        "o": [
            "It uses a CF-tree structure that requires only one data scan",
            "It automatically determines the optimal number of clusters",
            "It works with any distance metric without computation",
            "It handles mixed data types without preprocessing"
        ]
    },
    {
        "q": "The process of ______ in Factor Analysis helps identify if additional factors would improve model fit.",
        "type": "fill_blank",
        "answers": ["scree test examination"],
        "other_options": ["rotation application", "communality estimation", "goodness-of-fit testing"]
    },
    {
        "q": "Rearrange the steps for performing fuzzy clustering:",
        "type": "rearrange",
        "words": ["Initialize membership matrix", "Compute cluster centroids", "Update memberships", "Check convergence", "Defuzzify if needed"]
    },
    {
        "q": "In PCA, the ______ indicate the importance of each variable to the principal components.",
        "type": "fill_blank",
        "answers": ["loadings"],
        "other_options": ["scores", "eigenvalues", "weights"]
    },
    {
        "q": "The Jaccard similarity coefficient is particularly useful for comparing cluster solutions with binary data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which technique is most appropriate for reducing a large set of correlated stock returns while preserving market movement patterns?",
        "type": "mcq",
        "o": [
            "Principal Component Analysis",
            "Factor Analysis",
            "Cluster Analysis",
            "Discriminant Analysis"
        ]
    },
    {
        "q": "The ______ method in hierarchical clustering can produce balanced clusters but is sensitive to metric choice.",
        "type": "fill_blank",
        "answers": ["Ward"],
        "other_options": ["single", "complete", "average"]
    },
    {
        "q": "In Discriminant Analysis, the ______ probability is calculated using Bayes theorem with prior probabilities.",
        "type": "fill_blank",
        "answers": ["posterior"],
        "other_options": ["prior", "conditional", "marginal"]
    },
    {
        "q": "The ______ criterion in Factor Analysis compares the fit of different factor solutions using information criteria.",
        "type": "fill_blank",
        "answers": ["model selection"],
        "other_options": ["Kaiser", "scree", "parallel"]
    },
    {
        "q": "Gaussian Mixture Models can approximate any continuous distribution given enough components.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which method helps determine if a clustering solution is robust to different initialization methods?",
        "type": "mcq",
        "o": [
            "Multiple random starts analysis",
            "Silhouette width calculation",
            "Gap statistic computation",
            "External validation with labels"
        ]
    },
    {
        "q": "The ______ rotation method in Factor Analysis allows researchers to control the degree of factor correlation.",
        "type": "fill_blank",
        "answers": ["oblimin"],
        "other_options": ["varimax", "quartimax", "promax"]
    },
    {
        "q": "In cluster analysis, the ______ distance is particularly suitable for probability distributions.",
        "type": "fill_blank",
        "answers": ["Bhattacharyya"],
        "other_options": ["Euclidean", "Manhattan", "Mahalanobis"]
    },
    {
        "q": "The ______ index identifies compact, well-separated clusters using minimum and maximum distances.",
        "type": "fill_blank",
        "answers": ["Dunn"],
        "other_options": ["silhouette", "calinski", "davies"]
    },
    {
        "q": "Sparse PCA can help with variable selection by forcing some loadings to be exactly zero.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What distinguishes the objective of confirmatory factor analysis from exploratory factor analysis?",
        "type": "mcq",
        "o": [
            "CFA tests a pre-specified factor structure while EFA discovers structure from data",
            "CFA requires orthogonal factors while EFA allows oblique rotation",
            "EFA uses maximum likelihood estimation while CFA uses principal axis",
            "CFA works with smaller sample sizes than EFA"
        ]
    },
    {
        "q": "The ______ index measures clustering stability by comparing solutions from different data subsamples.",
        "type": "fill_blank",
        "answers": ["bootstrap stability"],
        "other_options": ["silhouette", "adjusted rand", "jaccard"]
    },
    {
        "q": "In Discriminant Analysis, the ______ function provides the probability of misclassification for each observation.",
        "type": "fill_blank",
        "answers": ["posterior probability"],
        "other_options": ["prior probability", "discriminant score", "canonical function"]
    },
    {
        "q": "The Hartigan index can be used to determine the optimal number of clusters in k-means clustering.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the multivariate technique with its typical assumption about group structure:",
        "type": "match",
        "left": ["PCA", "Factor Analysis", "K-means", "LDA"],
        "right": ["No group structure assumed", "Latent groups explain correlations", "Spherical clusters of equal size", "Predefined groups with linear boundaries"]
    },
    {
        "q": "Which characteristic makes OPTICS clustering superior to DBSCAN for datasets with varying densities?",
        "type": "mcq",
        "o": [
            "It produces a reachability plot that shows clusters at multiple density levels",
            "It requires only one parameter instead of two",
            "It automatically determines the exact number of clusters",
            "It works faster on high-dimensional data"
        ]
    },
    {
        "q": "The process of ______ in Factor Analysis involves checking if the residual correlations are small and random.",
        "type": "fill_blank",
        "answers": ["model adequacy assessment"],
        "other_options": ["factor extraction", "rotation validation", "communality estimation"]
    },
    {
        "q": "Rearrange the steps for performing cross-validated Discriminant Analysis:",
        "type": "rearrange",
        "words": ["Split data into folds", "Train on k-1 folds", "Test on held-out fold", "Average performance", "Select final model"]
    },
    {
        "q": "In PCA, the ______ represent the amount of variance explained by each principal component relative to total variance.",
        "type": "fill_blank",
        "answers": ["proportion of variance"],
        "other_options": ["eigenvalues", "loadings", "scores"]
    },
    {
        "q": "The F-measure can be used to evaluate clustering quality when true labels are available.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which technique is most appropriate for identifying the underlying constructs measured by a comprehensive personality assessment?",
        "type": "mcq",
        "o": [
            "Factor Analysis",
            "Principal Component Analysis",
            "Cluster Analysis",
            "Discriminant Analysis"
        ]
    },
    {
        "q": "The ______ method in hierarchical clustering is most likely to produce spherical, compact clusters.",
        "type": "fill_blank",
        "answers": ["Ward"],
        "other_options": ["single", "complete", "average"]
    },
    {
        "q": "In Discriminant Analysis, the ______ matrix shows how variables contribute to group separation.",
        "type": "fill_blank",
        "answers": ["structure"],
        "other_options": ["pattern", "loading", "canonical"]
    },
    {
        "q": "The ______ criterion in Factor Analysis uses a chi-square test to assess model fit.",
        "type": "fill_blank",
        "answers": ["likelihood ratio"],
        "other_options": ["Kaiser", "scree", "parallel"]
    },
    {
        "q": "Subspace clustering can identify clusters that exist in different subsets of variables.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which method helps address the problem of scale dependency in cluster analysis?",
        "type": "mcq",
        "o": [
            "Data standardization before clustering",
            "Using Manhattan distance instead of Euclidean",
            "Applying PCA as a preprocessing step",
            "Increasing the number of clusters"
        ]
    },
    {
        "q": "The ______ rotation method in Factor Analysis seeks to maximize the variance of squared loadings per factor.",
        "type": "fill_blank",
        "answers": ["varimax"],
        "other_options": ["quartimax", "oblimin", "promax"]
    },
    {
        "q": "In cluster analysis, the ______ distance is robust to outliers and works well with high-dimensional data.",
        "type": "fill_blank",
        "answers": ["Manhattan"],
        "other_options": ["Euclidean", "Mahalanobis", "Cosine"]
    },
    {
        "q": "The ______ index measures cluster validity by computing the average similarity within and between clusters.",
        "type": "fill_blank",
        "answers": ["silhouette"],
        "other_options": ["calinski", "davies", "dunn"]
    },
    {
        "q": "Regularized Discriminant Analysis can handle situations where the number of variables exceeds the sample size.",
        "type": "true_false",
        "correct": "True"
    }
]
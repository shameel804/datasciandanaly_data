{
    "id": "learn_unsupervised",
    "topicId": "unsupervised",
    "topicTitle": "Unsupervised Learning",
    "description": "Master clustering, dimensionality reduction, and pattern discovery techniques",
    "baseKP": 85,
    "slides": [
        {
            "id": "unsupervised_1",
            "type": "content",
            "title": "What is Unsupervised Learning",
            "content": "# Unsupervised Learning ğŸ”\n\nFinding patterns without labels.\n\n## Definition\n\nDiscover hidden structure in unlabeled data.\n\n## Main Types\n\n### Clustering\n- Group similar items\n- Customer segments\n\n### Dimensionality Reduction\n- Reduce features\n- Visualization\n\n### Association\n- Find co-occurrences\n- Market basket analysis\n\n## Use Cases\n\n- Customer segmentation\n- Anomaly detection\n- Feature extraction\n- Data exploration\n\n> ğŸ’¡ No right answers - you discover structure!"
        },
        {
            "id": "unsupervised_2",
            "type": "content",
            "title": "K-Means Clustering",
            "content": "# K-Means Clustering ğŸ¯\n\nPartition data into k clusters.\n\n## Algorithm\n\n1. Pick k initial centroids\n2. Assign points to nearest centroid\n3. Recalculate centroids\n4. Repeat until convergence\n\n## Choosing K\n\n- Elbow method\n- Silhouette score\n- Domain knowledge\n\n## Pros/Cons\n\n| Pros | Cons |\n|------|------|\n| Simple, fast | Must choose k |\n| Scales well | Spherical clusters only |\n| Easy to interpret | Sensitive to outliers |\n\n## Use Cases\n\n- Customer segments\n- Image compression\n- Document clustering\n\n> ğŸ¯ Most popular clustering algorithm!"
        },
        {
            "id": "unsupervised_quiz_1",
            "type": "quiz",
            "title": "Clustering Quiz",
            "content": "Test your knowledge!",
            "quizQuestion": "What is a major limitation of K-Means?",
            "quizOptions": [
                "Too slow",
                "Must specify k in advance",
                "Cannot handle numerical data",
                "Requires labels"
            ],
            "correctOptionIndex": 1
        },
        {
            "id": "unsupervised_3",
            "type": "content",
            "title": "Hierarchical Clustering",
            "content": "# Hierarchical Clustering ğŸŒ³\n\nBuild a tree of clusters.\n\n## Types\n\n### Agglomerative (Bottom-Up)\n- Start with individual points\n- Merge closest clusters\n\n### Divisive (Top-Down)\n- Start with all points\n- Split recursively\n\n## Dendrogram\n\nTree diagram showing merges.\n\n## Linkage Methods\n\n| Method | Definition |\n|--------|------------|\n| Single | Min distance |\n| Complete | Max distance |\n| Ward | Min variance increase |\n\n## When to Use\n\n- Explore hierarchical structure\n- Unknown number of clusters\n- Small to medium datasets\n\n> ğŸ’¡ Great for exploration!"
        },
        {
            "id": "unsupervised_4",
            "type": "content",
            "title": "DBSCAN",
            "content": "# DBSCAN ğŸ”¬\n\nDensity-based clustering.\n\n## Key Concepts\n\n- **Îµ (eps):** Neighborhood radius\n- **MinPts:** Minimum points in neighborhood\n\n## Point Types\n\n| Type | Definition |\n|------|------------|\n| Core | â‰¥ MinPts in radius |\n| Border | Near core point |\n| Noise | Neither |\n\n## Pros/Cons\n\n| Pros | Cons |\n|------|------|\n| No k needed | Sensitive to parameters |\n| Finds outliers | Varying density issues |\n| Any shape clusters | Can be slow |\n\n## Use Cases\n\n- Geographic clustering\n- Anomaly detection\n- Non-spherical clusters\n\n> ğŸ¯ Great when clusters have irregular shapes!"
        },
        {
            "id": "unsupervised_5",
            "type": "content",
            "title": "PCA",
            "content": "# Principal Component Analysis ğŸ“‰\n\nDimensionality reduction.\n\n## Goal\n\nReduce features while keeping variance.\n\n## How It Works\n\n1. Standardize data\n2. Find principal components\n3. Keep top k components\n4. Transform data\n\n## Use Cases\n\n- Visualization (2D/3D)\n- Speed up ML\n- Remove noise\n- Feature extraction\n\n## Keep How Many?\n\n- Explain 80-95% of variance\n- Use scree plot\n- Balance compression vs info loss\n\n> ğŸ’¡ PCA is often a first step in ML pipelines!"
        },
        {
            "id": "unsupervised_quiz_2",
            "type": "quiz",
            "title": "PCA Quiz",
            "content": "Test your knowledge!",
            "quizQuestion": "PCA aims to preserve as much _____ as possible.",
            "quizOptions": [
                "Labels",
                "Variance",
                "Missing values",
                "Clusters"
            ],
            "correctOptionIndex": 1
        },
        {
            "id": "unsupervised_6",
            "type": "content",
            "title": "Summary",
            "content": "# Congratulations! ğŸ‰\n\nYou've mastered Unsupervised Learning!\n\n## Methods Summary\n\n| Method | Use Case |\n|--------|----------|\n| K-Means | General clustering |\n| Hierarchical | Explore structure |\n| DBSCAN | Arbitrary shapes |\n| PCA | Dimension reduction |\n\n## Key Takeaways\n\n- âœ… No labels needed\n- âœ… Discover hidden patterns\n- âœ… Choose method based on data\n- âœ… Validate using domain knowledge\n\n## Remember\n\n> ğŸ¯ Unsupervised learning is about exploration!\n\nKeep discovering! ğŸ”"
        }
    ]
}
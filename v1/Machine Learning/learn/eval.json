{
    "id": "learn_model_eval",
    "topicId": "model_eval",
    "topicTitle": "Model Evaluation",
    "description": "Master evaluation metrics, cross-validation, and model selection techniques",
    "baseKP": 85,
    "slides": [
        {
            "id": "model_eval_1",
            "type": "content",
            "title": "Why Model Evaluation",
            "content": "# Model Evaluation ğŸ“Š\n\nMeasuring model performance.\n\n## Why Evaluate?\n\n- Compare models\n- Detect overfitting\n- Improve models\n- Select best model\n\n## Train vs Test\n\n| Set | Purpose |\n|-----|--------|\n| Training | Fit model |\n| Validation | Tune hyperparameters |\n| Test | Final evaluation |\n\n## Golden Rule\n\n> âš ï¸ Never evaluate on training data!\n\n## Typical Split\n\n- 70% Training\n- 15% Validation\n- 15% Test"
        },
        {
            "id": "model_eval_2",
            "type": "content",
            "title": "Classification Metrics",
            "content": "# Classification Metrics ğŸ¯\n\n## Confusion Matrix\n\n|  | Predicted + | Predicted - |\n|--|------------|------------|\n| Actual + | TP | FN |\n| Actual - | FP | TN |\n\n## Key Metrics\n\n**Accuracy** = (TP + TN) / Total\n\n**Precision** = TP / (TP + FP)\n\"Of predicted positive, how many correct?\"\n\n**Recall** = TP / (TP + FN)\n\"Of actual positive, how many found?\"\n\n**F1 Score** = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)\n\n## When to Use\n\n| Metric | Use When |\n|--------|----------|\n| Accuracy | Balanced classes |\n| Precision | False positives costly |\n| Recall | False negatives costly |\n| F1 | Balance both |"
        },
        {
            "id": "model_eval_quiz_1",
            "type": "quiz",
            "title": "Metrics Quiz",
            "content": "Test your knowledge!",
            "quizQuestion": "For cancer detection, which metric is MOST important?",
            "quizOptions": [
                "Accuracy",
                "Precision",
                "Recall (Sensitivity)",
                "Specificity"
            ],
            "correctOptionIndex": 2
        },
        {
            "id": "model_eval_3",
            "type": "content",
            "title": "Regression Metrics",
            "content": "# Regression Metrics ğŸ“ˆ\n\n## Common Metrics\n\n### MAE (Mean Absolute Error)\n**MAE = (1/n) Î£|y - Å·|**\n\n### MSE (Mean Squared Error)\n**MSE = (1/n) Î£(y - Å·)Â²**\n\n### RMSE\n**RMSE = âˆšMSE**\n\n### RÂ² (Coefficient of Determination)\n**RÂ² = 1 - (SSE/SST)**\nProportion of variance explained.\n\n## Comparison\n\n| Metric | Scale | Outliers |\n|--------|-------|----------|\n| MAE | Same as y | Robust |\n| RMSE | Same as y | Penalizes large |\n| RÂ² | 0-1 | N/A |\n\n> ğŸ¯ RÂ² tells how much variance is explained!"
        },
        {
            "id": "model_eval_4",
            "type": "content",
            "title": "Cross-Validation",
            "content": "# Cross-Validation ğŸ”„\n\nMore robust evaluation.\n\n## K-Fold CV\n\n1. Split data into k folds\n2. Train on k-1 folds\n3. Test on remaining fold\n4. Repeat k times\n5. Average results\n\n## Common Values\n\n- 5-fold (default)\n- 10-fold (more robust)\n- LOOCV (small datasets)\n\n## Benefits\n\n- Use all data for testing\n- Reduce variance in estimates\n- Better generalization measure\n\n## Stratified K-Fold\n\n- Maintains class proportions\n- Use for imbalanced data\n\n> ğŸ’¡ CV gives more reliable estimates!"
        },
        {
            "id": "model_eval_5",
            "type": "content",
            "title": "Overfitting and Underfitting",
            "content": "# Bias-Variance Tradeoff âš–ï¸\n\n## Underfitting (High Bias)\n\n- Model too simple\n- High train AND test error\n- Solution: More complex model\n\n## Overfitting (High Variance)\n\n- Model memorizes training data\n- Low train, high test error\n- Solution: Regularization, more data\n\n## Diagnosing\n\n| Train Error | Test Error | Diagnosis |\n|------------|------------|----------|\n| High | High | Underfit |\n| Low | High | Overfit |\n| Low | Low | Good fit |\n\n## Solutions\n\n- Regularization (L1, L2)\n- Early stopping\n- Dropout (neural nets)\n- More data\n- Simpler model\n\n> ğŸ¯ Balance complexity with generalization!"
        },
        {
            "id": "model_eval_quiz_2",
            "type": "quiz",
            "title": "Overfitting Quiz",
            "content": "Test your understanding!",
            "quizQuestion": "A model with 95% training accuracy and 60% test accuracy is likely:",
            "quizOptions": [
                "Underfitting",
                "Overfitting",
                "Well-balanced",
                "Perfect"
            ],
            "correctOptionIndex": 1
        },
        {
            "id": "model_eval_6",
            "type": "content",
            "title": "Summary",
            "content": "# Congratulations! ğŸ‰\n\nYou've mastered Model Evaluation!\n\n## Key Metrics\n\n### Classification\n- Accuracy, Precision, Recall, F1\n\n### Regression\n- MAE, MSE, RMSE, RÂ²\n\n## Key Takeaways\n\n- âœ… Never test on training data\n- âœ… Use cross-validation\n- âœ… Choose metrics based on problem\n- âœ… Balance bias and variance\n\n## Best Practices\n\n- [ ] Split data properly\n- [ ] Use stratified sampling\n- [ ] Check for overfitting\n- [ ] Report multiple metrics\n\n> ğŸš€ Good evaluation = trustworthy models!\n\nKeep evaluating! ğŸ“Š"
        }
    ]
}
{
    "id": "learn_supervise",
    "topicId": "supervise",
    "topicTitle": "Supervised Learning",
    "description": "Master supervised learning algorithms including linear models, trees, SVM, ensemble methods, and practical applications",
    "baseKP": 75,
    "slides": [
        {
            "id": "supervise_1",
            "type": "content",
            "title": "Welcome to Supervised Learning",
            "content": "# Supervised Learning ğŸ¯\n\nLearning from labeled examples!\n\n## What you'll learn:\n- **Linear Models** - Regression and classification\n- **Tree-Based** - Decision trees and random forests\n- **SVM** - Support vector machines\n- **Ensemble Methods** - Combining models\n\n> ğŸ’¡ **Key Insight:** Supervised learning is the workhorse of ML!\n\n## The Supervised Paradigm\n\n```\nTraining: X (features) + y (labels) â†’ Learn f\nPrediction: X_new â†’ f(X_new) â†’ y_pred\n```\n\n## Algorithm Categories\n\n| Category | Examples |\n|----------|----------|\n| Linear | Linear/Logistic Regression |\n| Tree-based | Decision Tree, Random Forest |\n| Instance-based | KNN |\n| Kernel | SVM |\n| Ensemble | Boosting, Bagging |"
        },
        {
            "id": "supervise_2",
            "type": "content",
            "title": "Linear Regression",
            "content": "# Linear Regression ğŸ“ˆ\n\nPredicting continuous values.\n\n## Concept\n\n```\ny = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Îµ\n```\n\nFind line that minimizes error!\n\n## Implementation\n\n```python\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\n# Coefficients\nprint(f\"Intercept: {model.intercept_}\")\nprint(f\"Coefficients: {model.coef_}\")\n```\n\n## Evaluation\n\n```python\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nmse = mean_squared_error(y_test, y_pred)\nrmse = mse ** 0.5\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"RMSE: {rmse:.2f}\")\nprint(f\"RÂ²: {r2:.4f}\")\n```\n\n## Regularization\n\n```python\nfrom sklearn.linear_model import Ridge, Lasso\n\nridge = Ridge(alpha=1.0)  # L2\nlasso = Lasso(alpha=1.0)  # L1 (feature selection)\n```"
        },
        {
            "id": "supervise_3",
            "type": "content",
            "title": "Logistic Regression",
            "content": "# Logistic Regression ğŸ”€\n\nBinary classification despite the name!\n\n## Concept\n\n```\nP(y=1) = 1 / (1 + e^(-z))\n\nwhere z = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ...\n```\n\n## Implementation\n\n```python\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\ny_prob = model.predict_proba(X_test)[:, 1]  # Probability\n```\n\n## Evaluation\n\n```python\nfrom sklearn.metrics import (accuracy_score, \n                              classification_report,\n                              confusion_matrix)\n\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.2%}\")\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n```\n\n## Multi-class\n\n```python\nmodel = LogisticRegression(multi_class='multinomial')\n# Handles 3+ classes automatically\n```"
        },
        {
            "id": "supervise_quiz_1",
            "type": "quiz",
            "title": "Linear Models Check",
            "content": "Test your understanding!",
            "quizQuestion": "Logistic Regression is used for:",
            "quizOptions": [
                "Predicting continuous values",
                "Classification tasks",
                "Clustering",
                "Dimensionality reduction"
            ],
            "correctOptionIndex": 1
        },
        {
            "id": "supervise_4",
            "type": "content",
            "title": "Decision Trees",
            "content": "# Decision Trees ğŸŒ³\n\nIf-then rules from data.\n\n## How It Works\n\n```\n           Is Age > 30?\n          /          \\\n        Yes           No\n       /               \\\n  Income > 50K?     Deny\n   /        \\\nApprove    Deny\n```\n\n## Implementation\n\n```python\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier(\n    max_depth=5,\n    min_samples_split=10,\n    min_samples_leaf=5\n)\nmodel.fit(X_train, y_train)\n```\n\n## Visualization\n\n```python\nfrom sklearn.tree import plot_tree\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(15, 10))\nplot_tree(model, feature_names=X.columns, filled=True)\nplt.show()\n```\n\n## Pros and Cons\n\n| Pros | Cons |\n|------|------|\n| âœ… Interpretable | âŒ Prone to overfit |\n| âœ… Handles non-linear | âŒ Instable |\n| âœ… No scaling needed | âŒ Greedy splits |"
        },
        {
            "id": "supervise_5",
            "type": "content",
            "title": "Random Forest",
            "content": "# Random Forest ğŸŒ²ğŸŒ²ğŸŒ²\n\nEnsemble of decision trees.\n\n## Concept\n\n```\nTrain many trees on random subsets\nCombine predictions (vote/average)\nâ†’ More robust than single tree\n```\n\n## Implementation\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(\n    n_estimators=100,      # Number of trees\n    max_depth=10,          # Tree depth\n    min_samples_split=5,   # Min samples to split\n    random_state=42\n)\nmodel.fit(X_train, y_train)\n```\n\n## Feature Importance\n\n```python\nimport pandas as pd\n\nimportances = pd.Series(\n    model.feature_importances_,\n    index=X.columns\n).sort_values(ascending=False)\n\nimportances.plot(kind='bar')\n```\n\n## Why Random Forest Works\n\n- Reduces variance (averaging)\n- Reduces overfitting\n- Handles noise well\n- Robust to outliers"
        },
        {
            "id": "supervise_6",
            "type": "content",
            "title": "Gradient Boosting",
            "content": "# Gradient Boosting ğŸš€\n\nSequentially improving weak learners.\n\n## Concept\n\n```\n1. Fit model to data\n2. Fit next model to residuals (errors)\n3. Combine predictions\n4. Repeat\n```\n\n## XGBoost\n\n```python\nimport xgboost as xgb\n\nmodel = xgb.XGBClassifier(\n    n_estimators=100,\n    max_depth=6,\n    learning_rate=0.1,\n    subsample=0.8\n)\nmodel.fit(X_train, y_train)\n```\n\n## LightGBM (Faster)\n\n```python\nimport lightgbm as lgb\n\nmodel = lgb.LGBMClassifier(\n    n_estimators=100,\n    num_leaves=31\n)\nmodel.fit(X_train, y_train)\n```\n\n## Comparison\n\n| Algorithm | Speed | Memory |\n|-----------|-------|--------|\n| XGBoost | Medium | Higher |\n| LightGBM | Fast | Lower |\n| CatBoost | Medium | Medium |\n\n> ğŸ’¡ XGBoost/LightGBM win most Kaggle competitions!"
        },
        {
            "id": "supervise_quiz_2",
            "type": "quiz",
            "title": "Ensemble Quiz",
            "content": "Test your ensemble knowledge!",
            "quizQuestion": "Random Forest reduces overfitting by:",
            "quizOptions": [
                "Using fewer features",
                "Training multiple trees and averaging",
                "Increasing tree depth",
                "Removing outliers"
            ],
            "correctOptionIndex": 1
        },
        {
            "id": "supervise_7",
            "type": "content",
            "title": "Support Vector Machines",
            "content": "# Support Vector Machines ğŸ“Š\n\nFinding optimal decision boundaries.\n\n## Concept\n\n```\nFind hyperplane that maximizes margin\nbetween classes\n\n  Class A        Margin        Class B\n    o  o    |    ====    |    x  x\n      o     |            |    x\n    o       |            |      x\n```\n\n## Implementation\n\n```python\nfrom sklearn.svm import SVC\n\nmodel = SVC(\n    kernel='rbf',   # 'linear', 'poly', 'rbf'\n    C=1.0,          # Regularization\n    gamma='scale'   # Kernel coefficient\n)\nmodel.fit(X_train, y_train)\n```\n\n## Kernels\n\n| Kernel | Use Case |\n|--------|----------|\n| linear | Linearly separable |\n| poly | Polynomial relationships |\n| rbf | Non-linear (default) |\n\n## Important Notes\n\n- âš¡ Requires feature scaling!\n- ğŸ“Š Works well with high dimensions\n- ğŸŒ Slow on large datasets"
        },
        {
            "id": "supervise_8",
            "type": "content",
            "title": "K-Nearest Neighbors",
            "content": "# K-Nearest Neighbors ğŸ‘¥\n\nClassify based on neighbors.\n\n## Concept\n\n```\nTo classify new point:\n1. Find k nearest neighbors\n2. Majority vote (classification)\n   or average (regression)\n\n   o o     â†’ If k=3: 2 circles, 1 cross\n     ? x   â†’ Classify as circle\n   o\n```\n\n## Implementation\n\n```python\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(\n    n_neighbors=5,\n    weights='uniform',  # or 'distance'\n    metric='euclidean'\n)\nmodel.fit(X_train, y_train)\n```\n\n## Choosing K\n\n```python\nfrom sklearn.model_selection import cross_val_score\n\nfor k in range(1, 21):\n    model = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(model, X, y, cv=5)\n    print(f\"k={k}: {scores.mean():.4f}\")\n```\n\n## Notes\n\n- âš¡ Must scale features!\n- ğŸŒ Slow prediction on large data\n- ğŸ’¡ No training phase"
        },
        {
            "id": "supervise_quiz_3",
            "type": "quiz",
            "title": "Final Quiz",
            "content": "Test your overall understanding!",
            "quizQuestion": "Which algorithm is most commonly used in Kaggle winning solutions?",
            "quizOptions": [
                "Logistic Regression",
                "Decision Trees",
                "XGBoost/LightGBM",
                "KNN"
            ],
            "correctOptionIndex": 2
        },
        {
            "id": "supervise_9",
            "type": "content",
            "title": "Summary",
            "content": "# Congratulations! ğŸ‰\n\nYou've mastered Supervised Learning!\n\n## Algorithm Guide\n\n### Start Simple\n- Linear/Logistic Regression\n- Interpretable baseline\n\n### For Accuracy\n- Random Forest\n- XGBoost/LightGBM\n\n### For Interpretability\n- Decision Trees\n- Logistic Regression\n\n### For High Dimensions\n- SVM\n\n## Key Takeaways\n\n| Algorithm | Scale? | Interpretable? |\n|-----------|--------|----------------|\n| Linear/Logistic | No | âœ… |\n| Decision Tree | No | âœ… |\n| Random Forest | No | âš ï¸ |\n| XGBoost | No | âš ï¸ |\n| SVM | âœ… | âŒ |\n| KNN | âœ… | âœ… |\n\n## Remember\n\n> ğŸ¯ \"Start simple, add complexity only if needed.\"\n\n## Next Steps\n- ğŸ” Learn **Unsupervised Learning**\n- ğŸ“Š Master **Model Evaluation**\n\nKeep learning! ğŸ¯"
        }
    ]
}
{
    "id": "learn_dl",
    "topicId": "dl",
    "topicTitle": "Deep Learning Introduction",
    "description": "Master deep learning fundamentals including neural networks, backpropagation, and common architectures",
    "baseKP": 75,
    "slides": [
        {
            "id": "dl_1",
            "type": "content",
            "title": "Welcome to Deep Learning",
            "content": "# Deep Learning ğŸ§ \n\nNeural networks that learn complex patterns!\n\n## What you'll learn:\n- **Neural Networks** - Layers of neurons\n- **Backpropagation** - How networks learn\n- **Activation Functions** - Non-linear transforms\n- **Common Architectures** - CNN, RNN basics\n\n> ğŸ’¡ **Key Insight:** Deep learning enables machines to learn from raw data!\n\n## Deep Learning vs ML\n\n| Machine Learning | Deep Learning |\n|-----------------|---------------|\n| Feature engineering | Automatic features |\n| Works on smaller data | Needs big data |\n| Faster training | GPU required |\n| More interpretable | Black box |\n\n## Applications\n- ğŸ–¼ï¸ Image recognition\n- ğŸ—£ï¸ Speech recognition\n- ğŸ“ Text generation\n- ğŸ® Game playing"
        },
        {
            "id": "dl_2",
            "type": "content",
            "title": "Neural Network Basics",
            "content": "# Neural Network Basics ğŸ”—\n\nBuilding blocks of deep learning.\n\n## Structure\n\n```\nInput Layer â†’ Hidden Layers â†’ Output Layer\n\n     [xâ‚]â”€â”€â”     â”Œâ”€â”€[hâ‚]â”€â”€â”     â”Œâ”€â”€[y]\n     [xâ‚‚]â”€â”€â”¼â”€â”€â†’â”€â”€â”¼â”€â”€[hâ‚‚]â”€â”€â”¼â”€â”€â†’â”€â”€â”¤\n     [xâ‚ƒ]â”€â”€â”˜     â””â”€â”€[hâ‚ƒ]â”€â”€â”˜     â””â”€â”€[y]\n```\n\n## Single Neuron\n\n```\nInputs: xâ‚, xâ‚‚, xâ‚ƒ\nWeights: wâ‚, wâ‚‚, wâ‚ƒ\nBias: b\n\nz = wâ‚xâ‚ + wâ‚‚xâ‚‚ + wâ‚ƒxâ‚ƒ + b\noutput = activation(z)\n```\n\n## Matrix Form\n\n```python\nimport numpy as np\n\n# Forward pass\nz = np.dot(W, x) + b\na = activation(z)\n```\n\n## Key Terms\n\n| Term | Definition |\n|------|------------|\n| Weight | Connection strength |\n| Bias | Offset term |\n| Activation | Non-linear function |\n| Layer | Group of neurons |"
        },
        {
            "id": "dl_3",
            "type": "content",
            "title": "Activation Functions",
            "content": "# Activation Functions âš¡\n\nAdding non-linearity.\n\n## ReLU (Most Common)\n\n```python\ndef relu(x):\n    return max(0, x)\n\n# f(x) = 0 if x < 0\n#      = x if x >= 0\n```\n\n## Sigmoid\n\n```python\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# Output: (0, 1) - good for probabilities\n```\n\n## Tanh\n\n```python\ndef tanh(x):\n    return np.tanh(x)\n\n# Output: (-1, 1)\n```\n\n## Softmax (Output Layer)\n\n```python\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x))\n    return exp_x / exp_x.sum()\n\n# Multi-class probabilities that sum to 1\n```\n\n## When to Use\n\n| Function | Use Case |\n|----------|----------|\n| ReLU | Hidden layers (default) |\n| Sigmoid | Binary output |\n| Softmax | Multi-class output |\n| Tanh | RNN hidden states |"
        },
        {
            "id": "dl_quiz_1",
            "type": "quiz",
            "title": "Activation Check",
            "content": "Test your understanding!",
            "quizQuestion": "Which activation function is most commonly used for hidden layers?",
            "quizOptions": [
                "Sigmoid",
                "ReLU",
                "Softmax",
                "Linear"
            ],
            "correctOptionIndex": 1
        },
        {
            "id": "dl_4",
            "type": "content",
            "title": "Backpropagation",
            "content": "# Backpropagation ğŸ”„\n\nHow neural networks learn.\n\n## The Learning Process\n\n```\n1. Forward Pass: Compute predictions\n2. Compute Loss: How wrong are we?\n3. Backward Pass: Compute gradients\n4. Update Weights: Gradient descent\n5. Repeat\n```\n\n## Loss Functions\n\n```python\n# Binary Cross-Entropy\nloss = -mean(y*log(pred) + (1-y)*log(1-pred))\n\n# Mean Squared Error\nloss = mean((y - pred)Â²)\n\n# Categorical Cross-Entropy\nloss = -sum(y * log(pred))\n```\n\n## Gradient Descent\n\n```python\n# Update rule\nweight = weight - learning_rate * gradient\n\n# w_new = w_old - lr * âˆ‚Loss/âˆ‚w\n```\n\n## Chain Rule\n\n```\nâˆ‚Loss/âˆ‚wâ‚ = âˆ‚Loss/âˆ‚output Ã— âˆ‚output/âˆ‚z Ã— âˆ‚z/âˆ‚wâ‚\n\nGradients flow backwards through layers!\n```"
        },
        {
            "id": "dl_5",
            "type": "content",
            "title": "Building with Keras",
            "content": "# Building with Keras ğŸ”§\n\nHigh-level deep learning API.\n\n## Simple Neural Network\n\n```python\nimport tensorflow as tf\nfrom tensorflow import keras\n\nmodel = keras.Sequential([\n    keras.layers.Dense(128, activation='relu', input_shape=(10,)),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()\n```\n\n## Training\n\n```python\nhistory = model.fit(\n    X_train, y_train,\n    epochs=50,\n    batch_size=32,\n    validation_split=0.2\n)\n```\n\n## Evaluation\n\n```python\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {accuracy:.2%}\")\n\n# Predictions\ny_pred = model.predict(X_test)\n```"
        },
        {
            "id": "dl_6",
            "type": "content",
            "title": "Training Techniques",
            "content": "# Training Techniques ğŸ›ï¸\n\nImproving neural network training.\n\n## Optimizers\n\n```python\n# SGD - Stochastic Gradient Descent\nkeras.optimizers.SGD(learning_rate=0.01)\n\n# Adam - Most popular\nkeras.optimizers.Adam(learning_rate=0.001)\n\n# RMSprop - Good for RNNs\nkeras.optimizers.RMSprop(learning_rate=0.001)\n```\n\n## Regularization\n\n```python\n# Dropout - Randomly disable neurons\nkeras.layers.Dropout(0.3)\n\n# L2 Regularization\nkeras.layers.Dense(64, kernel_regularizer='l2')\n```\n\n## Batch Normalization\n\n```python\nmodel = keras.Sequential([\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n```\n\n## Callbacks\n\n```python\ncallbacks = [\n    keras.callbacks.EarlyStopping(patience=5),\n    keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True)\n]\n\nmodel.fit(..., callbacks=callbacks)\n```"
        },
        {
            "id": "dl_quiz_2",
            "type": "quiz",
            "title": "Training Quiz",
            "content": "Test your training knowledge!",
            "quizQuestion": "What does Dropout do?",
            "quizOptions": [
                "Speeds up training",
                "Randomly disables neurons to prevent overfitting",
                "Reduces the number of layers",
                "Increases learning rate"
            ],
            "correctOptionIndex": 1
        },
        {
            "id": "dl_7",
            "type": "content",
            "title": "CNN and RNN Overview",
            "content": "# CNN and RNN Overview ğŸ“Š\n\nSpecialized architectures.\n\n## CNN (Convolutional Neural Networks)\n\nFor spatial data (images).\n\n```python\nmodel = keras.Sequential([\n    keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n    keras.layers.MaxPooling2D((2,2)),\n    keras.layers.Conv2D(64, (3,3), activation='relu'),\n    keras.layers.Flatten(),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dense(10, activation='softmax')\n])\n```\n\n## RNN (Recurrent Neural Networks)\n\nFor sequential data (text, time series).\n\n```python\nmodel = keras.Sequential([\n    keras.layers.Embedding(vocab_size, 64),\n    keras.layers.LSTM(128, return_sequences=True),\n    keras.layers.LSTM(64),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n```\n\n## Use Cases\n\n| Architecture | Applications |\n|--------------|---------------|\n| CNN | Images, vision, video |\n| RNN/LSTM | Text, speech, time series |\n| Transformer | NLP, everything |"
        },
        {
            "id": "dl_quiz_3",
            "type": "quiz",
            "title": "Final Quiz",
            "content": "Test your overall understanding!",
            "quizQuestion": "Which architecture is best for image classification?",
            "quizOptions": [
                "RNN",
                "CNN",
                "Dense only",
                "Autoencoder"
            ],
            "correctOptionIndex": 1
        },
        {
            "id": "dl_8",
            "type": "content",
            "title": "Summary",
            "content": "# Congratulations! ğŸ‰\n\nYou've mastered Deep Learning Introduction!\n\n## Key Takeaways\n\n### Neural Networks\n- âœ… Layers of connected neurons\n- âœ… Weights, biases, activations\n\n### Backpropagation\n- âœ… Forward â†’ Loss â†’ Backward â†’ Update\n- âœ… Gradient descent optimization\n\n### Activation Functions\n- âœ… ReLU for hidden layers\n- âœ… Sigmoid/Softmax for output\n\n### Training\n- âœ… Adam optimizer\n- âœ… Dropout for regularization\n- âœ… Early stopping\n\n## Keras Template\n\n```python\nmodel = keras.Sequential([...])\nmodel.compile(optimizer='adam', loss='...', metrics=[...])\nmodel.fit(X_train, y_train, epochs=50)\nmodel.evaluate(X_test, y_test)\n```\n\n## Remember\n\n> ğŸ§  \"Deep learning is about learning representations.\"\n\n## Next Steps\n- ğŸ“Š Master **Model Evaluation**\n- ğŸ—£ï¸ Explore **NLP**\n\nKeep learning! ğŸ§ "
        }
    ]
}
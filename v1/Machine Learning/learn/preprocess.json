{
    "id": "learn_preprocess",
    "topicId": "preprocess",
    "topicTitle": "Data Preprocessing",
    "description": "Master data cleaning, feature engineering, and data transformation techniques",
    "baseKP": 80,
    "slides": [
        {
            "id": "preprocess_1",
            "type": "content",
            "title": "Why Preprocessing Matters",
            "content": "# Data Preprocessing ğŸ§¹\n\nGarbage in, garbage out!\n\n## Why Preprocess?\n\n- ML models are sensitive to data quality\n- Real data is messy\n- Features need proper format\n\n## Key Steps\n\n1. Handle missing values\n2. Encode categories\n3. Scale/normalize\n4. Feature engineering\n5. Handle outliers\n\n> ğŸ’¡ 80% of ML time is preprocessing!"
        },
        {
            "id": "preprocess_2",
            "type": "content",
            "title": "Missing Values",
            "content": "# Handling Missing Values ğŸ”\n\n## Detection\n```python\nimport numpy as np\ndata = [1, np.nan, 3, np.nan, 5]\nmissing = np.isnan(data).sum()\n```\n\n## Strategies\n\n| Strategy | Use When |\n|----------|----------|\n| Delete rows | Few missing |\n| Mean/median | Numerical |\n| Mode | Categorical |\n| Prediction | Complex |\n\n## Best Practices\n\n- Analyze missing patterns\n- Consider why data is missing\n- Don't delete too much\n\n> ğŸ¯ Missing data needs thoughtful handling!"
        },
        {
            "id": "preprocess_quiz_1",
            "type": "quiz",
            "title": "Missing Data Quiz",
            "content": "Test your knowledge!",
            "quizQuestion": "For categorical missing values, which imputation is typically used?",
            "quizOptions": [
                "Mean",
                "Median",
                "Mode",
                "Zero"
            ],
            "correctOptionIndex": 2
        },
        {
            "id": "preprocess_3",
            "type": "content",
            "title": "Encoding Categories",
            "content": "# Encoding Categorical Variables ğŸ·ï¸\n\n## Label Encoding\nAssign numbers to categories.\n\n```python\ncolors = ['red', 'blue', 'green']\n# red=0, blue=1, green=2\n```\n\n## One-Hot Encoding\nCreate binary columns.\n\n```python\n# red â†’ [1, 0, 0]\n# blue â†’ [0, 1, 0]\n# green â†’ [0, 0, 1]\n```\n\n## When to Use\n\n| Method | Use For |\n|--------|--------|\n| Label | Ordinal (order matters) |\n| One-Hot | Nominal (no order) |\n\n> ğŸ’¡ Wrong encoding can mislead models!"
        },
        {
            "id": "preprocess_4",
            "type": "content",
            "title": "Feature Scaling",
            "content": "# Feature Scaling ğŸ“\n\n## Standardization (Z-score)\n\n**X' = (X - Î¼) / Ïƒ**\n- Mean = 0, Std = 1\n- Good for most algorithms\n\n## Min-Max Normalization\n\n**X' = (X - min) / (max - min)**\n- Range [0, 1]\n- Sensitive to outliers\n\n## When to Scale\n\n| Algorithm | Need Scaling? |\n|-----------|---------------|\n| Linear/Logistic | Yes |\n| SVM, KNN | Yes |\n| Tree-based | No |\n| Neural Networks | Yes |\n\n```python\nimport numpy as np\ndata = [10, 20, 30, 40, 50]\nstandardized = (data - np.mean(data)) / np.std(data)\n```"
        },
        {
            "id": "preprocess_5",
            "type": "content",
            "title": "Feature Engineering",
            "content": "# Feature Engineering ğŸ”§\n\nCreating informative features.\n\n## Techniques\n\n### Combine Features\n- Ratios: price_per_sqft = price / sqft\n- Differences: age = current_year - birth\n\n### Extract Features\n- Date: year, month, day, weekday\n- Text: word count, sentiment\n\n### Binning\n- Age â†’ Age groups\n- Continuous â†’ Categorical\n\n### Polynomial Features\n- x, xÂ², xÂ³\n- Feature interactions\n\n## Impact\n\n| Good Features | Poor Features |\n|--------------|---------------|\n| High signal | Mostly noise |\n| Domain relevant | Arbitrary |\n| Interpretable | Black box |\n\n> ğŸ¯ Feature engineering often beats algorithm choice!"
        },
        {
            "id": "preprocess_quiz_2",
            "type": "quiz",
            "title": "Feature Quiz",
            "content": "Test your knowledge!",
            "quizQuestion": "Which algorithms do NOT require feature scaling?",
            "quizOptions": [
                "Linear Regression",
                "SVM",
                "Tree-based (Random Forest)",
                "Neural Networks"
            ],
            "correctOptionIndex": 2
        },
        {
            "id": "preprocess_6",
            "type": "content",
            "title": "Summary",
            "content": "# Congratulations! ğŸ‰\n\nYou've mastered Data Preprocessing!\n\n## Key Takeaways\n\n- âœ… Handle missing values thoughtfully\n- âœ… Encode categories properly\n- âœ… Scale features for distance-based models\n- âœ… Engineer meaningful features\n\n## Checklist\n\n- [ ] Check for missing values\n- [ ] Handle outliers\n- [ ] Encode categories\n- [ ] Scale if needed\n- [ ] Create new features\n\n> ğŸš€ Good preprocessing = better models!\n\nKeep cleaning! ğŸ§¹"
        }
    ]
}
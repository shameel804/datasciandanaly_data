{
    "id": "learn_preprocess",
    "topicId": "preprocess",
    "topicTitle": "Data Preprocessing",
    "description": "Master data preprocessing techniques including scaling, encoding, feature selection, and handling imbalanced data",
    "baseKP": 75,
    "slides": [
        {
            "id": "preprocess_1",
            "type": "content",
            "title": "Welcome to Data Preprocessing",
            "content": "# Data Preprocessing ğŸ”§\n\nPreparing data for machine learning!\n\n## What you'll learn:\n- **Feature Scaling** - Normalize data ranges\n- **Encoding** - Convert categories to numbers\n- **Feature Selection** - Choose important features\n- **Imbalanced Data** - Handle class imbalance\n\n> ğŸ’¡ **Key Insight:** Good preprocessing can be more impactful than model choice!\n\n## Why Preprocessing?\n\n| Raw Data | Preprocessed Data |\n|----------|-------------------|\n| Different scales | Uniform scales |\n| Categories as text | Numeric encoding |\n| Missing values | Handled properly |\n| Irrelevant features | Key features only |\n\n## Impact on Models\n\n- KNN, SVM: Very sensitive to scale\n- Trees: Less sensitive\n- Neural Networks: Need proper scaling"
        },
        {
            "id": "preprocess_2",
            "type": "content",
            "title": "Feature Scaling",
            "content": "# Feature Scaling ğŸ“\n\nNormalizing data ranges.\n\n## StandardScaler (Z-score)\n\n```python\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Result: mean=0, std=1\n```\n\n## MinMaxScaler\n\n```python\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X_train)\n\n# Result: range [0, 1]\n```\n\n## RobustScaler (Outlier-resistant)\n\n```python\nfrom sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()\nX_scaled = scaler.fit_transform(X_train)\n\n# Uses median and IQR (less affected by outliers)\n```\n\n## When to Use\n\n| Scaler | Best For |\n|--------|----------|\n| Standard | Normally distributed |\n| MinMax | Bounded range needed |\n| Robust | Data with outliers |"
        },
        {
            "id": "preprocess_3",
            "type": "content",
            "title": "Categorical Encoding",
            "content": "# Categorical Encoding ğŸ”¤\n\nConverting categories to numbers.\n\n## Label Encoding (Ordinal)\n\n```python\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndf['encoded'] = le.fit_transform(df['category'])\n\n# Low=0, Medium=1, High=2\n```\n\n## One-Hot Encoding (Nominal)\n\n```python\nfrom sklearn.preprocessing import OneHotEncoder\nimport pandas as pd\n\n# Pandas method\ndf_encoded = pd.get_dummies(df, columns=['color'])\n\n# Sklearn method\nencoder = OneHotEncoder(sparse=False)\nencoded = encoder.fit_transform(df[['color']])\n```\n\n## Result Example\n\n```\nOriginal:     One-Hot Encoded:\ncolor         red  blue  green\nred    â†’      1    0     0\nblue   â†’      0    1     0\ngreen  â†’      0    0     1\n```\n\n## When to Use\n\n| Encoding | Use When |\n|----------|----------|\n| Label | Ordinal data (low/med/high) |\n| One-Hot | Nominal data (no order) |\n| Target | Binary classification target |"
        },
        {
            "id": "preprocess_quiz_1",
            "type": "quiz",
            "title": "Encoding Check",
            "content": "Test your understanding!",
            "quizQuestion": "For a 'color' feature with values red/blue/green (no natural order), which encoding is best?",
            "quizOptions": [
                "Label Encoding",
                "One-Hot Encoding",
                "StandardScaler",
                "No encoding needed"
            ],
            "correctOptionIndex": 1
        },
        {
            "id": "preprocess_4",
            "type": "content",
            "title": "Feature Selection",
            "content": "# Feature Selection ğŸ¯\n\nChoosing important features.\n\n## Why Select Features?\n\n- Reduce overfitting\n- Improve speed\n- Better interpretability\n- Remove noise\n\n## Correlation Analysis\n\n```python\nimport seaborn as sns\n\n# Correlation with target\ncorrelations = df.corr()['target'].sort_values()\n\n# Heatmap\nsns.heatmap(df.corr(), annot=True)\n```\n\n## SelectKBest\n\n```python\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\nselector = SelectKBest(f_classif, k=10)\nX_selected = selector.fit_transform(X, y)\n\n# Get selected feature names\nselected_features = X.columns[selector.get_support()]\n```\n\n## Feature Importance (from models)\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier()\nmodel.fit(X, y)\n\n# Get importance\nimportances = pd.Series(\n    model.feature_importances_,\n    index=X.columns\n).sort_values(ascending=False)\n```"
        },
        {
            "id": "preprocess_5",
            "type": "content",
            "title": "Handling Imbalanced Data",
            "content": "# Imbalanced Data âš–ï¸\n\nWhen classes aren't equal.\n\n## Problem\n\n```\nClass 0: 9,500 samples (95%)\nClass 1:   500 samples (5%)\n\nâ†’ Model predicts all Class 0 = 95% accuracy!\n   But useless for detecting Class 1.\n```\n\n## Oversampling (SMOTE)\n\n```python\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X, y)\n```\n\n## Undersampling\n\n```python\nfrom imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X, y)\n```\n\n## Class Weights\n\n```python\n# Built into many models\nmodel = RandomForestClassifier(class_weight='balanced')\nmodel = LogisticRegression(class_weight='balanced')\n```\n\n## Strategy Comparison\n\n| Method | Pros | Cons |\n|--------|------|------|\n| SMOTE | Creates synthetic | Can add noise |\n| Undersample | Fast | Loses data |\n| Weights | Simple | May not be enough |"
        },
        {
            "id": "preprocess_6",
            "type": "content",
            "title": "Pipelines",
            "content": "# Sklearn Pipelines ğŸ”—\n\nChaining preprocessing steps.\n\n## Why Pipelines?\n\n- Prevent data leakage\n- Reproducible workflow\n- Clean code\n- Easy deployment\n\n## Basic Pipeline\n\n```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\n\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', RandomForestClassifier())\n])\n\npipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_test)\n```\n\n## ColumnTransformer\n\n```python\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\npreprocessor = ColumnTransformer([\n    ('num', StandardScaler(), numeric_cols),\n    ('cat', OneHotEncoder(), categorical_cols)\n])\n\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('classifier', RandomForestClassifier())\n])\n```"
        },
        {
            "id": "preprocess_quiz_2",
            "type": "quiz",
            "title": "Pipeline Quiz",
            "content": "Test your preprocessing knowledge!",
            "quizQuestion": "Why should you ONLY fit scalers on training data?",
            "quizOptions": [
                "It's faster that way",
                "To prevent data leakage from test set",
                "Test data doesn't need scaling",
                "It doesn't matter"
            ],
            "correctOptionIndex": 1
        },
        {
            "id": "preprocess_7",
            "type": "content",
            "title": "Dimensionality Reduction",
            "content": "# Dimensionality Reduction ğŸ“‰\n\nReducing features while preserving information.\n\n## PCA (Principal Component Analysis)\n\n```python\nfrom sklearn.decomposition import PCA\n\n# Keep 95% of variance\npca = PCA(n_components=0.95)\nX_reduced = pca.fit_transform(X)\n\nprint(f\"Reduced from {X.shape[1]} to {X_reduced.shape[1]} features\")\n```\n\n## Explained Variance\n\n```python\nimport matplotlib.pyplot as plt\n\npca = PCA()\npca.fit(X)\n\nplt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n         np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\n```\n\n## When to Use\n\n| Scenario | Use |\n|----------|-----|\n| Many correlated features | PCA |\n| Visualization | t-SNE, UMAP |\n| Sparse data | Truncated SVD |\n\n## Benefits\n\n- âœ… Reduces noise\n- âœ… Speeds up training\n- âœ… Avoids curse of dimensionality\n- âŒ Loses interpretability"
        },
        {
            "id": "preprocess_8",
            "type": "content",
            "title": "Complete Preprocessing Example",
            "content": "# Complete Example ğŸ”§\n\nEnd-to-end preprocessing.\n\n```python\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Define column types\nnumeric_features = ['age', 'salary', 'experience']\ncategorical_features = ['department', 'location']\n\n# Numeric pipeline\nnumeric_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\n# Categorical pipeline\ncategorical_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine\npreprocessor = ColumnTransformer([\n    ('num', numeric_transformer, numeric_features),\n    ('cat', categorical_transformer, categorical_features)\n])\n\n# Full pipeline\nmodel = Pipeline([\n    ('preprocessor', preprocessor),\n    ('classifier', RandomForestClassifier())\n])\n\nmodel.fit(X_train, y_train)\n```"
        },
        {
            "id": "preprocess_quiz_3",
            "type": "quiz",
            "title": "Final Quiz",
            "content": "Test your overall understanding!",
            "quizQuestion": "SMOTE is used to handle:",
            "quizOptions": [
                "Missing values",
                "Categorical encoding",
                "Imbalanced classes",
                "Feature scaling"
            ],
            "correctOptionIndex": 2
        },
        {
            "id": "preprocess_9",
            "type": "content",
            "title": "Summary",
            "content": "# Congratulations! ğŸ‰\n\nYou've mastered Data Preprocessing!\n\n## Key Takeaways\n\n### Feature Scaling\n- âœ… StandardScaler: mean=0, std=1\n- âœ… MinMaxScaler: range [0,1]\n- âœ… RobustScaler: for outliers\n\n### Encoding\n- âœ… LabelEncoder: ordinal\n- âœ… OneHotEncoder: nominal\n\n### Feature Selection\n- âœ… Correlation analysis\n- âœ… SelectKBest\n- âœ… Feature importance\n\n### Imbalanced Data\n- âœ… SMOTE oversampling\n- âœ… Undersampling\n- âœ… Class weights\n\n### Pipelines\n- âœ… Chain transformations\n- âœ… Prevent data leakage\n\n## Golden Rule\n\n> ğŸ”§ \"Fit on train, transform on both!\"\n\n## Remember\n\n```python\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n```\n\n## Next Steps\n- ğŸ¯ Learn **Supervised Learning**\n- ğŸ” Explore **Model Evaluation**\n\nKeep preprocessing! ğŸ”§"
        }
    ]
}
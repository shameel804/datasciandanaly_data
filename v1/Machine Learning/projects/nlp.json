[
    {
        "title": "Text Preprocessing üìù",
        "ques": "What are **text preprocessing steps** for NLP?",
        "answer": {
            "type": "text",
            "content": "### Text Preprocessing:\n\n| Step | Purpose |\n|------|--------|\n| **Lowercasing** | Standardize case |\n| **Tokenization** | Split into words |\n| **Stop word removal** | Remove common words |\n| **Stemming/Lemmatization** | Reduce to root form |\n| **Punctuation removal** | Clean text |"
        },
        "explanation": "**Preprocessing** prepares text for ML algorithms."
    },
    {
        "title": "Text Vectorization üî¢",
        "ques": "What is **TF-IDF**?",
        "answer": {
            "type": "text",
            "content": "### TF-IDF:\n\n**Term Frequency - Inverse Document Frequency**\n\n### Formula:\n```\nTF-IDF = TF √ó IDF\nTF = word count in doc / total words\nIDF = log(total docs / docs with word)\n```\n\n### Purpose:\nWeight words by importance, not just frequency."
        },
        "explanation": "**TF-IDF** identifies important words in documents."
    },
    {
        "title": "Word Embeddings üî§",
        "ques": "What are **word embeddings**?",
        "answer": {
            "type": "text",
            "content": "### Word Embeddings:\n\n**Definition:** Dense vector representations of words.\n\n### Popular Methods:\n| Method | Characteristic |\n|--------|---------------|\n| **Word2Vec** | Context-based |\n| **GloVe** | Co-occurrence based |\n| **FastText** | Subword aware |\n\n### Property:\nSimilar words have similar vectors."
        },
        "explanation": "**Embeddings** capture semantic meaning of words."
    },
    {
        "title": "NLP Models ü§ñ",
        "ques": "What is **BERT**?",
        "answer": {
            "type": "text",
            "content": "### BERT:\n\n**Bidirectional Encoder Representations from Transformers**\n\n### Key Features:\n| Feature | Benefit |\n|---------|--------|\n| Bidirectional | Understands full context |\n| Pre-trained | Transfer learning |\n| Transformer-based | Captures relationships |\n\n### Applications:\nQ&A, sentiment, classification"
        },
        "explanation": "**BERT** revolutionized NLP with contextual understanding."
    },
    {
        "title": "NLP Applications üí¨",
        "ques": "Name **three NLP applications**.",
        "answer": {
            "type": "text",
            "content": "### NLP Applications:\n\n| Application | Task |\n|-------------|------|\n| **Sentiment Analysis** | Positive/negative classification |\n| **Named Entity Recognition** | Identify people, places, orgs |\n| **Machine Translation** | Convert between languages |\n\n### More:\n- Chatbots\n- Text summarization\n- Question answering"
        },
        "explanation": "**NLP** enables computers to understand human language."
    },
    {
        "title": "Text Classification üè∑Ô∏è",
        "ques": "How is **text classification** performed?",
        "answer": {
            "type": "text",
            "content": "### Text Classification Pipeline:\n\n1. **Preprocess** text\n2. **Vectorize** (TF-IDF, embeddings)\n3. **Train classifier** (Naive Bayes, SVM, Neural)\n4. **Evaluate** with accuracy, F1\n\n### Use Cases:\n- Spam detection\n- Topic categorization\n- Intent recognition"
        },
        "explanation": "**Text classification** assigns categories to documents."
    }
]
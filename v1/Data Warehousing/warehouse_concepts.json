[
    {
        "q": "What does OLTP stand for?",
        "type": "mcq",
        "o": [
            "Online Transaction Processing",
            "Online Transfer Protocol",
            "Offline Transaction Processing",
            "Online Table Processing"
        ]
    },
    {
        "q": "What does OLAP stand for?",
        "type": "mcq",
        "o": [
            "Online Analytical Processing",
            "Online Application Processing",
            "Offline Analytical Processing",
            "Online Archive Processing"
        ]
    },
    {
        "q": "OLTP systems are optimized for _____ operations.",
        "type": "fill_blank",
        "answers": [
            "transactional"
        ],
        "other_options": [
            "analytical",
            "reporting",
            "archival"
        ]
    },
    {
        "q": "A data warehouse is primarily used for operational day-to-day transactions.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which system is designed for complex queries and reporting?",
        "type": "mcq",
        "o": [
            "OLAP",
            "OLTP",
            "CRUD",
            "REST"
        ]
    },
    {
        "q": "What is a Data Mart?",
        "type": "mcq",
        "o": [
            "A subset of a data warehouse focused on a specific business area",
            "A complete enterprise data warehouse",
            "A temporary storage for raw data",
            "A real-time processing system"
        ]
    },
    {
        "q": "Match the system type with its primary purpose:",
        "type": "match",
        "left": [
            "OLTP",
            "OLAP",
            "Data Mart",
            "Data Lake"
        ],
        "right": [
            "Daily transactions",
            "Business analysis",
            "Department-specific data",
            "Raw data storage"
        ]
    },
    {
        "q": "ETL stands for Extract, Transform, and _____.",
        "type": "fill_blank",
        "answers": [
            "Load"
        ],
        "other_options": [
            "Link",
            "List",
            "Log"
        ]
    },
    {
        "q": "Which step in ETL involves cleaning and formatting data?",
        "type": "mcq",
        "o": [
            "Transform",
            "Extract",
            "Load",
            "Validate"
        ]
    },
    {
        "q": "OLTP systems typically handle many short online transactions.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the ETL process steps in correct order:",
        "type": "rearrange",
        "words": [
            "Extract",
            "Transform",
            "Load"
        ]
    },
    {
        "q": "Which architecture pattern uses a central repository for enterprise data?",
        "type": "mcq",
        "o": [
            "Enterprise Data Warehouse",
            "Data Mart",
            "Operational Data Store",
            "Staging Area"
        ]
    },
    {
        "q": "A data warehouse stores _____ data for analysis.",
        "type": "fill_blank",
        "answers": [
            "historical"
        ],
        "other_options": [
            "temporary",
            "encrypted",
            "compressed"
        ]
    },
    {
        "q": "Which is a characteristic of OLAP systems?",
        "type": "mcq",
        "o": [
            "Complex queries on large datasets",
            "High volume of simple transactions",
            "Real-time updates",
            "Minimal data storage"
        ]
    },
    {
        "q": "Data marts are always independent and do not connect to a data warehouse.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What is the primary benefit of cloud-based data warehousing?",
        "type": "mcq",
        "o": [
            "Scalability and flexibility",
            "Lower security",
            "Fixed capacity",
            "Manual maintenance"
        ]
    },
    {
        "q": "Match the ETL phase with its action:",
        "type": "match",
        "left": [
            "Extract",
            "Transform",
            "Load",
            "Validate"
        ],
        "right": [
            "Pull data from sources",
            "Clean and format",
            "Store in warehouse",
            "Check data quality"
        ]
    },
    {
        "q": "On-premise data warehouses require companies to manage their own _____.",
        "type": "fill_blank",
        "answers": [
            "infrastructure"
        ],
        "other_options": [
            "employees",
            "customers",
            "vendors"
        ]
    },
    {
        "q": "Which type of data warehouse architecture has multiple data marts feeding into a central warehouse?",
        "type": "mcq",
        "o": [
            "Hub and Spoke",
            "Independent Data Mart",
            "Flat Architecture",
            "Single Layer"
        ]
    },
    {
        "q": "Cloud data warehouses typically offer pay-as-you-go pricing.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is a staging area in data warehousing?",
        "type": "mcq",
        "o": [
            "A temporary storage area before data is loaded into the warehouse",
            "The final destination for analyzed data",
            "A backup storage system",
            "A user interface for queries"
        ]
    },
    {
        "q": "OLAP systems are optimized for _____ queries.",
        "type": "fill_blank",
        "answers": [
            "analytical"
        ],
        "other_options": [
            "transactional",
            "simple",
            "delete"
        ]
    },
    {
        "q": "Which is NOT a benefit of data warehousing?",
        "type": "mcq",
        "o": [
            "Real-time transaction processing",
            "Historical data analysis",
            "Improved decision making",
            "Consolidated data view"
        ]
    },
    {
        "q": "Rearrange the data warehouse layers from source to user:",
        "type": "rearrange",
        "words": [
            "Source Systems",
            "Staging Area",
            "Data Warehouse",
            "Data Marts",
            "End Users"
        ]
    },
    {
        "q": "A dependent data mart is created from an existing enterprise data warehouse.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does the Extract phase in ETL involve?",
        "type": "mcq",
        "o": [
            "Pulling data from source systems",
            "Cleaning data for quality",
            "Loading data into the warehouse",
            "Creating reports"
        ]
    },
    {
        "q": "Match the data warehouse component with its description:",
        "type": "match",
        "left": [
            "Staging Area",
            "ODS",
            "Data Mart",
            "EDW"
        ],
        "right": [
            "Temporary data storage",
            "Near real-time data",
            "Subject-specific data",
            "Enterprise-wide data"
        ]
    },
    {
        "q": "The _____ layer in a data warehouse is where end users access data.",
        "type": "fill_blank",
        "answers": [
            "presentation"
        ],
        "other_options": [
            "staging",
            "extraction",
            "source"
        ]
    },
    {
        "q": "Which architecture uses a single integrated repository for all data?",
        "type": "mcq",
        "o": [
            "Centralized Data Warehouse",
            "Federated Data Warehouse",
            "Virtual Data Warehouse",
            "Distributed Database"
        ]
    },
    {
        "q": "OLTP databases typically have normalized schemas.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is an Operational Data Store (ODS)?",
        "type": "mcq",
        "o": [
            "A database for current operational data with near real-time updates",
            "A long-term historical data storage",
            "A backup system for the data warehouse",
            "A reporting tool"
        ]
    },
    {
        "q": "Cloud data warehouses eliminate the need for _____ management.",
        "type": "fill_blank",
        "answers": [
            "hardware"
        ],
        "other_options": [
            "data",
            "query",
            "user"
        ]
    },
    {
        "q": "Which is a common on-premise data warehouse solution?",
        "type": "mcq",
        "o": [
            "Teradata",
            "BigQuery",
            "Snowflake",
            "Redshift"
        ]
    },
    {
        "q": "Data warehouses typically store data in a denormalized format for query performance.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the warehouse type with its deployment:",
        "type": "match",
        "left": [
            "On-premise",
            "Cloud",
            "Hybrid",
            "Virtual"
        ],
        "right": [
            "Local hardware",
            "Remote servers",
            "Combined approach",
            "Federated access"
        ]
    },
    {
        "q": "What is the main purpose of data transformation?",
        "type": "mcq",
        "o": [
            "Converting data into a consistent format",
            "Deleting duplicate records",
            "Backing up source data",
            "Creating user accounts"
        ]
    },
    {
        "q": "A _____ data warehouse combines on-premise and cloud resources.",
        "type": "fill_blank",
        "answers": [
            "hybrid"
        ],
        "other_options": [
            "virtual",
            "federated",
            "distributed"
        ]
    },
    {
        "q": "Which characteristic describes OLTP systems?",
        "type": "mcq",
        "o": [
            "High volume of short transactions",
            "Complex analytical queries",
            "Historical data focus",
            "Slow response times"
        ]
    },
    {
        "q": "Rearrange the data flow from raw to refined:",
        "type": "rearrange",
        "words": [
            "Raw Data",
            "Cleansed Data",
            "Transformed Data",
            "Aggregated Data"
        ]
    },
    {
        "q": "Data marts can be built using a top-down or bottom-up approach.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is data lineage?",
        "type": "mcq",
        "o": [
            "The tracking of data from origin to destination",
            "The age of data in the warehouse",
            "The size of data files",
            "The format of data storage"
        ]
    },
    {
        "q": "ETL processes typically run during _____ hours to minimize impact.",
        "type": "fill_blank",
        "answers": [
            "off-peak"
        ],
        "other_options": [
            "business",
            "morning",
            "afternoon"
        ]
    },
    {
        "q": "Which is a benefit of using data marts?",
        "type": "mcq",
        "o": [
            "Faster query performance for specific departments",
            "Complete enterprise data access",
            "Reduced data quality",
            "Increased complexity"
        ]
    },
    {
        "q": "Match the term with its meaning:",
        "type": "match",
        "left": [
            "Subject-oriented",
            "Integrated",
            "Non-volatile",
            "Time-variant"
        ],
        "right": [
            "Focused on business area",
            "Consistent data format",
            "Data not frequently changed",
            "Historical perspective"
        ]
    },
    {
        "q": "Bill Inmon is known as the father of data warehousing.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which data warehouse characteristic ensures data consistency across sources?",
        "type": "mcq",
        "o": [
            "Integrated",
            "Subject-oriented",
            "Time-variant",
            "Non-volatile"
        ]
    },
    {
        "q": "The _____ approach builds data marts first before the enterprise warehouse.",
        "type": "fill_blank",
        "answers": [
            "bottom-up"
        ],
        "other_options": [
            "top-down",
            "middle-out",
            "side-in"
        ]
    },
    {
        "q": "What is the primary difference between OLTP and OLAP response times?",
        "type": "mcq",
        "o": [
            "OLTP is faster for individual transactions",
            "OLAP is faster for individual transactions",
            "Both have the same response times",
            "Neither focuses on response time"
        ]
    },
    {
        "q": "Data warehouses support CRUD operations for daily transactions.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which component validates data before loading into the warehouse?",
        "type": "mcq",
        "o": [
            "Data quality rules",
            "User interface",
            "Report generator",
            "Backup system"
        ]
    },
    {
        "q": "Rearrange the Kimball dimensional modeling steps:",
        "type": "rearrange",
        "words": [
            "Select Process",
            "Declare Grain",
            "Identify Dimensions",
            "Identify Facts"
        ]
    },
    {
        "q": "The _____ approach builds the enterprise warehouse first, then creates data marts.",
        "type": "fill_blank",
        "answers": [
            "top-down"
        ],
        "other_options": [
            "bottom-up",
            "iterative",
            "agile"
        ]
    },
    {
        "q": "Which is a characteristic of cloud data warehouses?",
        "type": "mcq",
        "o": [
            "Elastic scalability",
            "Fixed storage capacity",
            "Manual software updates",
            "Physical hardware ownership"
        ]
    },
    {
        "q": "Match the architect with their approach:",
        "type": "match",
        "left": [
            "Bill Inmon",
            "Ralph Kimball",
            "Hybrid",
            "Modern"
        ],
        "right": [
            "Top-down EDW",
            "Bottom-up data marts",
            "Combined approach",
            "Data lakehouse"
        ]
    },
    {
        "q": "An independent data mart does not rely on a central data warehouse.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the Load phase in ETL responsible for?",
        "type": "mcq",
        "o": [
            "Writing transformed data to the target database",
            "Reading data from source systems",
            "Cleaning and validating data",
            "Creating backup copies"
        ]
    },
    {
        "q": "OLAP databases are designed for _____ users.",
        "type": "fill_blank",
        "answers": [
            "analytical"
        ],
        "other_options": [
            "operational",
            "administrative",
            "technical"
        ]
    },
    {
        "q": "Which is NOT a typical data source for a data warehouse?",
        "type": "mcq",
        "o": [
            "End user reports",
            "Transactional databases",
            "CRM systems",
            "ERP systems"
        ]
    },
    {
        "q": "Cloud warehouses require upfront capital investment for hardware.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What is metadata in the context of data warehousing?",
        "type": "mcq",
        "o": [
            "Data about data",
            "Compressed data",
            "Deleted data",
            "Encrypted data"
        ]
    },
    {
        "q": "Match the cloud provider with its data warehouse service:",
        "type": "match",
        "left": [
            "AWS",
            "Google Cloud",
            "Microsoft Azure",
            "Snowflake"
        ],
        "right": [
            "Redshift",
            "BigQuery",
            "Synapse Analytics",
            "Snowflake"
        ]
    },
    {
        "q": "The _____ of a data warehouse refers to the level of detail stored.",
        "type": "fill_blank",
        "answers": [
            "granularity"
        ],
        "other_options": [
            "capacity",
            "bandwidth",
            "latency"
        ]
    },
    {
        "q": "Which type of ETL runs continuously rather than in batches?",
        "type": "mcq",
        "o": [
            "Real-time ETL",
            "Batch ETL",
            "Manual ETL",
            "Offline ETL"
        ]
    },
    {
        "q": "Rearrange the data warehouse development phases:",
        "type": "rearrange",
        "words": [
            "Requirements",
            "Design",
            "Development",
            "Testing",
            "Deployment"
        ]
    },
    {
        "q": "OLTP systems typically store years of historical data.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What is a conformed dimension?",
        "type": "mcq",
        "o": [
            "A dimension shared across multiple data marts",
            "A dimension unique to one data mart",
            "A temporary dimension",
            "A deleted dimension"
        ]
    },
    {
        "q": "Data warehouses are _____ oriented, focusing on specific business subjects.",
        "type": "fill_blank",
        "answers": [
            "subject"
        ],
        "other_options": [
            "object",
            "process",
            "system"
        ]
    },
    {
        "q": "Which describes the non-volatile nature of data warehouses?",
        "type": "mcq",
        "o": [
            "Data is not updated once loaded",
            "Data changes frequently",
            "Data is temporary",
            "Data is compressed"
        ]
    },
    {
        "q": "Match the data warehouse layer with its function:",
        "type": "match",
        "left": [
            "Source",
            "Staging",
            "Integration",
            "Access"
        ],
        "right": [
            "Original data",
            "Temporary storage",
            "Combined data",
            "User queries"
        ]
    },
    {
        "q": "On-premise solutions offer more control over data security.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is a data dictionary?",
        "type": "mcq",
        "o": [
            "A repository of metadata definitions",
            "A database backup",
            "A query optimizer",
            "A data validation tool"
        ]
    },
    {
        "q": "The time-variant characteristic means data warehouses capture data at _____ points in time.",
        "type": "fill_blank",
        "answers": [
            "different"
        ],
        "other_options": [
            "single",
            "random",
            "unknown"
        ]
    },
    {
        "q": "Which ETL approach processes data as it arrives?",
        "type": "mcq",
        "o": [
            "Streaming ETL",
            "Batch ETL",
            "Daily ETL",
            "Weekly ETL"
        ]
    },
    {
        "q": "Data marts always contain summarized data only.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What is Change Data Capture (CDC)?",
        "type": "mcq",
        "o": [
            "A method to identify and capture data changes",
            "A backup strategy",
            "A security protocol",
            "A user interface"
        ]
    },
    {
        "q": "Rearrange the ETL error handling priority:",
        "type": "rearrange",
        "words": [
            "Detect Error",
            "Log Error",
            "Notify Admin",
            "Resolve Issue"
        ]
    },
    {
        "q": "_____ ETL extracts only the data that has changed since the last run.",
        "type": "fill_blank",
        "answers": [
            "Incremental"
        ],
        "other_options": [
            "Full",
            "Partial",
            "Complete"
        ]
    },
    {
        "q": "Which is a disadvantage of on-premise data warehouses?",
        "type": "mcq",
        "o": [
            "High upfront costs",
            "Unlimited scalability",
            "No maintenance required",
            "Automatic updates"
        ]
    },
    {
        "q": "Match the ETL tool with its vendor:",
        "type": "match",
        "left": [
            "Informatica",
            "Talend",
            "SSIS",
            "DataStage"
        ],
        "right": [
            "Informatica Corp",
            "Open source",
            "Microsoft",
            "IBM"
        ]
    },
    {
        "q": "Cloud data warehouses typically provide built-in disaster recovery.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is a data pipeline?",
        "type": "mcq",
        "o": [
            "An automated flow of data from source to destination",
            "A physical network cable",
            "A manual data entry process",
            "A user login system"
        ]
    },
    {
        "q": "OLAP cubes provide _____ views of data.",
        "type": "fill_blank",
        "answers": [
            "multidimensional"
        ],
        "other_options": [
            "single",
            "flat",
            "linear"
        ]
    },
    {
        "q": "Which component is NOT part of a typical data warehouse architecture?",
        "type": "mcq",
        "o": [
            "Web server",
            "Staging area",
            "Data marts",
            "ETL processes"
        ]
    },
    {
        "q": "Data warehouses store transactional and analytical data together.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What is data profiling?",
        "type": "mcq",
        "o": [
            "Analyzing data to understand its structure and quality",
            "Encrypting sensitive data",
            "Deleting outdated records",
            "Creating user profiles"
        ]
    },
    {
        "q": "Match the data warehouse model with its focus:",
        "type": "match",
        "left": [
            "Inmon",
            "Kimball",
            "Data Vault",
            "Lakehouse"
        ],
        "right": [
            "Normalized EDW",
            "Dimensional marts",
            "Auditability",
            "Combined storage"
        ]
    },
    {
        "q": "_____ is the process of removing duplicate records from data.",
        "type": "fill_blank",
        "answers": [
            "Deduplication"
        ],
        "other_options": [
            "Aggregation",
            "Normalization",
            "Encryption"
        ]
    },
    {
        "q": "Which is a benefit of separating OLTP and OLAP systems?",
        "type": "mcq",
        "o": [
            "Preventing analytical queries from slowing transactions",
            "Reducing data storage needs",
            "Eliminating the need for backups",
            "Simplifying user training"
        ]
    },
    {
        "q": "Rearrange the data quality dimensions in order of typical priority:",
        "type": "rearrange",
        "words": [
            "Accuracy",
            "Completeness",
            "Consistency",
            "Timeliness"
        ]
    },
    {
        "q": "Hybrid cloud architecture allows data to move between on-premise and cloud.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is a surrogate key?",
        "type": "mcq",
        "o": [
            "An artificial key generated by the data warehouse",
            "A natural key from the source system",
            "A temporary key for testing",
            "A foreign key reference"
        ]
    },
    {
        "q": "The _____ layer is where raw data is first loaded before processing.",
        "type": "fill_blank",
        "answers": [
            "staging"
        ],
        "other_options": [
            "presentation",
            "access",
            "reporting"
        ]
    },
    {
        "q": "Which data warehouse characteristic refers to focusing on specific business areas?",
        "type": "mcq",
        "o": [
            "Subject-oriented",
            "Integrated",
            "Time-variant",
            "Non-volatile"
        ]
    },
    {
        "q": "ETL and ELT are the same process with no differences.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What is an enterprise data warehouse (EDW)?",
        "type": "mcq",
        "o": [
            "A central repository of integrated data from across the organization",
            "A small departmental database",
            "A temporary storage system",
            "A reporting tool"
        ]
    },
    {
        "q": "Match the data type with its typical source:",
        "type": "match",
        "left": [
            "Sales Data",
            "Customer Data",
            "Inventory Data",
            "Financial Data"
        ],
        "right": [
            "POS Systems",
            "CRM",
            "ERP",
            "Accounting Systems"
        ]
    },
    {
        "q": "What SQL query is used to extract data from a source database?",
        "type": "mcq",
        "c": "SELECT customer_id, order_date, total_amount\nFROM orders\nWHERE order_date >= '2024-01-01'",
        "o": [
            "SELECT with WHERE clause",
            "INSERT statement",
            "DELETE query",
            "DROP command"
        ]
    },
    {
        "q": "A _____ schema uses multiple related tables for improved data integrity.",
        "type": "fill_blank",
        "answers": [
            "normalized"
        ],
        "other_options": [
            "denormalized",
            "flat",
            "simple"
        ]
    },
    {
        "q": "Which loading strategy inserts all records and replaces existing data?",
        "type": "mcq",
        "o": [
            "Full load",
            "Incremental load",
            "Delta load",
            "Micro-batch load"
        ]
    },
    {
        "q": "Data warehouses in the cloud automatically handle software patches and updates.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this SQL query calculating data freshness?",
        "type": "mcq",
        "c": "SELECT DATEDIFF(CURRENT_DATE, MAX(load_date)) as days_since_load\nFROM warehouse_log",
        "o": [
            "Number of days since last data load",
            "Total number of records",
            "Average load time",
            "Minimum date value"
        ]
    },
    {
        "q": "Match the data mart type with its characteristic:",
        "type": "match",
        "left": [
            "Dependent",
            "Independent",
            "Hybrid",
            "Virtual"
        ],
        "right": [
            "Fed from EDW",
            "Direct from sources",
            "Combined approach",
            "No physical storage"
        ]
    },
    {
        "q": "The _____ layer transforms data into a format suitable for analysis.",
        "type": "fill_blank",
        "answers": [
            "integration"
        ],
        "other_options": [
            "source",
            "access",
            "staging"
        ]
    },
    {
        "q": "Which OLAP operation drills down from year to quarter level?",
        "type": "mcq",
        "o": [
            "Drill-down",
            "Roll-up",
            "Slice",
            "Dice"
        ]
    },
    {
        "q": "Federated data warehouses physically move and copy all source data.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What is a data lake?",
        "type": "mcq",
        "o": [
            "A storage repository for raw data in native format",
            "A structured data warehouse",
            "A real-time processing engine",
            "A backup system"
        ]
    },
    {
        "q": "Rearrange the OLAP operations from detailed to summarized:",
        "type": "rearrange",
        "words": [
            "Transaction Level",
            "Daily Summary",
            "Monthly Aggregate",
            "Yearly Total"
        ]
    },
    {
        "q": "_____ processing handles data in fixed-size groups at scheduled intervals.",
        "type": "fill_blank",
        "answers": [
            "Batch"
        ],
        "other_options": [
            "Stream",
            "Real-time",
            "Interactive"
        ]
    },
    {
        "q": "Which component manages ETL job scheduling and dependencies?",
        "type": "mcq",
        "o": [
            "Workflow orchestrator",
            "Data dictionary",
            "Report server",
            "User portal"
        ]
    },
    {
        "q": "Match the architecture pattern with its data flow:",
        "type": "match",
        "left": [
            "Inmon",
            "Kimball",
            "Data Vault",
            "Lakehouse"
        ],
        "right": [
            "Sources to EDW to marts",
            "Sources to marts",
            "Sources to hub-satellite",
            "Lake with warehouse layer"
        ]
    },
    {
        "q": "A virtual data warehouse creates an abstraction layer over source systems.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the result of this transformation logic?",
        "type": "mcq",
        "c": "CASE \n  WHEN total_sales > 100000 THEN 'High'\n  WHEN total_sales > 50000 THEN 'Medium'\n  ELSE 'Low'\nEND as sales_tier",
        "o": [
            "Categorizing sales into tiers",
            "Calculating total sales",
            "Filtering null values",
            "Joining tables"
        ]
    },
    {
        "q": "The _____ key is the original identifier from the source system.",
        "type": "fill_blank",
        "answers": [
            "natural"
        ],
        "other_options": [
            "surrogate",
            "foreign",
            "composite"
        ]
    },
    {
        "q": "Which cloud pricing model charges based on data scanned?",
        "type": "mcq",
        "o": [
            "Query-based pricing",
            "Fixed subscription",
            "Per-user licensing",
            "Hardware leasing"
        ]
    },
    {
        "q": "OLAP cubes pre-aggregate data for faster query performance.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is data latency in a data warehouse context?",
        "type": "mcq",
        "o": [
            "The delay between source data changes and warehouse updates",
            "The speed of query execution",
            "The size of data files",
            "The number of users"
        ]
    },
    {
        "q": "Match the ETL challenge with its solution:",
        "type": "match",
        "left": [
            "Data quality issues",
            "Performance bottlenecks",
            "Schema changes",
            "Error handling"
        ],
        "right": [
            "Validation rules",
            "Parallel processing",
            "Schema evolution",
            "Error logging"
        ]
    },
    {
        "q": "_____ metadata describes the technical aspects of data storage.",
        "type": "fill_blank",
        "answers": [
            "Technical"
        ],
        "other_options": [
            "Business",
            "Operational",
            "Strategic"
        ]
    },
    {
        "q": "Which technique optimizes query performance by pre-computing aggregates?",
        "type": "mcq",
        "o": [
            "Materialized views",
            "Temporary tables",
            "Cursors",
            "Triggers"
        ]
    },
    {
        "q": "Rearrange the data warehouse maturity levels:",
        "type": "rearrange",
        "words": [
            "Spreadsheets",
            "Data Marts",
            "Enterprise DW",
            "Data Lakehouse"
        ]
    },
    {
        "q": "Active data warehousing supports near real-time data updates.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this SQL accomplish in ETL?",
        "type": "mcq",
        "c": "MERGE INTO target_table t\nUSING source_table s\nON t.id = s.id\nWHEN MATCHED THEN UPDATE SET t.value = s.value\nWHEN NOT MATCHED THEN INSERT (id, value) VALUES (s.id, s.value)",
        "o": [
            "Upsert - update existing or insert new records",
            "Delete all matching records",
            "Create a backup copy",
            "Drop the source table"
        ]
    },
    {
        "q": "The _____ of a query plan shows the steps the database will execute.",
        "type": "fill_blank",
        "answers": [
            "execution"
        ],
        "other_options": [
            "compilation",
            "optimization",
            "validation"
        ]
    },
    {
        "q": "Which architecture separates compute and storage resources?",
        "type": "mcq",
        "o": [
            "Cloud-native architecture",
            "Monolithic architecture",
            "Single-tier architecture",
            "Mainframe architecture"
        ]
    },
    {
        "q": "Match the data quality dimension with its meaning:",
        "type": "match",
        "left": [
            "Accuracy",
            "Completeness",
            "Consistency",
            "Timeliness"
        ],
        "right": [
            "Data correctness",
            "No missing values",
            "Same across systems",
            "Current data"
        ]
    },
    {
        "q": "Slowly Changing Dimensions track changes to dimension attributes over time.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the purpose of a staging table?",
        "type": "mcq",
        "o": [
            "Temporarily hold data during ETL processing",
            "Store final reports",
            "Manage user access",
            "Archive deleted records"
        ]
    },
    {
        "q": "_____ integration combines data from multiple sources into a unified view.",
        "type": "fill_blank",
        "answers": [
            "Data"
        ],
        "other_options": [
            "System",
            "Process",
            "Network"
        ]
    },
    {
        "q": "Which OLAP operation selects a subset of data based on one dimension?",
        "type": "mcq",
        "o": [
            "Slice",
            "Dice",
            "Roll-up",
            "Drill-down"
        ]
    },
    {
        "q": "Rearrange the ETL testing phases:",
        "type": "rearrange",
        "words": [
            "Unit Testing",
            "Integration Testing",
            "System Testing",
            "UAT"
        ]
    },
    {
        "q": "Data governance includes policies for data access and security.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this window function calculate?",
        "type": "mcq",
        "c": "SELECT product_id, sales_amount,\n  SUM(sales_amount) OVER (PARTITION BY category ORDER BY sale_date) as running_total\nFROM sales",
        "o": [
            "Running total of sales per category",
            "Grand total of all sales",
            "Average sales amount",
            "Maximum sale value"
        ]
    },
    {
        "q": "The _____ bus architecture ensures dimension conformity across data marts.",
        "type": "fill_blank",
        "answers": [
            "enterprise"
        ],
        "other_options": [
            "local",
            "departmental",
            "regional"
        ]
    },
    {
        "q": "Which component tracks data origin and transformation history?",
        "type": "mcq",
        "o": [
            "Data lineage system",
            "User management system",
            "Billing system",
            "Notification system"
        ]
    },
    {
        "q": "Match the warehouse layer with example activities:",
        "type": "match",
        "left": [
            "Extract",
            "Stage",
            "Transform",
            "Load"
        ],
        "right": [
            "Connect to source",
            "Land raw data",
            "Apply business rules",
            "Insert into DW"
        ]
    },
    {
        "q": "Multi-tenant cloud warehouses share resources among multiple customers.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the primary purpose of data cleansing?",
        "type": "mcq",
        "o": [
            "Removing errors and inconsistencies from data",
            "Encrypting sensitive information",
            "Compressing data for storage",
            "Backing up databases"
        ]
    },
    {
        "q": "A _____ refresh strategy reloads the entire dataset.",
        "type": "fill_blank",
        "answers": [
            "full"
        ],
        "other_options": [
            "partial",
            "delta",
            "micro"
        ]
    },
    {
        "q": "Which SQL clause is commonly used in ETL for conditional logic?",
        "type": "mcq",
        "o": [
            "CASE WHEN",
            "GROUP BY",
            "ORDER BY",
            "HAVING"
        ]
    },
    {
        "q": "Rearrange the data quality management steps:",
        "type": "rearrange",
        "words": [
            "Profile Data",
            "Define Rules",
            "Cleanse Data",
            "Monitor Quality"
        ]
    },
    {
        "q": "A sandbox environment is used for production workloads.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What is the result of this ETL transformation?",
        "type": "mcq",
        "c": "SELECT UPPER(TRIM(customer_name)) as clean_name,\n  COALESCE(phone, 'N/A') as phone_number\nFROM raw_customers",
        "o": [
            "Standardizing and handling null values",
            "Deleting duplicate records",
            "Creating new customers",
            "Archiving old data"
        ]
    },
    {
        "q": "_____ partitioning divides data based on a key value range.",
        "type": "fill_blank",
        "answers": [
            "Range"
        ],
        "other_options": [
            "Hash",
            "List",
            "Round-robin"
        ]
    },
    {
        "q": "Which type of metadata describes business meaning and context?",
        "type": "mcq",
        "o": [
            "Business metadata",
            "Technical metadata",
            "Operational metadata",
            "Administrative metadata"
        ]
    },
    {
        "q": "Match the data issue with its type:",
        "type": "match",
        "left": [
            "Missing values",
            "Invalid format",
            "Duplicate records",
            "Outdated data"
        ],
        "right": [
            "Completeness",
            "Accuracy",
            "Uniqueness",
            "Timeliness"
        ]
    },
    {
        "q": "Data masking protects sensitive information during testing.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the OLAP dice operation?",
        "type": "mcq",
        "o": [
            "Selecting data across multiple dimensions",
            "Aggregating data to higher level",
            "Breaking down to detailed level",
            "Filtering on single dimension"
        ]
    },
    {
        "q": "A _____ table stores business process measurements.",
        "type": "fill_blank",
        "answers": [
            "fact"
        ],
        "other_options": [
            "dimension",
            "bridge",
            "lookup"
        ]
    },
    {
        "q": "Which pattern handles many-to-many relationships in dimensional models?",
        "type": "mcq",
        "o": [
            "Bridge table",
            "Fact table",
            "Dimension table",
            "Staging table"
        ]
    },
    {
        "q": "Rearrange the cloud migration phases:",
        "type": "rearrange",
        "words": [
            "Assessment",
            "Planning",
            "Migration",
            "Optimization"
        ]
    },
    {
        "q": "Real-time data warehousing typically has higher latency than batch processing.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What does this validation query check?",
        "type": "mcq",
        "c": "SELECT COUNT(*) as source_count FROM source_table\nUNION ALL\nSELECT COUNT(*) as target_count FROM target_table",
        "o": [
            "Record count reconciliation between source and target",
            "Data type validation",
            "Duplicate detection",
            "Null value count"
        ]
    },
    {
        "q": "_____ compression reduces storage space by encoding repeated values.",
        "type": "fill_blank",
        "answers": [
            "Data"
        ],
        "other_options": [
            "File",
            "Network",
            "Memory"
        ]
    },
    {
        "q": "Which service level agreement metric measures system availability?",
        "type": "mcq",
        "o": [
            "Uptime percentage",
            "Query count",
            "User satisfaction",
            "Data volume"
        ]
    },
    {
        "q": "Match the cloud warehouse feature with its benefit:",
        "type": "match",
        "left": [
            "Auto-scaling",
            "Separation of compute/storage",
            "Serverless",
            "Caching"
        ],
        "right": [
            "Handle variable workloads",
            "Independent scaling",
            "No infrastructure management",
            "Faster queries"
        ]
    },
    {
        "q": "Partitioning improves query performance by reducing data scanned.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is a junk dimension?",
        "type": "mcq",
        "o": [
            "A dimension combining low-cardinality flags and indicators",
            "A dimension for deleted records",
            "A temporary dimension for testing",
            "An unused dimension table"
        ]
    },
    {
        "q": "The _____ pattern uses a central fact connected to dimension tables.",
        "type": "fill_blank",
        "answers": [
            "star"
        ],
        "other_options": [
            "snowflake",
            "galaxy",
            "flat"
        ]
    },
    {
        "q": "Which technique improves query performance by distributing data across nodes?",
        "type": "mcq",
        "o": [
            "Data sharding",
            "Data archiving",
            "Data encryption",
            "Data validation"
        ]
    },
    {
        "q": "Rearrange the data retention lifecycle:",
        "type": "rearrange",
        "words": [
            "Active",
            "Near-line",
            "Archive",
            "Purge"
        ]
    },
    {
        "q": "Column-oriented storage is optimized for analytical queries.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the purpose of this ETL logging?",
        "type": "mcq",
        "c": "INSERT INTO etl_log (job_name, start_time, status, records_processed)\nVALUES ('daily_sales_load', CURRENT_TIMESTAMP, 'RUNNING', 0)",
        "o": [
            "Tracking ETL job execution for monitoring",
            "Creating sales records",
            "Deleting old log entries",
            "Updating customer data"
        ]
    },
    {
        "q": "_____ tables store descriptive attributes for business entities.",
        "type": "fill_blank",
        "answers": [
            "Dimension"
        ],
        "other_options": [
            "Fact",
            "Bridge",
            "Aggregate"
        ]
    },
    {
        "q": "Which approach extracts only changed records since last load?",
        "type": "mcq",
        "o": [
            "Change Data Capture",
            "Full extraction",
            "Random sampling",
            "Manual export"
        ]
    },
    {
        "q": "Match ETL best practice with its purpose:",
        "type": "match",
        "left": [
            "Logging",
            "Error handling",
            "Validation",
            "Documentation"
        ],
        "right": [
            "Audit trail",
            "Recovery",
            "Data quality",
            "Maintenance"
        ]
    },
    {
        "q": "Aggregate tables pre-calculate summaries for common queries.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the primary benefit of columnar storage?",
        "type": "mcq",
        "o": [
            "Better compression and faster analytical queries",
            "Faster row-level updates",
            "Simpler schema design",
            "Lower storage costs for OLTP"
        ]
    },
    {
        "q": "A _____ query spans multiple data sources without moving data.",
        "type": "fill_blank",
        "answers": [
            "federated"
        ],
        "other_options": [
            "local",
            "cached",
            "temporary"
        ]
    },
    {
        "q": "Which tool category handles workflow automation and job dependencies?",
        "type": "mcq",
        "o": [
            "Orchestration tools",
            "Visualization tools",
            "Reporting tools",
            "Security tools"
        ]
    },
    {
        "q": "Rearrange the data modeling steps:",
        "type": "rearrange",
        "words": [
            "Conceptual Model",
            "Logical Model",
            "Physical Model",
            "Implementation"
        ]
    },
    {
        "q": "A denormalized schema reduces the number of joins needed for queries.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this query identify?",
        "type": "mcq",
        "c": "SELECT customer_id, COUNT(*) as occurrences\nFROM customers\nGROUP BY customer_id\nHAVING COUNT(*) > 1",
        "o": [
            "Duplicate customer records",
            "Top customers by sales",
            "Missing customer data",
            "Inactive customers"
        ]
    },
    {
        "q": "The _____ approach to data warehousing emphasizes agility and iteration.",
        "type": "fill_blank",
        "answers": [
            "agile"
        ],
        "other_options": [
            "waterfall",
            "rigid",
            "traditional"
        ]
    },
    {
        "q": "Which characteristic differentiates a data lake from a data warehouse?",
        "type": "mcq",
        "o": [
            "Data lake stores raw unstructured data",
            "Data lake only stores structured data",
            "Data lake uses strict schemas",
            "Data lake has faster query performance"
        ]
    },
    {
        "q": "Match the storage format with its use case:",
        "type": "match",
        "left": [
            "CSV",
            "Parquet",
            "Avro",
            "JSON"
        ],
        "right": [
            "Simple exports",
            "Columnar analytics",
            "Schema evolution",
            "Semi-structured data"
        ]
    },
    {
        "q": "ACID properties ensure transaction reliability in databases.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is eventual consistency in distributed systems?",
        "type": "mcq",
        "o": [
            "All copies will eventually have the same data",
            "Instant synchronization across nodes",
            "Data is never consistent",
            "Consistency is optional"
        ]
    },
    {
        "q": "_____ keys uniquely identify dimension records without business meaning.",
        "type": "fill_blank",
        "answers": [
            "Surrogate"
        ],
        "other_options": [
            "Natural",
            "Composite",
            "Foreign"
        ]
    },
    {
        "q": "Which strategy handles data that arrives late for ETL?",
        "type": "mcq",
        "o": [
            "Late arriving data handling",
            "Early data deletion",
            "Real-time rejection",
            "Automatic archiving"
        ]
    },
    {
        "q": "Rearrange the SCD Type 2 steps:",
        "type": "rearrange",
        "words": [
            "Detect Change",
            "Close Current Row",
            "Insert New Row",
            "Update Keys"
        ]
    },
    {
        "q": "Type 1 SCD overwrites old values with new ones.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the purpose of a degenerate dimension?",
        "type": "mcq",
        "o": [
            "Store dimension attributes directly in the fact table",
            "Create a separate dimension table",
            "Archive old dimension data",
            "Link multiple fact tables"
        ]
    },
    {
        "q": "The _____ of an ETL job indicates whether it completed successfully.",
        "type": "fill_blank",
        "answers": [
            "status"
        ],
        "other_options": [
            "name",
            "schedule",
            "owner"
        ]
    },
    {
        "q": "Which clause limits query results for pagination?",
        "type": "mcq",
        "c": "SELECT * FROM large_table\nORDER BY created_date\nLIMIT 100 OFFSET 200",
        "o": [
            "LIMIT and OFFSET",
            "WHERE and AND",
            "GROUP BY and HAVING",
            "JOIN and ON"
        ]
    },
    {
        "q": "Match the data loading pattern with its description:",
        "type": "match",
        "left": [
            "Full load",
            "Incremental",
            "Delta",
            "Streaming"
        ],
        "right": [
            "Complete replacement",
            "Changed records only",
            "Differences from last load",
            "Continuous flow"
        ]
    },
    {
        "q": "Indexing speeds up data retrieval at the cost of write performance.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is a role-playing dimension?",
        "type": "mcq",
        "o": [
            "A dimension used multiple times with different roles in a fact table",
            "A dimension with user role information",
            "A temporary dimension for testing",
            "A dimension for game data"
        ]
    },
    {
        "q": "Data _____ ensures that sensitive information is protected appropriately.",
        "type": "fill_blank",
        "answers": [
            "security"
        ],
        "other_options": [
            "quality",
            "volume",
            "velocity"
        ]
    },
    {
        "q": "Which metric measures the time to complete an ETL job?",
        "type": "mcq",
        "o": [
            "Job duration",
            "Data volume",
            "Error count",
            "User count"
        ]
    },
    {
        "q": "Rearrange the data warehouse project phases:",
        "type": "rearrange",
        "words": [
            "Discovery",
            "Design",
            "Build",
            "Deploy",
            "Operate"
        ]
    },
    {
        "q": "Conformed dimensions enable consistent reporting across data marts.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the purpose of this incremental load query?",
        "type": "mcq",
        "c": "SELECT * FROM source_orders\nWHERE last_modified > (\n  SELECT MAX(load_timestamp)\n  FROM etl_control\n  WHERE table_name = 'orders'\n)",
        "o": [
            "Extract only records changed since last ETL run",
            "Delete all old records",
            "Create a full backup",
            "Update all timestamps"
        ]
    },
    {
        "q": "A _____ dimension contains hierarchical data that can be navigated up or down.",
        "type": "fill_blank",
        "answers": [
            "hierarchical"
        ],
        "other_options": [
            "flat",
            "junk",
            "degenerate"
        ]
    },
    {
        "q": "Which SCD type adds a new row for each change while keeping history?",
        "type": "mcq",
        "o": [
            "Type 2",
            "Type 1",
            "Type 0",
            "Type 3"
        ]
    },
    {
        "q": "Match the SCD type with its behavior:",
        "type": "match",
        "left": [
            "Type 0",
            "Type 1",
            "Type 2",
            "Type 3"
        ],
        "right": [
            "Retain original",
            "Overwrite",
            "Add row with history",
            "Add column for previous"
        ]
    },
    {
        "q": "Micro-batch processing provides lower latency than traditional batch processing.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this query accomplish for data quality?",
        "type": "mcq",
        "c": "SELECT column_name, \n  COUNT(*) as total,\n  COUNT(column_name) as non_null,\n  COUNT(DISTINCT column_name) as unique_vals\nFROM target_table\nGROUP BY column_name",
        "o": [
            "Profile data completeness and cardinality",
            "Delete null values",
            "Create indexes",
            "Backup the table"
        ]
    },
    {
        "q": "The _____ pattern in Data Vault stores business keys and relationships.",
        "type": "fill_blank",
        "answers": [
            "hub"
        ],
        "other_options": [
            "satellite",
            "link",
            "bridge"
        ]
    },
    {
        "q": "Which component in Data Vault architecture stores descriptive attributes?",
        "type": "mcq",
        "o": [
            "Satellite",
            "Hub",
            "Link",
            "Bridge"
        ]
    },
    {
        "q": "Rearrange the Data Vault components from core to descriptive:",
        "type": "rearrange",
        "words": [
            "Hub",
            "Link",
            "Satellite",
            "Point-in-Time Table"
        ]
    },
    {
        "q": "Data Vault is designed for auditability and historical tracking.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the result of this hash key generation?",
        "type": "mcq",
        "c": "SELECT MD5(CONCAT(\n  COALESCE(customer_id, ''),\n  COALESCE(order_id, '')\n)) as hash_key\nFROM staging_orders",
        "o": [
            "Create a unique identifier from multiple columns",
            "Encrypt customer data",
            "Calculate order total",
            "Validate data format"
        ]
    },
    {
        "q": "Match the architecture component with its Data Vault equivalent:",
        "type": "match",
        "left": [
            "Business key",
            "Relationship",
            "Attributes",
            "History"
        ],
        "right": [
            "Hub",
            "Link",
            "Satellite",
            "Effectivity satellite"
        ]
    },
    {
        "q": "A _____ satellite captures changes to attributes over time.",
        "type": "fill_blank",
        "answers": [
            "historized"
        ],
        "other_options": [
            "static",
            "temporary",
            "cached"
        ]
    },
    {
        "q": "Which loading pattern applies all changes in a single transaction?",
        "type": "mcq",
        "o": [
            "Atomic load",
            "Trickle load",
            "Partial load",
            "Random load"
        ]
    },
    {
        "q": "Zero-copy cloning creates a full physical copy of the data.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What optimization does this query use?",
        "type": "mcq",
        "c": "SELECT /*+ PARALLEL(8) */ \n  customer_id, SUM(amount)\nFROM large_fact_table\nGROUP BY customer_id",
        "o": [
            "Parallel query execution with 8 threads",
            "Sequential processing",
            "Single-threaded optimization",
            "Index-only scan"
        ]
    },
    {
        "q": "The _____ framework provides a methodology for data warehouse automation.",
        "type": "fill_blank",
        "answers": [
            "automation"
        ],
        "other_options": [
            "manual",
            "legacy",
            "simple"
        ]
    },
    {
        "q": "Which technique reduces ETL processing time by processing data in parallel?",
        "type": "mcq",
        "o": [
            "Parallel processing",
            "Sequential processing",
            "Single-threaded execution",
            "Synchronous loading"
        ]
    },
    {
        "q": "Match the performance technique with its benefit:",
        "type": "match",
        "left": [
            "Partitioning",
            "Indexing",
            "Caching",
            "Compression"
        ],
        "right": [
            "Reduce data scanned",
            "Speed up lookups",
            "Faster repeated queries",
            "Lower storage costs"
        ]
    },
    {
        "q": "Rearrange the query optimization steps:",
        "type": "rearrange",
        "words": [
            "Parse Query",
            "Generate Plan",
            "Optimize Plan",
            "Execute Query"
        ]
    },
    {
        "q": "Predicate pushdown filters data at the source before loading.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the purpose of this data type conversion?",
        "type": "mcq",
        "c": "SELECT CAST(order_date AS DATE) as order_date,\n  CAST(amount AS DECIMAL(10,2)) as amount,\n  CAST(quantity AS INT) as quantity\nFROM staging_orders",
        "o": [
            "Standardize data types for the target schema",
            "Delete invalid records",
            "Create backup copies",
            "Generate reports"
        ]
    },
    {
        "q": "A _____ index stores data in the same order as the table rows.",
        "type": "fill_blank",
        "answers": [
            "clustered"
        ],
        "other_options": [
            "non-clustered",
            "bitmap",
            "hash"
        ]
    },
    {
        "q": "Which index type is optimal for low-cardinality columns?",
        "type": "mcq",
        "o": [
            "Bitmap index",
            "B-tree index",
            "Hash index",
            "Full-text index"
        ]
    },
    {
        "q": "Match the index type with its ideal use case:",
        "type": "match",
        "left": [
            "B-tree",
            "Bitmap",
            "Hash",
            "Clustered"
        ],
        "right": [
            "Range queries",
            "Low cardinality",
            "Equality lookups",
            "Row ordering"
        ]
    },
    {
        "q": "Statistics help the query optimizer choose the best execution plan.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this window function calculate?",
        "type": "mcq",
        "c": "SELECT order_id, order_date,\n  LAG(order_date) OVER (PARTITION BY customer_id ORDER BY order_date) as prev_order,\n  DATEDIFF(order_date, LAG(order_date) OVER (PARTITION BY customer_id ORDER BY order_date)) as days_between\nFROM orders",
        "o": [
            "Days between consecutive customer orders",
            "Total number of orders",
            "Average order value",
            "First order date"
        ]
    },
    {
        "q": "A _____ table stores pre-computed results for common query patterns.",
        "type": "fill_blank",
        "answers": [
            "summary"
        ],
        "other_options": [
            "staging",
            "temporary",
            "backup"
        ]
    },
    {
        "q": "Which technique avoids repeated calculations by storing results?",
        "type": "mcq",
        "o": [
            "Materialized views",
            "Temporary tables",
            "Views",
            "Cursors"
        ]
    },
    {
        "q": "Rearrange the data warehouse refresh frequency from least to most frequent:",
        "type": "rearrange",
        "words": [
            "Monthly",
            "Weekly",
            "Daily",
            "Hourly",
            "Real-time"
        ]
    },
    {
        "q": "Query hints can override the optimizer's automatic plan selection.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the purpose of this control flow logic?",
        "type": "mcq",
        "c": "IF (SELECT COUNT(*) FROM staging_table) > 0\nBEGIN\n  EXEC sp_load_dimension\n  EXEC sp_load_facts\nEND\nELSE\n  PRINT 'No data to process'",
        "o": [
            "Conditionally execute ETL based on data availability",
            "Always run the load process",
            "Delete staging data",
            "Create new tables"
        ]
    },
    {
        "q": "Match the ETL pattern with its scenario:",
        "type": "match",
        "left": [
            "Full refresh",
            "Incremental",
            "CDC",
            "Real-time"
        ],
        "right": [
            "Small dimension tables",
            "Large fact tables",
            "High-frequency changes",
            "Streaming data"
        ]
    },
    {
        "q": "_____ tables temporarily store data during complex multi-step transformations.",
        "type": "fill_blank",
        "answers": [
            "Work"
        ],
        "other_options": [
            "Final",
            "Report",
            "Archive"
        ]
    },
    {
        "q": "Which approach handles schema evolution in modern data warehouses?",
        "type": "mcq",
        "o": [
            "Schema-on-read",
            "Schema-on-write only",
            "Fixed schema",
            "No schema"
        ]
    },
    {
        "q": "A data lakehouse combines the benefits of data lakes and data warehouses.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this partition pruning achieve?",
        "type": "mcq",
        "c": "SELECT SUM(revenue)\nFROM sales_fact\nWHERE sale_date BETWEEN '2024-01-01' AND '2024-03-31'\nAND region = 'North'",
        "o": [
            "Only scan partitions matching the date and region criteria",
            "Scan all partitions",
            "Create new partitions",
            "Delete old partitions"
        ]
    },
    {
        "q": "_____ partitioning distributes data evenly using a mathematical function.",
        "type": "fill_blank",
        "answers": [
            "Hash"
        ],
        "other_options": [
            "Range",
            "List",
            "Composite"
        ]
    },
    {
        "q": "Which storage format is optimized for both reads and writes in lakehouses?",
        "type": "mcq",
        "o": [
            "Delta Lake",
            "CSV",
            "Plain text",
            "XML"
        ]
    },
    {
        "q": "Match the lakehouse format with its creator:",
        "type": "match",
        "left": [
            "Delta Lake",
            "Apache Iceberg",
            "Apache Hudi",
            "ORC"
        ],
        "right": [
            "Databricks",
            "Netflix",
            "Uber",
            "Apache"
        ]
    },
    {
        "q": "Rearrange the data governance activities:",
        "type": "rearrange",
        "words": [
            "Define Policies",
            "Implement Controls",
            "Monitor Compliance",
            "Audit Results"
        ]
    },
    {
        "q": "Time travel allows querying data as it existed at a previous point in time.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the benefit of this clustering strategy?",
        "type": "mcq",
        "c": "CREATE TABLE sales_clustered\nCLUSTER BY (region, product_category)\nAS SELECT * FROM sales",
        "o": [
            "Co-locate related data for faster filtered queries",
            "Randomize data distribution",
            "Create a backup copy",
            "Delete duplicate records"
        ]
    },
    {
        "q": "Match the data processing paradigm with its characteristic:",
        "type": "match",
        "left": [
            "Batch",
            "Micro-batch",
            "Streaming",
            "Lambda"
        ],
        "right": [
            "Scheduled intervals",
            "Small frequent batches",
            "Continuous processing",
            "Batch plus stream"
        ]
    },
    {
        "q": "_____ architecture processes data through both batch and streaming paths.",
        "type": "fill_blank",
        "answers": [
            "Lambda"
        ],
        "other_options": [
            "Kappa",
            "Delta",
            "Simple"
        ]
    },
    {
        "q": "Which architecture uses only streaming for all data processing?",
        "type": "mcq",
        "o": [
            "Kappa architecture",
            "Lambda architecture",
            "Traditional ETL",
            "Batch-only architecture"
        ]
    },
    {
        "q": "Kappa architecture eliminates the batch processing layer entirely.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this error handling implement?",
        "type": "mcq",
        "c": "BEGIN TRY\n  INSERT INTO fact_table\n  SELECT * FROM staging_validated\nEND TRY\nBEGIN CATCH\n  INSERT INTO error_log (error_msg, error_date)\n  VALUES (ERROR_MESSAGE(), GETDATE())\n  RAISERROR('Load failed', 16, 1)\nEND CATCH",
        "o": [
            "Capture and log errors during data loading",
            "Silently ignore all errors",
            "Delete failed records",
            "Retry automatically"
        ]
    },
    {
        "q": "A _____ provides a standardized way to access metadata across tools.",
        "type": "fill_blank",
        "answers": [
            "catalog"
        ],
        "other_options": [
            "dictionary",
            "glossary",
            "index"
        ]
    },
    {
        "q": "Which component manages data discovery and classification?",
        "type": "mcq",
        "o": [
            "Data catalog",
            "ETL engine",
            "Report server",
            "Query optimizer"
        ]
    },
    {
        "q": "Match the metadata type with its content:",
        "type": "match",
        "left": [
            "Business",
            "Technical",
            "Operational",
            "Administrative"
        ],
        "right": [
            "Definitions and context",
            "Schema and types",
            "Run statistics",
            "Access policies"
        ]
    },
    {
        "q": "Rearrange the data classification levels from lowest to highest sensitivity:",
        "type": "rearrange",
        "words": [
            "Public",
            "Internal",
            "Confidential",
            "Restricted"
        ]
    },
    {
        "q": "Row-level security restricts data access based on user attributes.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this dynamic data masking accomplish?",
        "type": "mcq",
        "c": "SELECT \n  customer_id,\n  CASE WHEN user_role = 'admin' \n    THEN ssn \n    ELSE CONCAT('XXX-XX-', RIGHT(ssn, 4))\n  END as ssn\nFROM customers",
        "o": [
            "Show full SSN to admins, masked to others",
            "Delete all SSN values",
            "Encrypt SSN data",
            "Validate SSN format"
        ]
    },
    {
        "q": "_____ encryption protects data while it is being transferred.",
        "type": "fill_blank",
        "answers": [
            "In-transit"
        ],
        "other_options": [
            "At-rest",
            "Application",
            "Database"
        ]
    },
    {
        "q": "Which access control model uses roles to manage permissions?",
        "type": "mcq",
        "o": [
            "Role-Based Access Control (RBAC)",
            "Discretionary Access Control",
            "Mandatory Access Control",
            "Attribute-Based Access Control"
        ]
    },
    {
        "q": "Match the security measure with its protection scope:",
        "type": "match",
        "left": [
            "Encryption at rest",
            "Encryption in transit",
            "Data masking",
            "Tokenization"
        ],
        "right": [
            "Stored data",
            "Network transfer",
            "Display obfuscation",
            "Value substitution"
        ]
    },
    {
        "q": "GDPR compliance requires the ability to delete personal data on request.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the purpose of this audit logging?",
        "type": "mcq",
        "c": "CREATE TRIGGER audit_sensitive_access\nAFTER SELECT ON sensitive_table\nFOR EACH STATEMENT\nBEGIN\n  INSERT INTO access_log (user_id, table_name, access_time)\n  VALUES (CURRENT_USER, 'sensitive_table', NOW())\nEND",
        "o": [
            "Track who accesses sensitive data and when",
            "Prevent all data access",
            "Delete old records",
            "Optimize query performance"
        ]
    },
    {
        "q": "A _____ enables data retention policies based on business requirements.",
        "type": "fill_blank",
        "answers": [
            "lifecycle policy"
        ],
        "other_options": [
            "backup schedule",
            "query plan",
            "index strategy"
        ]
    },
    {
        "q": "Which regulation affects data warehousing in healthcare?",
        "type": "mcq",
        "o": [
            "HIPAA",
            "PCI-DSS only",
            "SOX only",
            "GDPR only"
        ]
    },
    {
        "q": "Rearrange the incident response steps:",
        "type": "rearrange",
        "words": [
            "Detect",
            "Contain",
            "Eradicate",
            "Recover",
            "Learn"
        ]
    },
    {
        "q": "Data anonymization removes all personally identifiable information.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What optimization does this explain plan reveal?",
        "type": "mcq",
        "c": "EXPLAIN SELECT * FROM orders\nWHERE order_date = '2024-01-15'\n\n-- Output: Index Scan using idx_order_date",
        "o": [
            "Query uses an index for efficient date lookup",
            "Full table scan is required",
            "No optimization is possible",
            "Query will create an index"
        ]
    },
    {
        "q": "Match the compliance requirement with its industry:",
        "type": "match",
        "left": [
            "SOX",
            "HIPAA",
            "PCI-DSS",
            "GDPR"
        ],
        "right": [
            "Finance",
            "Healthcare",
            "Payment cards",
            "EU personal data"
        ]
    },
    {
        "q": "_____ converts sensitive data to a non-sensitive placeholder value.",
        "type": "fill_blank",
        "answers": [
            "Tokenization"
        ],
        "other_options": [
            "Encryption",
            "Hashing",
            "Compression"
        ]
    },
    {
        "q": "Which pattern ensures data consistency during warehouse updates?",
        "type": "mcq",
        "o": [
            "Publish-subscribe with transactions",
            "Fire and forget",
            "Best effort delivery",
            "No consistency guarantee"
        ]
    },
    {
        "q": "Change Data Capture can use log-based or trigger-based approaches.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this data reconciliation query verify?",
        "type": "mcq",
        "c": "SELECT 'Source' as system, COUNT(*) as cnt, SUM(amount) as total\nFROM source_orders\nWHERE order_date = '2024-01-15'\nUNION ALL\nSELECT 'Target', COUNT(*), SUM(amount)\nFROM dw_orders\nWHERE order_date = '2024-01-15'",
        "o": [
            "Compare record counts and totals between source and warehouse",
            "Delete mismatched records",
            "Create new orders",
            "Update order amounts"
        ]
    },
    {
        "q": "A _____ join returns all records from both tables regardless of matches.",
        "type": "fill_blank",
        "answers": [
            "full outer"
        ],
        "other_options": [
            "inner",
            "left",
            "cross"
        ]
    },
    {
        "q": "Which join type is most efficient for large tables with indexes?",
        "type": "mcq",
        "o": [
            "Hash join",
            "Nested loop for all cases",
            "Cartesian product",
            "No join optimization exists"
        ]
    },
    {
        "q": "Match the join algorithm with its best use case:",
        "type": "match",
        "left": [
            "Nested loop",
            "Hash join",
            "Merge join",
            "Index join"
        ],
        "right": [
            "Small tables",
            "Large unsorted tables",
            "Pre-sorted data",
            "Indexed lookup"
        ]
    },
    {
        "q": "Rearrange the query execution order:",
        "type": "rearrange",
        "words": [
            "FROM",
            "WHERE",
            "GROUP BY",
            "HAVING",
            "SELECT",
            "ORDER BY"
        ]
    },
    {
        "q": "Common Table Expressions (CTEs) can improve query readability and reusability.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this recursive CTE calculate?",
        "type": "mcq",
        "c": "WITH RECURSIVE org_hierarchy AS (\n  SELECT emp_id, manager_id, 1 as level\n  FROM employees WHERE manager_id IS NULL\n  UNION ALL\n  SELECT e.emp_id, e.manager_id, h.level + 1\n  FROM employees e\n  JOIN org_hierarchy h ON e.manager_id = h.emp_id\n)\nSELECT * FROM org_hierarchy",
        "o": [
            "Build organizational hierarchy with levels",
            "Calculate employee salaries",
            "Delete manager records",
            "Update employee names"
        ]
    },
    {
        "q": "A _____ provides an isolation layer between raw and presentation data.",
        "type": "fill_blank",
        "answers": [
            "semantic layer"
        ],
        "other_options": [
            "network layer",
            "storage layer",
            "backup layer"
        ]
    },
    {
        "q": "Which approach allows business users to define metrics without SQL?",
        "type": "mcq",
        "o": [
            "Semantic layer with metrics definitions",
            "Direct SQL access only",
            "Command line interfaces",
            "Manual calculations"
        ]
    },
    {
        "q": "Match the analytics type with its question:",
        "type": "match",
        "left": [
            "Descriptive",
            "Diagnostic",
            "Predictive",
            "Prescriptive"
        ],
        "right": [
            "What happened",
            "Why did it happen",
            "What will happen",
            "What should we do"
        ]
    },
    {
        "q": "Self-service BI allows business users to create their own reports.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the benefit of this view abstraction?",
        "type": "mcq",
        "c": "CREATE VIEW sales_summary AS\nSELECT region, product_category,\n  SUM(revenue) as total_revenue,\n  COUNT(DISTINCT customer_id) as unique_customers\nFROM fact_sales f\nJOIN dim_product p ON f.product_key = p.product_key\nJOIN dim_region r ON f.region_key = r.region_key\nGROUP BY region, product_category",
        "o": [
            "Simplify complex joins for end users",
            "Delete underlying data",
            "Create physical copies",
            "Prevent all queries"
        ]
    },
    {
        "q": "Match the BI tool category with its primary function:",
        "type": "match",
        "left": [
            "Reporting",
            "Dashboards",
            "Ad-hoc query",
            "Mobile BI"
        ],
        "right": [
            "Scheduled documents",
            "Real-time visuals",
            "Exploratory analysis",
            "On-the-go access"
        ]
    },
    {
        "q": "_____ analytics focuses on understanding the causes behind observed patterns.",
        "type": "fill_blank",
        "answers": [
            "Diagnostic"
        ],
        "other_options": [
            "Descriptive",
            "Predictive",
            "Prescriptive"
        ]
    },
    {
        "q": "Which visualization type is best for showing trends over time?",
        "type": "mcq",
        "o": [
            "Line chart",
            "Pie chart",
            "Bar chart",
            "Scatter plot"
        ]
    },
    {
        "q": "Rearrange the BI implementation phases:",
        "type": "rearrange",
        "words": [
            "Requirements",
            "Data Modeling",
            "ETL Development",
            "Report Design",
            "Deployment"
        ]
    },
    {
        "q": "Drill-through allows users to navigate from summary to detail data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this calculation measure?",
        "type": "mcq",
        "c": "SELECT \n  (current_month_sales - prev_month_sales) / prev_month_sales * 100 \n  as month_over_month_growth\nFROM sales_metrics",
        "o": [
            "Percentage change between consecutive months",
            "Total annual sales",
            "Average daily revenue",
            "Customer count"
        ]
    },
    {
        "q": "A _____ table allows data to exist temporarily outside normal dimension tables.",
        "type": "fill_blank",
        "answers": [
            "mini-dimension"
        ],
        "other_options": [
            "mega-dimension",
            "super-dimension",
            "master-dimension"
        ]
    },
    {
        "q": "Which technique handles rapidly changing dimension attributes?",
        "type": "mcq",
        "o": [
            "Mini-dimension",
            "Junk dimension",
            "Degenerate dimension",
            "Conformed dimension"
        ]
    },
    {
        "q": "Match the dimension type with its handling approach:",
        "type": "match",
        "left": [
            "Rapidly changing",
            "Low cardinality flags",
            "Transaction identifiers",
            "Shared across marts"
        ],
        "right": [
            "Mini-dimension",
            "Junk dimension",
            "Degenerate dimension",
            "Conformed dimension"
        ]
    },
    {
        "q": "Outrigger dimensions are secondary dimensions connected to other dimensions.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this factless fact table capture?",
        "type": "mcq",
        "c": "CREATE TABLE fact_student_attendance (\n  date_key INT,\n  student_key INT,\n  class_key INT,\n  attendance_flag BOOLEAN\n)",
        "o": [
            "Events or coverage without numeric measures",
            "Financial transactions",
            "Inventory quantities",
            "Sales revenue"
        ]
    },
    {
        "q": "_____ fact tables record events that happen at a point in time.",
        "type": "fill_blank",
        "answers": [
            "Transaction"
        ],
        "other_options": [
            "Periodic",
            "Accumulating",
            "Aggregated"
        ]
    },
    {
        "q": "Which fact table type tracks the lifecycle of a business process?",
        "type": "mcq",
        "o": [
            "Accumulating snapshot",
            "Transaction fact",
            "Periodic snapshot",
            "Factless fact"
        ]
    },
    {
        "q": "Match the fact table type with its grain:",
        "type": "match",
        "left": [
            "Transaction",
            "Periodic snapshot",
            "Accumulating snapshot",
            "Factless"
        ],
        "right": [
            "One row per event",
            "One row per period",
            "One row per process lifecycle",
            "One row per relationship"
        ]
    },
    {
        "q": "Rearrange the accumulating snapshot milestones for order processing:",
        "type": "rearrange",
        "words": [
            "Order Placed",
            "Order Shipped",
            "Order Delivered",
            "Payment Received"
        ]
    },
    {
        "q": "Aggregate navigation automatically routes queries to pre-computed summaries.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this complex SCD Type 2 implementation handle?",
        "type": "mcq",
        "c": "MERGE INTO dim_customer t\nUSING staging_customer s ON t.customer_id = s.customer_id AND t.is_current = 1\nWHEN MATCHED AND (t.address != s.address OR t.phone != s.phone) THEN\n  UPDATE SET t.is_current = 0, t.end_date = CURRENT_DATE\nWHEN NOT MATCHED THEN\n  INSERT (customer_id, name, address, phone, is_current, start_date)\n  VALUES (s.customer_id, s.name, s.address, s.phone, 1, CURRENT_DATE)",
        "o": [
            "Expire current records and insert new versions for changed attributes",
            "Delete all customer records",
            "Create backup copies of customers",
            "Update only the primary key"
        ]
    },
    {
        "q": "A _____ dimension stores multiple versions of slowly changing attributes in separate columns.",
        "type": "fill_blank",
        "answers": [
            "Type 3 SCD"
        ],
        "other_options": [
            "Type 1 SCD",
            "Type 2 SCD",
            "Type 0 SCD"
        ]
    },
    {
        "q": "Which strategy minimizes storage for rapidly changing dimensions?",
        "type": "mcq",
        "o": [
            "Mini-dimensions with separate fact table references",
            "Full SCD Type 2 for all attributes",
            "Daily full snapshots",
            "Delete and recreate approach"
        ]
    },
    {
        "q": "Match the advanced dimension pattern with its use case:",
        "type": "match",
        "left": [
            "Multi-valued dimension",
            "Heterogeneous product",
            "Swappable dimension",
            "Late arriving dimension"
        ],
        "right": [
            "Many-to-many relationships",
            "Different attribute sets",
            "Interchangeable dimensions",
            "Dimension loads after facts"
        ]
    },
    {
        "q": "Type 6 SCD combines Type 1, 2, and 3 approaches in a single implementation.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What optimization does this query benefit from?",
        "type": "mcq",
        "c": "SELECT /*+ BROADCAST(dim_product) */\n  f.order_id, p.product_name, f.quantity\nFROM fact_orders f\nJOIN dim_product p ON f.product_key = p.product_key\nWHERE f.order_date = '2024-01-15'",
        "o": [
            "Broadcast small dimension to all nodes for efficient join",
            "Sequential scan of all tables",
            "Index creation during query",
            "Delayed join execution"
        ]
    },
    {
        "q": "_____ optimization pushes join predicates closer to the data source.",
        "type": "fill_blank",
        "answers": [
            "Predicate pushdown"
        ],
        "other_options": [
            "Late binding",
            "Lazy evaluation",
            "Deferred loading"
        ]
    },
    {
        "q": "Which technique addresses data skew in distributed processing?",
        "type": "mcq",
        "o": [
            "Salting keys to distribute data evenly",
            "Processing all data on a single node",
            "Ignoring skewed partitions",
            "Deleting skewed records"
        ]
    },
    {
        "q": "Rearrange the advanced ETL optimization techniques by impact:",
        "type": "rearrange",
        "words": [
            "Partition Pruning",
            "Index Selection",
            "Join Optimization",
            "Parallel Execution"
        ]
    },
    {
        "q": "Adaptive query execution can modify execution plans during runtime.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this data quality rule enforce?",
        "type": "mcq",
        "c": "ALTER TABLE fact_orders\nADD CONSTRAINT chk_quantity CHECK (quantity > 0 AND quantity <= 10000)\nADD CONSTRAINT chk_amount CHECK (amount >= 0 AND amount <= 1000000)",
        "o": [
            "Range validation for business rule compliance",
            "Delete invalid records",
            "Create backup copies",
            "Generate reports"
        ]
    },
    {
        "q": "Match the data quality metric with its formula:",
        "type": "match",
        "left": [
            "Completeness",
            "Uniqueness",
            "Accuracy",
            "Freshness"
        ],
        "right": [
            "Non-null / Total records",
            "Distinct / Total records",
            "Correct / Total records",
            "Current time - Last update"
        ]
    },
    {
        "q": "_____ testing validates data transformations by comparing input to output.",
        "type": "fill_blank",
        "answers": [
            "Mapping"
        ],
        "other_options": [
            "Unit",
            "Integration",
            "Performance"
        ]
    },
    {
        "q": "Which approach ensures referential integrity in dimensional models?",
        "type": "mcq",
        "o": [
            "Foreign key constraints between facts and dimensions",
            "No constraints for flexibility",
            "Application-level checks only",
            "Manual verification"
        ]
    },
    {
        "q": "Data profiling should be performed only once during initial development.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What does this late-arriving data handling implement?",
        "type": "mcq",
        "c": "INSERT INTO fact_orders (order_key, date_key, product_key, amount)\nSELECT order_key,\n  COALESCE(d.date_key, -1),\n  COALESCE(p.product_key, -1),\n  amount\nFROM staging_orders s\nLEFT JOIN dim_date d ON s.order_date = d.full_date\nLEFT JOIN dim_product p ON s.product_id = p.product_id",
        "o": [
            "Handle missing dimension keys with unknown member placeholder",
            "Delete orphan records",
            "Create new dimensions",
            "Skip invalid records"
        ]
    },
    {
        "q": "An _____ member is a placeholder for unmatched dimension references.",
        "type": "fill_blank",
        "answers": [
            "unknown"
        ],
        "other_options": [
            "null",
            "empty",
            "default"
        ]
    },
    {
        "q": "Which strategy handles fact records arriving before their dimension data?",
        "type": "mcq",
        "o": [
            "Inferred dimension members",
            "Reject all early facts",
            "Delete dimension requirements",
            "Ignore referential integrity"
        ]
    },
    {
        "q": "Match the late-arriving scenario with its handling approach:",
        "type": "match",
        "left": [
            "Late dimension",
            "Late fact",
            "Missing dimension key",
            "Out-of-order events"
        ],
        "right": [
            "Inferred member then update",
            "Correct backfill",
            "Unknown member placeholder",
            "Event time vs processing time"
        ]
    },
    {
        "q": "Rearrange the data pipeline reliability techniques by criticality:",
        "type": "rearrange",
        "words": [
            "Idempotency",
            "Retry Logic",
            "Dead Letter Queue",
            "Monitoring"
        ]
    },
    {
        "q": "Idempotent ETL jobs produce the same result regardless of execution count.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this checkpoint mechanism provide?",
        "type": "mcq",
        "c": "BEGIN TRANSACTION\n  UPDATE etl_checkpoint SET last_processed = @current_batch WHERE job_name = 'sales_load'\n  INSERT INTO fact_sales SELECT * FROM staging_sales WHERE batch_id = @current_batch\nCOMMIT TRANSACTION",
        "o": [
            "Atomic updates with recovery point for restartability",
            "Delete processed records",
            "Create backup copies",
            "Generate execution logs"
        ]
    },
    {
        "q": "_____ ensures exactly-once processing in distributed systems.",
        "type": "fill_blank",
        "answers": [
            "Deduplication"
        ],
        "other_options": [
            "Replication",
            "Compression",
            "Encryption"
        ]
    },
    {
        "q": "Which pattern prevents duplicate processing in ETL restarts?",
        "type": "mcq",
        "o": [
            "High-water mark with idempotent operations",
            "Always full reload",
            "No recovery mechanism",
            "Manual intervention only"
        ]
    },
    {
        "q": "Match the reliability pattern with its implementation:",
        "type": "match",
        "left": [
            "Checkpointing",
            "Dead letter queue",
            "Circuit breaker",
            "Retry with backoff"
        ],
        "right": [
            "Save progress for restart",
            "Capture failed records",
            "Stop on repeated failures",
            "Gradual retry delays"
        ]
    },
    {
        "q": "Eventual consistency is acceptable for analytical workloads in most cases.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this slowly changing dimension Type 4 implement?",
        "type": "mcq",
        "c": "-- Main dimension table (current only)\nCREATE TABLE dim_customer_current AS SELECT * FROM dim_customer WHERE is_current = 1\n-- History table (all versions)\nCREATE TABLE dim_customer_history AS SELECT * FROM dim_customer",
        "o": [
            "Separate current and historical data into different tables",
            "Delete all historical records",
            "Merge current and historical data",
            "Create temporary backup"
        ]
    },
    {
        "q": "Type _____ SCD uses a mini-dimension for rapidly changing attributes.",
        "type": "fill_blank",
        "answers": [
            "5"
        ],
        "other_options": [
            "1",
            "2",
            "3"
        ]
    },
    {
        "q": "Which SCD type is best for balancing query performance and history?",
        "type": "mcq",
        "o": [
            "Type 4 with separate history table",
            "Type 0 ignoring all changes",
            "Type 1 overwriting everything",
            "No SCD tracking"
        ]
    },
    {
        "q": "Rearrange the SCD types by historical tracking completeness:",
        "type": "rearrange",
        "words": [
            "Type 0",
            "Type 1",
            "Type 3",
            "Type 2",
            "Type 6"
        ]
    },
    {
        "q": "Type 7 SCD provides both current and historical views through dual keys.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What optimization does this columnar storage query leverage?",
        "type": "mcq",
        "c": "SELECT customer_id, SUM(amount) as total\nFROM fact_sales\nWHERE region = 'North' AND year = 2024\nGROUP BY customer_id",
        "o": [
            "Read only required columns instead of full rows",
            "Scan all columns including unused ones",
            "Convert to row-based storage",
            "Create temporary row copies"
        ]
    },
    {
        "q": "_____ encoding reduces storage by replacing repeated values with references.",
        "type": "fill_blank",
        "answers": [
            "Dictionary"
        ],
        "other_options": [
            "Run-length",
            "Delta",
            "Bitmap"
        ]
    },
    {
        "q": "Which encoding is most effective for sorted numeric sequences?",
        "type": "mcq",
        "o": [
            "Delta encoding",
            "Dictionary encoding",
            "Plain encoding",
            "Random encoding"
        ]
    },
    {
        "q": "Match the compression technique with its ideal data pattern:",
        "type": "match",
        "left": [
            "Run-length",
            "Dictionary",
            "Delta",
            "Bitmap"
        ],
        "right": [
            "Consecutive repeated values",
            "Low cardinality strings",
            "Sorted numeric sequences",
            "Boolean flags"
        ]
    },
    {
        "q": "Zone maps store min/max metadata for data skipping during queries.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this query demonstrate about vectorized execution?",
        "type": "mcq",
        "c": "-- Vectorized engine processes batches of 1024 rows\nSELECT SUM(quantity * price) as total_revenue\nFROM sales\nWHERE sale_date >= '2024-01-01'",
        "o": [
            "Process multiple rows in CPU cache-efficient batches",
            "Process one row at a time",
            "Store results in vectors",
            "Create vector indexes"
        ]
    },
    {
        "q": "_____ execution processes columnar data in CPU register-sized batches.",
        "type": "fill_blank",
        "answers": [
            "Vectorized"
        ],
        "other_options": [
            "Scalar",
            "Sequential",
            "Random"
        ]
    },
    {
        "q": "Which execution model is most efficient for analytical queries?",
        "type": "mcq",
        "o": [
            "Vectorized columnar execution",
            "Row-by-row processing",
            "Single-threaded scalar",
            "No optimization"
        ]
    },
    {
        "q": "Match the execution optimization with its benefit:",
        "type": "match",
        "left": [
            "Vectorization",
            "SIMD",
            "Pipeline parallelism",
            "Cache optimization"
        ],
        "right": [
            "Batch row processing",
            "Single instruction multiple data",
            "Concurrent operators",
            "Reduced memory access"
        ]
    },
    {
        "q": "Rearrange the query execution stages:",
        "type": "rearrange",
        "words": [
            "Parse",
            "Bind",
            "Optimize",
            "Execute",
            "Return Results"
        ]
    },
    {
        "q": "Code generation compiles queries to native machine code for faster execution.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this workload management configuration achieve?",
        "type": "mcq",
        "c": "CREATE RESOURCE POOL analytics_pool\nWITH (max_memory_percent = 40, max_cpu_percent = 30)\nCREATE WORKLOAD GROUP analytics_group\nUSING analytics_pool",
        "o": [
            "Isolate query resources to prevent workload interference",
            "Delete unused resources",
            "Create backup pools",
            "Disable resource limits"
        ]
    },
    {
        "q": "_____ isolation ensures different workloads do not compete for resources.",
        "type": "fill_blank",
        "answers": [
            "Workload"
        ],
        "other_options": [
            "Data",
            "Network",
            "Storage"
        ]
    },
    {
        "q": "Which technique prevents a single query from consuming all resources?",
        "type": "mcq",
        "o": [
            "Query timeouts and resource limits",
            "Unlimited query execution",
            "No resource governance",
            "First-come-first-served only"
        ]
    },
    {
        "q": "Match the workload type with its priority:",
        "type": "match",
        "left": [
            "Executive dashboards",
            "Ad-hoc analysis",
            "Batch reporting",
            "Data science exploration"
        ],
        "right": [
            "High priority",
            "Medium priority",
            "Low priority",
            "Best effort"
        ]
    },
    {
        "q": "Auto-scaling adjusts warehouse resources based on workload demand.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this cost analysis query calculate?",
        "type": "mcq",
        "c": "SELECT query_type, \n  COUNT(*) as query_count,\n  SUM(bytes_scanned / 1e12) as tb_scanned,\n  SUM(bytes_scanned / 1e12) * cost_per_tb as total_cost\nFROM query_history\nGROUP BY query_type\nORDER BY total_cost DESC",
        "o": [
            "Query costs by type based on data scanned",
            "Total storage costs",
            "User session costs",
            "Network transfer fees"
        ]
    },
    {
        "q": "_____ queries benefit most from result caching in cloud warehouses.",
        "type": "fill_blank",
        "answers": [
            "Repeated"
        ],
        "other_options": [
            "Random",
            "Unique",
            "Rare"
        ]
    },
    {
        "q": "Which strategy reduces cloud data warehouse costs?",
        "type": "mcq",
        "o": [
            "Partition pruning and clustering",
            "Scanning full tables always",
            "Disabling compression",
            "Avoiding indexes"
        ]
    },
    {
        "q": "Match the cost optimization technique with its savings area:",
        "type": "match",
        "left": [
            "Auto-suspend",
            "Clustering",
            "Compression",
            "Result caching"
        ],
        "right": [
            "Compute costs",
            "Query efficiency",
            "Storage costs",
            "Repeated query costs"
        ]
    },
    {
        "q": "Rearrange the cloud warehouse cost components by typical magnitude:",
        "type": "rearrange",
        "words": [
            "Compute",
            "Storage",
            "Data Transfer",
            "Operations"
        ]
    },
    {
        "q": "Reserved capacity provides cost savings for predictable workloads.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this schema evolution handle?",
        "type": "mcq",
        "c": "ALTER TABLE fact_orders\nADD COLUMN discount_amount DECIMAL(10,2) DEFAULT 0\nADD COLUMN promo_code VARCHAR(20)",
        "o": [
            "Add new columns without breaking existing queries",
            "Delete all existing data",
            "Recreate the entire table",
            "Remove all constraints"
        ]
    },
    {
        "q": "_____ on read allows querying data without predefined schema.",
        "type": "fill_blank",
        "answers": [
            "Schema"
        ],
        "other_options": [
            "View",
            "Table",
            "Index"
        ]
    },
    {
        "q": "Which format best supports schema evolution in data lakes?",
        "type": "mcq",
        "o": [
            "Parquet with schema merging",
            "Fixed-width text files",
            "Static CSV schemas",
            "Binary formats without metadata"
        ]
    },
    {
        "q": "Match the schema change type with its handling approach:",
        "type": "match",
        "left": [
            "Add column",
            "Rename column",
            "Change data type",
            "Delete column"
        ],
        "right": [
            "Default value for existing",
            "Alias mapping",
            "Conversion function",
            "Soft delete or ignore"
        ]
    },
    {
        "q": "Backward compatible changes allow old queries to work with new schemas.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this data contract define?",
        "type": "mcq",
        "c": "{\n  \"table\": \"orders\",\n  \"columns\": [\n    {\"name\": \"order_id\", \"type\": \"BIGINT\", \"nullable\": false},\n    {\"name\": \"amount\", \"type\": \"DECIMAL(10,2)\", \"nullable\": false}\n  ],\n  \"sla\": {\"freshness\": \"1 hour\", \"completeness\": \"99.9%\"}\n}",
        "o": [
            "Schema and quality expectations between producer and consumer",
            "Query execution plan",
            "Data backup schedule",
            "User access permissions"
        ]
    },
    {
        "q": "A data _____ specifies the expected schema and quality of shared data.",
        "type": "fill_blank",
        "answers": [
            "contract"
        ],
        "other_options": [
            "catalog",
            "dictionary",
            "glossary"
        ]
    },
    {
        "q": "Which approach ensures data producers meet consumer expectations?",
        "type": "mcq",
        "o": [
            "Data contracts with automated validation",
            "No formal agreements",
            "Manual inspection only",
            "Hope for the best"
        ]
    },
    {
        "q": "Match the data mesh principle with its description:",
        "type": "match",
        "left": [
            "Domain ownership",
            "Data as product",
            "Self-serve platform",
            "Federated governance"
        ],
        "right": [
            "Teams own their data",
            "Treat data like a product",
            "Easy-to-use infrastructure",
            "Decentralized standards"
        ]
    },
    {
        "q": "Rearrange the data mesh implementation phases:",
        "type": "rearrange",
        "words": [
            "Identify Domains",
            "Define Data Products",
            "Build Platform",
            "Establish Governance"
        ]
    },
    {
        "q": "Data mesh decentralizes data ownership to domain teams.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this data product definition include?",
        "type": "mcq",
        "c": "DATA PRODUCT orders_domain.customer_orders\n  DESCRIPTION 'Customer order history'\n  SCHEMA orders_schema\n  QUALITY_CHECKS [\n    completeness >= 99%,\n    freshness <= 1 hour\n  ]\n  OWNER 'orders-team@company.com'",
        "o": [
            "Self-describing dataset with quality guarantees and ownership",
            "Simple data table definition",
            "Query optimization hint",
            "Backup configuration"
        ]
    },
    {
        "q": "A data _____ is a self-contained, quality-assured dataset with clear ownership.",
        "type": "fill_blank",
        "answers": [
            "product"
        ],
        "other_options": [
            "table",
            "view",
            "schema"
        ]
    },
    {
        "q": "Which characteristic distinguishes a data product from a raw table?",
        "type": "mcq",
        "o": [
            "Discoverable metadata, quality guarantees, and clear ownership",
            "Same as any database table",
            "No documentation needed",
            "Accessible only to data engineers"
        ]
    },
    {
        "q": "Match the data product interface with its purpose:",
        "type": "match",
        "left": [
            "Output ports",
            "Input ports",
            "Discovery API",
            "Quality metrics"
        ],
        "right": [
            "Data consumption",
            "Data ingestion",
            "Finding datasets",
            "Monitoring health"
        ]
    },
    {
        "q": "Data products should be version-controlled like software.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this observability setup monitor?",
        "type": "mcq",
        "c": "CREATE ALERT stale_data_alert\nWHEN (SELECT MAX(load_timestamp) FROM fact_orders) < DATEADD(hour, -2, CURRENT_TIMESTAMP)\nTHEN NOTIFY 'data-ops@company.com'",
        "o": [
            "Data freshness SLA violation",
            "Query performance issues",
            "Storage capacity limits",
            "User access problems"
        ]
    },
    {
        "q": "_____ observability includes data quality, freshness, and volume monitoring.",
        "type": "fill_blank",
        "answers": [
            "Data"
        ],
        "other_options": [
            "System",
            "Network",
            "Application"
        ]
    },
    {
        "q": "Which metric indicates data pipeline health?",
        "type": "mcq",
        "o": [
            "Record count variance from expected baseline",
            "User login count",
            "Server CPU temperature",
            "Network packet loss"
        ]
    },
    {
        "q": "Match the observability component with its focus:",
        "type": "match",
        "left": [
            "Metrics",
            "Logs",
            "Traces",
            "Alerts"
        ],
        "right": [
            "Numeric measurements",
            "Event records",
            "Request flow",
            "Anomaly notification"
        ]
    },
    {
        "q": "Rearrange the incident response severity levels:",
        "type": "rearrange",
        "words": [
            "Critical",
            "High",
            "Medium",
            "Low",
            "Informational"
        ]
    },
    {
        "q": "Anomaly detection can automatically identify unexpected data patterns.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this lineage tracking capture?",
        "type": "mcq",
        "c": "-- Lineage metadata\n{\n  \"target\": \"fact_revenue\",\n  \"sources\": [\"stg_orders\", \"stg_payments\"],\n  \"transformations\": [\"join\", \"aggregate\", \"filter\"],\n  \"last_updated\": \"2024-01-15T10:30:00Z\"\n}",
        "o": [
            "Data flow from sources through transformations to target",
            "User access history",
            "Query execution plan",
            "Storage location"
        ]
    },
    {
        "q": "Column-level _____ tracks which source columns contribute to each target column.",
        "type": "fill_blank",
        "answers": [
            "lineage"
        ],
        "other_options": [
            "access",
            "security",
            "validation"
        ]
    },
    {
        "q": "Which analysis benefits most from data lineage?",
        "type": "mcq",
        "o": [
            "Impact analysis when changing source systems",
            "Query performance tuning",
            "Storage optimization",
            "User management"
        ]
    },
    {
        "q": "Match the lineage use case with its benefit:",
        "type": "match",
        "left": [
            "Impact analysis",
            "Root cause",
            "Compliance",
            "Trust"
        ],
        "right": [
            "Change assessment",
            "Error tracing",
            "Audit requirements",
            "Data credibility"
        ]
    },
    {
        "q": "Automated lineage extraction parses code to capture data flows.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this semantic layer definition provide?",
        "type": "mcq",
        "c": "METRIC revenue\n  EXPRESSION SUM(fact_orders.amount)\n  DIMENSIONS [date, product, region]\n  FILTERS [status = 'completed']\n  DESCRIPTION 'Total completed order revenue'",
        "o": [
            "Consistent business metric definition across all tools",
            "Physical table structure",
            "ETL job configuration",
            "User access rules"
        ]
    },
    {
        "q": "A _____ layer translates business concepts to technical data structures.",
        "type": "fill_blank",
        "answers": [
            "semantic"
        ],
        "other_options": [
            "physical",
            "network",
            "storage"
        ]
    },
    {
        "q": "Which benefit does a semantic layer provide?",
        "type": "mcq",
        "o": [
            "Consistent metric definitions across all BI tools",
            "Faster disk access",
            "Reduced storage costs",
            "Simplified backup"
        ]
    },
    {
        "q": "Match the semantic layer component with its function:",
        "type": "match",
        "left": [
            "Metrics",
            "Dimensions",
            "Entities",
            "Relationships"
        ],
        "right": [
            "Aggregatable measures",
            "Analysis categories",
            "Business objects",
            "Entity connections"
        ]
    },
    {
        "q": "Rearrange the semantic layer adoption stages:",
        "type": "rearrange",
        "words": [
            "Define Metrics",
            "Map to Physical",
            "Expose API",
            "Integrate Tools"
        ]
    },
    {
        "q": "Headless BI separates the semantic layer from visualization tools.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this reverse ETL accomplish?",
        "type": "mcq",
        "c": "INSERT INTO salesforce.accounts (account_id, health_score, churn_risk)\nSELECT customer_id, \n  calculate_health_score(metrics),\n  predict_churn_probability(features)\nFROM warehouse.customer_360",
        "o": [
            "Push warehouse insights back to operational systems",
            "Extract data from Salesforce",
            "Delete warehouse data",
            "Create backup copies"
        ]
    },
    {
        "q": "_____ ETL moves data from the warehouse to operational applications.",
        "type": "fill_blank",
        "answers": [
            "Reverse"
        ],
        "other_options": [
            "Forward",
            "Traditional",
            "Standard"
        ]
    },
    {
        "q": "Which use case benefits most from reverse ETL?",
        "type": "mcq",
        "o": [
            "Activating ML predictions in CRM systems",
            "Loading raw data into the warehouse",
            "Creating data backups",
            "Generating static reports"
        ]
    },
    {
        "q": "Match the reverse ETL destination with its use case:",
        "type": "match",
        "left": [
            "CRM",
            "Marketing automation",
            "Product analytics",
            "Customer support"
        ],
        "right": [
            "Customer health scores",
            "Audience segments",
            "Feature usage data",
            "Ticket prioritization"
        ]
    },
    {
        "q": "Operational analytics requires low-latency data access.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What architecture does this implementation represent?",
        "type": "mcq",
        "c": "-- Medallion Architecture\nCREATE SCHEMA bronze  -- Raw data landing\nCREATE SCHEMA silver  -- Cleansed and conformed\nCREATE SCHEMA gold    -- Business-ready aggregates",
        "o": [
            "Multi-layer data lakehouse with progressive refinement",
            "Single flat data store",
            "Traditional star schema",
            "Normalized OLTP database"
        ]
    },
    {
        "q": "The _____ layer in medallion architecture contains raw, unprocessed data.",
        "type": "fill_blank",
        "answers": [
            "bronze"
        ],
        "other_options": [
            "silver",
            "gold",
            "platinum"
        ]
    },
    {
        "q": "Which medallion layer is optimized for business reporting?",
        "type": "mcq",
        "o": [
            "Gold layer",
            "Bronze layer",
            "Platinum layer",
            "All layers equally"
        ]
    },
    {
        "q": "Match the medallion layer with its data characteristics:",
        "type": "match",
        "left": [
            "Bronze",
            "Silver",
            "Gold",
            "Platinum"
        ],
        "right": [
            "Raw ingested data",
            "Cleaned and validated",
            "Business aggregates",
            "ML-ready features"
        ]
    },
    {
        "q": "Rearrange the data lakehouse query pattern by data freshness:",
        "type": "rearrange",
        "words": [
            "Streaming Inserts",
            "Micro-batch",
            "Hourly Batch",
            "Daily Batch"
        ]
    },
    {
        "q": "Unity Catalog provides unified governance for data and AI assets.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this advanced partitioning strategy implement?",
        "type": "mcq",
        "c": "CREATE TABLE sales_fact (\n  sale_date DATE,\n  region VARCHAR(50),\n  amount DECIMAL(10,2)\n)\nPARTITION BY RANGE (sale_date)\nSUBPARTITION BY LIST (region)\n(\n  PARTITION p2024 VALUES LESS THAN ('2025-01-01')\n    (SUBPARTITION p2024_north VALUES ('North'),\n     SUBPARTITION p2024_south VALUES ('South'))\n)",
        "o": [
            "Composite partitioning with range on date and list on region",
            "Simple range partitioning only",
            "Hash partitioning",
            "No partitioning strategy"
        ]
    },
    {
        "q": "_____ partitioning combines multiple partitioning methods for optimal data organization.",
        "type": "fill_blank",
        "answers": [
            "Composite"
        ],
        "other_options": [
            "Simple",
            "Linear",
            "Random"
        ]
    },
    {
        "q": "Which partitioning strategy is best for time-series data with regional analysis?",
        "type": "mcq",
        "o": [
            "Range partitioning by date with list subpartitioning by region",
            "Random distribution",
            "No partitioning",
            "Single partition for all data"
        ]
    },
    {
        "q": "Match the partitioning strategy with its optimization goal:",
        "type": "match",
        "left": [
            "Range",
            "Hash",
            "List",
            "Composite"
        ],
        "right": [
            "Time-based queries",
            "Even distribution",
            "Category filtering",
            "Multi-dimensional access"
        ]
    },
    {
        "q": "Rearrange the partition maintenance tasks by frequency:",
        "type": "rearrange",
        "words": [
            "Add New Partition",
            "Archive Old Partition",
            "Merge Partitions",
            "Rebuild Statistics"
        ]
    },
    {
        "q": "Automatic partition management reduces operational overhead in cloud warehouses.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this advanced indexing strategy enable?",
        "type": "mcq",
        "c": "CREATE INDEX idx_sales_covering ON sales (\n  customer_id,\n  sale_date\n) INCLUDE (\n  amount,\n  quantity,\n  product_id\n)",
        "o": [
            "Index-only scans by including all needed columns",
            "Simple lookup index",
            "Full table scan optimization",
            "Clustered storage"
        ]
    },
    {
        "q": "A _____ index includes non-key columns to satisfy queries without table access.",
        "type": "fill_blank",
        "answers": [
            "covering"
        ],
        "other_options": [
            "partial",
            "filtered",
            "composite"
        ]
    },
    {
        "q": "Which index type is most effective for high-cardinality equality lookups?",
        "type": "mcq",
        "o": [
            "Hash index",
            "Bitmap index",
            "Full-text index",
            "Spatial index"
        ]
    },
    {
        "q": "Match the index optimization with its benefit:",
        "type": "match",
        "left": [
            "Covering index",
            "Filtered index",
            "Partial index",
            "Columnstore index"
        ],
        "right": [
            "Avoid table lookup",
            "Subset of rows",
            "Specific conditions",
            "Analytical queries"
        ]
    },
    {
        "q": "Index maintenance overhead increases with the number of indexes on a table.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this query demonstrate about predicate ordering?",
        "type": "mcq",
        "c": "-- Optimizer reorders predicates for efficiency\nSELECT * FROM large_table\nWHERE expensive_function(col1) = 'result'  -- Called last\nAND simple_equality = 'value'              -- Called first\nAND col2 BETWEEN 1 AND 100                 -- Called second",
        "o": [
            "Optimizer evaluates cheap predicates before expensive ones",
            "Predicates are always evaluated in written order",
            "All predicates are evaluated simultaneously",
            "Expensive predicates are always first"
        ]
    },
    {
        "q": "_____ elimination removes unnecessary computations from query plans.",
        "type": "fill_blank",
        "answers": [
            "Predicate"
        ],
        "other_options": [
            "Table",
            "Index",
            "Column"
        ]
    },
    {
        "q": "Which optimization avoids processing data that cannot satisfy query conditions?",
        "type": "mcq",
        "o": [
            "Early filtering with predicate pushdown",
            "Late filtering after all joins",
            "No filtering optimization",
            "Processing all data always"
        ]
    },
    {
        "q": "Match the query optimization technique with its stage:",
        "type": "match",
        "left": [
            "Predicate pushdown",
            "Join reordering",
            "Subquery unnesting",
            "Constant folding"
        ],
        "right": [
            "Filter early",
            "Optimal join sequence",
            "Flatten nested queries",
            "Compile-time evaluation"
        ]
    },
    {
        "q": "Rearrange the query optimization phases:",
        "type": "rearrange",
        "words": [
            "Parsing",
            "Binding",
            "Cost Estimation",
            "Plan Selection",
            "Execution"
        ]
    },
    {
        "q": "Cost-based optimizers use statistics to estimate query execution costs.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this materialized view refresh strategy implement?",
        "type": "mcq",
        "c": "CREATE MATERIALIZED VIEW mv_daily_sales\nREFRESH FAST ON COMMIT\nAS SELECT sale_date, SUM(amount) as daily_total\nFROM sales\nGROUP BY sale_date",
        "o": [
            "Incremental refresh immediately after each commit",
            "Full refresh once daily",
            "Manual refresh only",
            "Never refresh"
        ]
    },
    {
        "q": "_____ refresh updates only changed portions of a materialized view.",
        "type": "fill_blank",
        "answers": [
            "Incremental"
        ],
        "other_options": [
            "Complete",
            "Full",
            "Total"
        ]
    },
    {
        "q": "Which materialized view refresh is most resource-intensive?",
        "type": "mcq",
        "o": [
            "Complete rebuild",
            "Fast incremental",
            "Delta refresh",
            "No-operation refresh"
        ]
    },
    {
        "q": "Match the materialized view type with its use case:",
        "type": "match",
        "left": [
            "Aggregate MV",
            "Join MV",
            "Nested MV",
            "Partitioned MV"
        ],
        "right": [
            "Pre-computed summaries",
            "Pre-joined tables",
            "Multi-level aggregation",
            "Large dataset efficiency"
        ]
    },
    {
        "q": "Materialized view rewrite automatically uses MVs in place of base tables.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this advanced window function compute?",
        "type": "mcq",
        "c": "SELECT customer_id, order_date, amount,\n  SUM(amount) OVER (\n    PARTITION BY customer_id \n    ORDER BY order_date \n    ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n  ) as rolling_3_order_sum,\n  AVG(amount) OVER (\n    PARTITION BY customer_id \n    ORDER BY order_date \n    RANGE BETWEEN INTERVAL 30 DAY PRECEDING AND CURRENT ROW\n  ) as rolling_30_day_avg\nFROM orders",
        "o": [
            "Row-based and time-based rolling aggregations",
            "Simple running total",
            "Grand total",
            "Static average"
        ]
    },
    {
        "q": "The _____ clause in window functions defines the set of rows for computation.",
        "type": "fill_blank",
        "answers": [
            "frame"
        ],
        "other_options": [
            "partition",
            "order",
            "filter"
        ]
    },
    {
        "q": "Which window frame specification is time-aware?",
        "type": "mcq",
        "o": [
            "RANGE with interval",
            "ROWS with count",
            "GROUPS with rank",
            "UNBOUNDED PRECEDING"
        ]
    },
    {
        "q": "Match the window function with its purpose:",
        "type": "match",
        "left": [
            "LAG",
            "LEAD",
            "NTILE",
            "PERCENT_RANK"
        ],
        "right": [
            "Previous row value",
            "Next row value",
            "Bucket distribution",
            "Percentile ranking"
        ]
    },
    {
        "q": "Rearrange the window function processing order:",
        "type": "rearrange",
        "words": [
            "FROM clause",
            "WHERE clause",
            "Window functions",
            "ORDER BY",
            "LIMIT"
        ]
    },
    {
        "q": "Window functions operate after GROUP BY but before final ORDER BY.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this temporal table query retrieve?",
        "type": "mcq",
        "c": "SELECT product_id, product_name, price\nFROM products\nFOR SYSTEM_TIME AS OF '2024-01-01 12:00:00'\nWHERE category = 'Electronics'",
        "o": [
            "Product data as it existed at a specific point in time",
            "Current product data only",
            "Deleted products",
            "Future product changes"
        ]
    },
    {
        "q": "_____ tables automatically track row validity periods for time travel.",
        "type": "fill_blank",
        "answers": [
            "Temporal"
        ],
        "other_options": [
            "Static",
            "Dynamic",
            "Cached"
        ]
    },
    {
        "q": "Which feature enables querying historical states of data?",
        "type": "mcq",
        "o": [
            "Time travel with temporal tables",
            "Simple SELECT statements",
            "DELETE operations",
            "TRUNCATE commands"
        ]
    },
    {
        "q": "Match the temporal query type with its syntax:",
        "type": "match",
        "left": [
            "Point in time",
            "Between periods",
            "All versions",
            "Contained in"
        ],
        "right": [
            "AS OF timestamp",
            "FROM t1 TO t2",
            "ALL",
            "CONTAINED IN period"
        ]
    },
    {
        "q": "Bi-temporal tables track both transaction time and valid time.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What optimization does this query plan show?",
        "type": "mcq",
        "c": "EXPLAIN ANALYZE\nSELECT c.customer_name, SUM(o.amount)\nFROM customers c\nJOIN orders o ON c.id = o.customer_id\nWHERE c.region = 'North'\nGROUP BY c.customer_name\n\n-- Plan: Hash Join with filter pushdown to customers",
        "o": [
            "Filter customers by region before joining with orders",
            "Scan all customers and orders first",
            "No optimization applied",
            "Full cartesian product"
        ]
    },
    {
        "q": "_____ join elimination removes unnecessary joins when results are unchanged.",
        "type": "fill_blank",
        "answers": [
            "Outer"
        ],
        "other_options": [
            "Inner",
            "Cross",
            "Natural"
        ]
    },
    {
        "q": "Which join algorithm is most efficient for joining pre-sorted data?",
        "type": "mcq",
        "o": [
            "Merge join",
            "Nested loop",
            "Hash join",
            "Cross join"
        ]
    },
    {
        "q": "Match the join optimization with its condition:",
        "type": "match",
        "left": [
            "Hash join",
            "Merge join",
            "Nested loop",
            "Broadcast join"
        ],
        "right": [
            "Large unsorted tables",
            "Sorted input data",
            "Small inner table",
            "Small table distributed"
        ]
    },
    {
        "q": "Rearrange the join algorithms by memory requirements:",
        "type": "rearrange",
        "words": [
            "Nested Loop",
            "Merge Join",
            "Hash Join",
            "Grace Hash Join"
        ]
    },
    {
        "q": "Adaptive join selection changes join algorithms based on runtime statistics.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this CDC implementation capture?",
        "type": "mcq",
        "c": "CREATE TABLE cdc_changes (\n  change_id BIGINT,\n  table_name VARCHAR(100),\n  operation CHAR(1),  -- I=Insert, U=Update, D=Delete\n  before_image JSON,\n  after_image JSON,\n  change_timestamp TIMESTAMP\n)",
        "o": [
            "Full before and after row states for each data modification",
            "Only insert operations",
            "Schema changes only",
            "User access logs"
        ]
    },
    {
        "q": "_____ CDC implementation reads database transaction logs directly.",
        "type": "fill_blank",
        "answers": [
            "Log-based"
        ],
        "other_options": [
            "Trigger-based",
            "Query-based",
            "Polling-based"
        ]
    },
    {
        "q": "Which CDC approach has the lowest impact on source system performance?",
        "type": "mcq",
        "o": [
            "Log-based CDC",
            "Trigger-based CDC",
            "Full table comparison",
            "Timestamp-based polling"
        ]
    },
    {
        "q": "Match the CDC component with its function:",
        "type": "match",
        "left": [
            "Source connector",
            "Change topic",
            "Consumer",
            "Schema registry"
        ],
        "right": [
            "Read changes",
            "Store events",
            "Process changes",
            "Track schemas"
        ]
    },
    {
        "q": "Debezium is a popular open-source CDC tool for streaming database changes.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this data quality framework validate?",
        "type": "mcq",
        "c": "RULE completeness_check:\n  EXPECT column_completeness(email) >= 0.95\n  EXPECT column_completeness(phone) >= 0.80\n\nRULE validity_check:\n  EXPECT values_in_set(status, ['active', 'inactive', 'pending'])\n  EXPECT regex_match(email, '^[a-zA-Z0-9+_.-]+@[a-zA-Z0-9.-]+$')",
        "o": [
            "Completeness thresholds and value validity rules",
            "Query performance metrics",
            "Storage usage limits",
            "User access permissions"
        ]
    },
    {
        "q": "_____ testing catches data quality issues before they impact downstream systems.",
        "type": "fill_blank",
        "answers": [
            "Proactive"
        ],
        "other_options": [
            "Reactive",
            "Passive",
            "Manual"
        ]
    },
    {
        "q": "Which data quality dimension measures the proportion of non-null values?",
        "type": "mcq",
        "o": [
            "Completeness",
            "Accuracy",
            "Consistency",
            "Timeliness"
        ]
    },
    {
        "q": "Match the data quality tool with its primary function:",
        "type": "match",
        "left": [
            "Great Expectations",
            "dbt tests",
            "Monte Carlo",
            "Datadog"
        ],
        "right": [
            "Expectation suites",
            "SQL-based tests",
            "Data observability",
            "Infrastructure monitoring"
        ]
    },
    {
        "q": "Rearrange the data quality improvement lifecycle:",
        "type": "rearrange",
        "words": [
            "Measure",
            "Analyze",
            "Improve",
            "Control",
            "Monitor"
        ]
    },
    {
        "q": "Data quality SLAs define acceptable thresholds for data issues.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this access control policy implement?",
        "type": "mcq",
        "c": "CREATE POLICY sales_region_policy ON sales_data\nFOR ALL\nUSING (region = current_user_attribute('assigned_region'))\nWITH CHECK (region = current_user_attribute('assigned_region'))",
        "o": [
            "Row-level security based on user region attribute",
            "Column-level encryption",
            "Full table access",
            "Read-only access"
        ]
    },
    {
        "q": "_____ policies filter query results based on user attributes.",
        "type": "fill_blank",
        "answers": [
            "Row-level security"
        ],
        "other_options": [
            "Column masking",
            "Table locking",
            "Schema hiding"
        ]
    },
    {
        "q": "Which security model automatically applies data filtering without query changes?",
        "type": "mcq",
        "o": [
            "Policy-based row-level security",
            "Manual WHERE clauses",
            "Application-side filtering",
            "No security model"
        ]
    },
    {
        "q": "Match the security control with its granularity:",
        "type": "match",
        "left": [
            "Database role",
            "Row-level security",
            "Column masking",
            "Cell-level security"
        ],
        "right": [
            "Object permissions",
            "Row filtering",
            "Value obfuscation",
            "Individual cell protection"
        ]
    },
    {
        "q": "Attribute-based access control uses user properties for authorization decisions.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this monitoring query detect?",
        "type": "mcq",
        "c": "SELECT table_name, \n  row_count_current,\n  row_count_previous,\n  ABS(row_count_current - row_count_previous) / row_count_previous * 100 as pct_change\nFROM table_statistics\nWHERE ABS(row_count_current - row_count_previous) / row_count_previous > 0.20",
        "o": [
            "Tables with more than 20% row count anomaly",
            "Tables with perfect data",
            "Empty tables",
            "Largest tables"
        ]
    },
    {
        "q": "_____ detection identifies unusual patterns in data volumes or quality metrics.",
        "type": "fill_blank",
        "answers": [
            "Anomaly"
        ],
        "other_options": [
            "Pattern",
            "Trend",
            "Cycle"
        ]
    },
    {
        "q": "Which metric best indicates potential ETL failures?",
        "type": "mcq",
        "o": [
            "Unexpected zero row counts in incremental loads",
            "Consistent daily volumes",
            "Increasing storage usage",
            "Stable query performance"
        ]
    },
    {
        "q": "Match the monitoring alert with its potential cause:",
        "type": "match",
        "left": [
            "Zero records loaded",
            "Duplicate spike",
            "Schema mismatch",
            "Late arrival"
        ],
        "right": [
            "Source system down",
            "CDC replay",
            "Upstream change",
            "Network delays"
        ]
    },
    {
        "q": "Rearrange the incident investigation steps:",
        "type": "rearrange",
        "words": [
            "Alert Received",
            "Root Cause Analysis",
            "Impact Assessment",
            "Resolution",
            "Post-mortem"
        ]
    },
    {
        "q": "Automated rollback capabilities reduce recovery time from data issues.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this data virtualization layer provide?",
        "type": "mcq",
        "c": "CREATE VIRTUAL TABLE unified_customers AS\nSELECT * FROM postgres.crm.customers\nUNION ALL\nSELECT * FROM salesforce.contacts\nUNION ALL\nSELECT * FROM mongodb.user_profiles",
        "o": [
            "Unified view across multiple data sources without data movement",
            "Physical copy of all data",
            "Delete from all sources",
            "Backup of source systems"
        ]
    },
    {
        "q": "Data _____ creates logical views over distributed data without physical consolidation.",
        "type": "fill_blank",
        "answers": [
            "virtualization"
        ],
        "other_options": [
            "replication",
            "migration",
            "archiving"
        ]
    },
    {
        "q": "Which approach minimizes data latency by querying sources directly?",
        "type": "mcq",
        "o": [
            "Data virtualization with query federation",
            "Batch ETL loading",
            "Manual data export",
            "Daily snapshots"
        ]
    },
    {
        "q": "Match the data integration approach with its latency:",
        "type": "match",
        "left": [
            "Batch ETL",
            "Micro-batch",
            "CDC streaming",
            "Virtualization"
        ],
        "right": [
            "Hours to days",
            "Minutes",
            "Seconds",
            "Real-time query"
        ]
    },
    {
        "q": "Query federation distributes queries to source systems for execution.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this data mesh implementation define?",
        "type": "mcq",
        "c": "DOMAIN orders_domain\n  DATA_PRODUCT order_history\n    SOURCE warehouse.gold.order_facts\n    SCHEMA order_schema_v2\n    SLA freshness <= 1h, completeness >= 99.5%\n    OWNER orders-team@company.com\n    CLASSIFICATION PII\n    ACCESS_POLICY sales_read_policy",
        "o": [
            "Self-service data product with quality, ownership, and access definitions",
            "Simple table definition",
            "ETL job configuration",
            "Report template"
        ]
    },
    {
        "q": "Domain _____ ensures teams are responsible for their data quality and availability.",
        "type": "fill_blank",
        "answers": [
            "ownership"
        ],
        "other_options": [
            "sharing",
            "copying",
            "deleting"
        ]
    },
    {
        "q": "Which data mesh principle emphasizes treating data like a product?",
        "type": "mcq",
        "o": [
            "Data as a product",
            "Centralized data team",
            "Single data warehouse",
            "No data governance"
        ]
    },
    {
        "q": "Match the data mesh role with its responsibility:",
        "type": "match",
        "left": [
            "Domain team",
            "Platform team",
            "Data governance",
            "Data consumer"
        ],
        "right": [
            "Own data products",
            "Provide infrastructure",
            "Set standards",
            "Use data products"
        ]
    },
    {
        "q": "Rearrange the data product maturity levels:",
        "type": "rearrange",
        "words": [
            "Discoverable",
            "Addressable",
            "Self-describing",
            "Interoperable",
            "Secure"
        ]
    },
    {
        "q": "Federated computational governance enables decentralized data management with global standards.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this streaming architecture implement?",
        "type": "mcq",
        "c": "CREATE STREAM orders_stream (\n  order_id BIGINT,\n  amount DECIMAL(10,2),\n  event_time TIMESTAMP\n) WITH (kafka_topic='orders', value_format='JSON')\n\nCREATE TABLE order_aggregates AS\nSELECT TUMBLING_WINDOW(event_time, 5 MINUTES) as window,\n  COUNT(*) as order_count,\n  SUM(amount) as total_amount\nFROM orders_stream\nGROUP BY TUMBLING_WINDOW(event_time, 5 MINUTES)",
        "o": [
            "Real-time streaming aggregations with windowed processing",
            "Batch ETL job",
            "Static table creation",
            "Data deletion"
        ]
    },
    {
        "q": "_____ windows in stream processing divide time into fixed-size, non-overlapping buckets.",
        "type": "fill_blank",
        "answers": [
            "Tumbling"
        ],
        "other_options": [
            "Hopping",
            "Session",
            "Sliding"
        ]
    },
    {
        "q": "Which window type creates overlapping time buckets for sliding analysis?",
        "type": "mcq",
        "o": [
            "Hopping window",
            "Tumbling window",
            "Session window",
            "Global window"
        ]
    },
    {
        "q": "Match the stream window type with its behavior:",
        "type": "match",
        "left": [
            "Tumbling",
            "Hopping",
            "Session",
            "Sliding"
        ],
        "right": [
            "Fixed non-overlapping",
            "Fixed overlapping",
            "Activity-based gaps",
            "Continuous sliding"
        ]
    },
    {
        "q": "Event time processing uses timestamps from the data rather than processing time.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this data quality expectation suite validate?",
        "type": "mcq",
        "c": "expectation_suite = {\n  'expectations': [\n    {'expectation_type': 'expect_column_to_exist', 'column': 'order_id'},\n    {'expectation_type': 'expect_column_values_to_be_unique', 'column': 'order_id'},\n    {'expectation_type': 'expect_column_values_to_be_between', 'column': 'amount', 'min_value': 0, 'max_value': 1000000},\n    {'expectation_type': 'expect_column_values_to_not_be_null', 'column': 'customer_id'}\n  ]\n}",
        "o": [
            "Schema, uniqueness, range, and null checks using Great Expectations",
            "Query performance metrics",
            "Storage optimization rules",
            "User access patterns"
        ]
    },
    {
        "q": "_____ expectations define automated data quality checks that run during pipeline execution.",
        "type": "fill_blank",
        "answers": [
            "Data"
        ],
        "other_options": [
            "System",
            "User",
            "Network"
        ]
    },
    {
        "q": "Which tool provides declarative data quality testing with expectation suites?",
        "type": "mcq",
        "o": [
            "Great Expectations",
            "Apache Spark",
            "Apache Kafka",
            "PostgreSQL"
        ]
    },
    {
        "q": "Match the data quality check type with its validation:",
        "type": "match",
        "left": [
            "Schema check",
            "Uniqueness check",
            "Range check",
            "Pattern check"
        ],
        "right": [
            "Column exists",
            "No duplicates",
            "Within bounds",
            "Matches regex"
        ]
    },
    {
        "q": "Rearrange the data contract enforcement stages:",
        "type": "rearrange",
        "words": [
            "Define Contract",
            "Validate Schema",
            "Check Quality",
            "Publish Report",
            "Alert on Violation"
        ]
    },
    {
        "q": "Data quality gates can block deployments when expectations fail.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What optimization does this query engine configuration enable?",
        "type": "mcq",
        "c": "SET spark.sql.adaptive.enabled = true\nSET spark.sql.adaptive.coalescePartitions.enabled = true\nSET spark.sql.adaptive.skewJoin.enabled = true\nSET spark.sql.adaptive.localShuffleReader.enabled = true",
        "o": [
            "Runtime query plan adaptation based on actual data characteristics",
            "Fixed query execution plans",
            "Disable all optimizations",
            "Manual tuning only"
        ]
    },
    {
        "q": "Adaptive query _____ modifies execution plans based on runtime statistics.",
        "type": "fill_blank",
        "answers": [
            "execution"
        ],
        "other_options": [
            "parsing",
            "binding",
            "compilation"
        ]
    },
    {
        "q": "Which adaptive optimization handles data distribution imbalances?",
        "type": "mcq",
        "o": [
            "Skew join handling",
            "Static partitioning",
            "Fixed parallelism",
            "No optimization"
        ]
    },
    {
        "q": "Match the adaptive optimization with its runtime action:",
        "type": "match",
        "left": [
            "Coalesce partitions",
            "Skew join split",
            "Local shuffle",
            "Dynamic pruning"
        ],
        "right": [
            "Reduce small partitions",
            "Handle hot keys",
            "Avoid network shuffle",
            "Runtime filter pushdown"
        ]
    },
    {
        "q": "Dynamic partition pruning applies filters discovered during query execution.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this data sharing configuration enable?",
        "type": "mcq",
        "c": "CREATE SHARE customer_data_share\n  COMMENT 'Shared customer analytics data'\n  ADD TABLE gold.customer_360\n  ADD TABLE gold.customer_segments\n\nGRANT USAGE ON SHARE customer_data_share\nTO CONSUMER partner_account",
        "o": [
            "Secure data sharing with external organizations without data copying",
            "Internal backup creation",
            "Data deletion",
            "Schema migration"
        ]
    },
    {
        "q": "_____ data sharing provides live access without data replication.",
        "type": "fill_blank",
        "answers": [
            "Zero-copy"
        ],
        "other_options": [
            "Full-copy",
            "Snapshot",
            "Backup"
        ]
    },
    {
        "q": "Which cloud warehouse feature enables secure cross-account data access?",
        "type": "mcq",
        "o": [
            "Data sharing with access grants",
            "Manual file exports",
            "Email attachments",
            "USB transfers"
        ]
    },
    {
        "q": "Match the data sharing model with its access pattern:",
        "type": "match",
        "left": [
            "Direct share",
            "Data exchange",
            "Clean room",
            "Marketplace"
        ],
        "right": [
            "Point-to-point",
            "Multi-party hub",
            "Privacy-preserving",
            "Public discovery"
        ]
    },
    {
        "q": "Rearrange the data monetization maturity levels:",
        "type": "rearrange",
        "words": [
            "Internal Use",
            "Partner Sharing",
            "Data Exchange",
            "Data Product Marketplace"
        ]
    },
    {
        "q": "Data clean rooms enable analytics on combined datasets without exposing raw data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this differential privacy implementation protect?",
        "type": "mcq",
        "c": "SELECT region,\n  COUNT(*) + RANDOM_LAPLACE(0, 1/epsilon) as noisy_count,\n  AVG(amount) + RANDOM_LAPLACE(0, sensitivity/epsilon) as noisy_avg\nFROM orders\nGROUP BY region",
        "o": [
            "Individual record privacy while allowing aggregate analysis",
            "Query performance optimization",
            "Data compression",
            "Schema validation"
        ]
    },
    {
        "q": "_____ privacy adds calibrated noise to query results to protect individual records.",
        "type": "fill_blank",
        "answers": [
            "Differential"
        ],
        "other_options": [
            "Static",
            "Dynamic",
            "Standard"
        ]
    },
    {
        "q": "Which privacy technique bounds the information revealed about any single record?",
        "type": "mcq",
        "o": [
            "Differential privacy with epsilon bounds",
            "Simple encryption",
            "Password protection",
            "No privacy technique"
        ]
    },
    {
        "q": "Match the privacy technique with its mechanism:",
        "type": "match",
        "left": [
            "Differential privacy",
            "K-anonymity",
            "L-diversity",
            "T-closeness"
        ],
        "right": [
            "Noise addition",
            "Generalization",
            "Attribute diversity",
            "Distribution similarity"
        ]
    },
    {
        "q": "Rearrange the privacy protection levels from weakest to strongest:",
        "type": "rearrange",
        "words": [
            "Pseudonymization",
            "K-anonymity",
            "L-diversity",
            "Differential Privacy"
        ]
    },
    {
        "q": "Epsilon in differential privacy controls the privacy-utility tradeoff.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this multi-cluster warehouse configuration achieve?",
        "type": "mcq",
        "c": "CREATE WAREHOUSE analytics_wh\n  WITH WAREHOUSE_SIZE = 'MEDIUM'\n  MIN_CLUSTER_COUNT = 1\n  MAX_CLUSTER_COUNT = 10\n  SCALING_POLICY = 'STANDARD'\n  AUTO_SUSPEND = 300\n  AUTO_RESUME = TRUE",
        "o": [
            "Automatic scaling based on workload demand with cost optimization",
            "Fixed single-cluster deployment",
            "Manual scaling only",
            "No resource management"
        ]
    },
    {
        "q": "_____ scaling adds or removes compute clusters based on query queue depth.",
        "type": "fill_blank",
        "answers": [
            "Auto"
        ],
        "other_options": [
            "Manual",
            "Static",
            "Scheduled"
        ]
    },
    {
        "q": "Which metric triggers automatic cluster scale-out?",
        "type": "mcq",
        "o": [
            "Query queue exceeding threshold",
            "Storage usage",
            "User login count",
            "Manual trigger only"
        ]
    },
    {
        "q": "Match the scaling policy with its behavior:",
        "type": "match",
        "left": [
            "Standard",
            "Economy",
            "Aggressive",
            "Manual"
        ],
        "right": [
            "Balanced scaling",
            "Minimize costs",
            "Fastest response",
            "User controlled"
        ]
    },
    {
        "q": "Auto-suspend reduces compute costs during idle periods.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this cross-database query federation accomplish?",
        "type": "mcq",
        "c": "SELECT\n  p.product_name,\n  s.total_sales,\n  i.inventory_level\nFROM postgres_products@postgres_link p\nJOIN snowflake_sales@snowflake_link s ON p.id = s.product_id\nJOIN oracle_inventory@oracle_link i ON p.id = i.product_id",
        "o": [
            "Join data across multiple database systems in a single query",
            "Copy all data to one location",
            "Delete cross-database references",
            "Create local backups"
        ]
    },
    {
        "q": "Query _____ distributes execution across multiple data sources.",
        "type": "fill_blank",
        "answers": [
            "federation"
        ],
        "other_options": [
            "isolation",
            "separation",
            "duplication"
        ]
    },
    {
        "q": "Which challenge is most significant in federated query execution?",
        "type": "mcq",
        "o": [
            "Optimizing distributed joins across network boundaries",
            "Local query execution",
            "Single-source aggregation",
            "File storage"
        ]
    },
    {
        "q": "Match the federation component with its function:",
        "type": "match",
        "left": [
            "Query router",
            "Metadata catalog",
            "Data connector",
            "Query optimizer"
        ],
        "right": [
            "Distribute queries",
            "Track sources",
            "Access data",
            "Plan execution"
        ]
    },
    {
        "q": "Rearrange the federated query execution steps:",
        "type": "rearrange",
        "words": [
            "Parse Query",
            "Identify Sources",
            "Plan Distribution",
            "Execute Remotely",
            "Combine Results"
        ]
    },
    {
        "q": "Predicate pushdown in federation reduces data transfer by filtering at source.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this AI/ML integration enable?",
        "type": "mcq",
        "c": "CREATE MODEL customer_churn_model\n  FROM customer_features\n  TARGET is_churned\n  USING XGBoost(\n    max_depth=6,\n    n_estimators=100\n  )\n\nSELECT customer_id,\n  PREDICT(customer_churn_model, *) as churn_probability\nFROM current_customers",
        "o": [
            "In-database machine learning model training and inference",
            "External API calls",
            "Manual data export",
            "Static rule engine"
        ]
    },
    {
        "q": "_____ machine learning executes models directly within the database engine.",
        "type": "fill_blank",
        "answers": [
            "In-database"
        ],
        "other_options": [
            "External",
            "Offline",
            "Manual"
        ]
    },
    {
        "q": "Which benefit does in-database ML provide over external model serving?",
        "type": "mcq",
        "o": [
            "Reduced data movement and lower latency",
            "More complex models",
            "Better visualization",
            "Simpler deployment"
        ]
    },
    {
        "q": "Match the ML integration pattern with its characteristic:",
        "type": "match",
        "left": [
            "In-database ML",
            "External model serving",
            "Feature store",
            "MLOps pipeline"
        ],
        "right": [
            "SQL-based training",
            "API-based inference",
            "Centralized features",
            "Automated lifecycle"
        ]
    },
    {
        "q": "Feature stores provide consistent feature values for training and inference.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this governance policy automate?",
        "type": "mcq",
        "c": "CREATE TAG pii_tag\nCREATE MASKING POLICY pii_mask AS (val STRING)\n  RETURNS STRING -> CASE WHEN IS_ROLE_IN_SESSION('admin') THEN val ELSE 'MASKED' END\n\nALTER TABLE customers MODIFY COLUMN ssn SET TAG pii_tag\nALTER COLUMN ssn SET MASKING POLICY pii_mask",
        "o": [
            "Automatic PII detection and dynamic masking based on user role",
            "Manual data classification",
            "Static encryption",
            "Data deletion"
        ]
    },
    {
        "q": "Tag-based _____ enables consistent policy enforcement across all tagged columns.",
        "type": "fill_blank",
        "answers": [
            "governance"
        ],
        "other_options": [
            "storage",
            "compression",
            "indexing"
        ]
    },
    {
        "q": "Which approach scales data governance across thousands of tables?",
        "type": "mcq",
        "o": [
            "Policy-based automation with classification tags",
            "Manual column inspection",
            "Individual table policies",
            "No governance"
        ]
    },
    {
        "q": "Match the governance automation with its trigger:",
        "type": "match",
        "left": [
            "Classification scan",
            "Policy inheritance",
            "Lineage propagation",
            "Access audit"
        ],
        "right": [
            "New data detection",
            "Schema changes",
            "Transformation tracking",
            "Query execution"
        ]
    },
    {
        "q": "Rearrange the data governance maturity stages:",
        "type": "rearrange",
        "words": [
            "Ad-hoc",
            "Documented",
            "Automated",
            "Optimized",
            "Predictive"
        ]
    },
    {
        "q": "AI-powered data cataloging can automatically classify and tag sensitive data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this lakehouse ACID transaction guarantee?",
        "type": "mcq",
        "c": "BEGIN TRANSACTION\n  DELETE FROM delta_table WHERE obsolete = true\n  INSERT INTO delta_table SELECT * FROM staging\n  UPDATE delta_table SET status = 'active' WHERE status = 'pending'\nCOMMIT\n\n-- All changes are atomic - all succeed or all rollback",
        "o": [
            "Atomic multi-statement transactions on data lake storage",
            "Eventual consistency only",
            "No transaction support",
            "Read-only operations"
        ]
    },
    {
        "q": "_____ isolation in lakehouses prevents dirty reads during concurrent operations.",
        "type": "fill_blank",
        "answers": [
            "Snapshot"
        ],
        "other_options": [
            "Read committed",
            "Serializable",
            "No"
        ]
    },
    {
        "q": "Which lakehouse format provides ACID transactions on object storage?",
        "type": "mcq",
        "o": [
            "Delta Lake",
            "CSV files",
            "Plain Parquet",
            "JSON files"
        ]
    },
    {
        "q": "Match the ACID property with its lakehouse implementation:",
        "type": "match",
        "left": [
            "Atomicity",
            "Consistency",
            "Isolation",
            "Durability"
        ],
        "right": [
            "Transaction log",
            "Schema enforcement",
            "MVCC",
            "Object storage"
        ]
    },
    {
        "q": "Optimistic concurrency control in lakehouses detects conflicts at commit time.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What optimization does this Z-ordering achieve?",
        "type": "mcq",
        "c": "OPTIMIZE delta_table\nZORDER BY (region, product_category)\n\n-- Interleaves data by both columns for multi-dimensional filtering",
        "o": [
            "Co-locate related data for efficient multi-column filtering",
            "Sort by single column only",
            "Random data distribution",
            "Delete all data"
        ]
    },
    {
        "q": "_____ ordering interleaves data by multiple columns for multi-dimensional queries.",
        "type": "fill_blank",
        "answers": [
            "Z"
        ],
        "other_options": [
            "Linear",
            "Random",
            "Hash"
        ]
    },
    {
        "q": "Which data layout optimization is best for queries filtering on multiple columns?",
        "type": "mcq",
        "o": [
            "Z-ordering with space-filling curves",
            "Single-column sorting",
            "No ordering",
            "Random distribution"
        ]
    },
    {
        "q": "Match the data layout technique with its query pattern:",
        "type": "match",
        "left": [
            "Partitioning",
            "Clustering",
            "Z-ordering",
            "Bucketing"
        ],
        "right": [
            "High-cardinality filter",
            "Range queries",
            "Multi-dimensional",
            "Join optimization"
        ]
    },
    {
        "q": "Rearrange the lakehouse optimization techniques by complexity:",
        "type": "rearrange",
        "words": [
            "Compaction",
            "Partitioning",
            "Clustering",
            "Z-ordering",
            "Liquid Clustering"
        ]
    },
    {
        "q": "Liquid clustering in lakehouse tables automatically maintains optimal data layout.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this streaming-batch unification pattern implement?",
        "type": "mcq",
        "c": "CREATE STREAMING TABLE enriched_orders AS\nSELECT \n  o.*,\n  c.customer_name,\n  p.product_name\nFROM STREAM(raw_orders) o\nJOIN dim_customers c ON o.customer_id = c.id\nJOIN dim_products p ON o.product_id = p.id\n\n-- Same table serves both real-time and batch queries",
        "o": [
            "Unified streaming and batch processing with single table serving both",
            "Separate streaming and batch pipelines",
            "Batch-only processing",
            "Stream-only processing"
        ]
    },
    {
        "q": "Table _____ unifies streaming ingestion with batch query serving.",
        "type": "fill_blank",
        "answers": [
            "format"
        ],
        "other_options": [
            "structure",
            "schema",
            "type"
        ]
    },
    {
        "q": "Which architecture eliminates the need for separate streaming and batch systems?",
        "type": "mcq",
        "o": [
            "Unified lakehouse with Delta Live Tables",
            "Lambda architecture",
            "Separate stream and batch",
            "Manual data copying"
        ]
    },
    {
        "q": "Match the streaming lakehouse concept with its benefit:",
        "type": "match",
        "left": [
            "Auto-refresh MV",
            "Incremental ingestion",
            "Schema evolution",
            "Quality enforcement"
        ],
        "right": [
            "Fresh aggregates",
            "Efficient updates",
            "Backward compatibility",
            "Data validation"
        ]
    },
    {
        "q": "Delta Live Tables automatically manages streaming pipeline dependencies.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this semantic model integration enable?",
        "type": "mcq",
        "c": "DEFINE METRIC revenue AS SUM(amount)\n  WHERE status = 'completed'\n  ACROSS DIMENSIONS [date, product, region, customer_segment]\n  CERTIFIED BY data-team@company.com\n  DESCRIPTION 'Official revenue metric'\n\n-- All BI tools query this centralized definition",
        "o": [
            "Single source of truth for business metrics across all tools",
            "Duplicate metric definitions",
            "Tool-specific calculations",
            "No metric governance"
        ]
    },
    {
        "q": "A _____ layer provides unified business logic for all downstream consumers.",
        "type": "fill_blank",
        "answers": [
            "metrics"
        ],
        "other_options": [
            "data",
            "storage",
            "network"
        ]
    },
    {
        "q": "Which approach ensures consistent metric calculations across BI tools?",
        "type": "mcq",
        "o": [
            "Centralized semantic layer with metric definitions",
            "Each tool calculates independently",
            "Manual calculation comparison",
            "No shared definitions"
        ]
    },
    {
        "q": "Match the semantic layer component with its responsibility:",
        "type": "match",
        "left": [
            "Metric definitions",
            "Entity models",
            "Joins specifications",
            "Access policies"
        ],
        "right": [
            "Business calculations",
            "Domain objects",
            "Relationship mapping",
            "Security rules"
        ]
    },
    {
        "q": "Rearrange the headless BI architecture layers:",
        "type": "rearrange",
        "words": [
            "Physical Tables",
            "Semantic Layer",
            "Metrics API",
            "BI Tools",
            "End Users"
        ]
    },
    {
        "q": "Metric stores decouple business definitions from visualization tools.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this active metadata application enable?",
        "type": "mcq",
        "c": "RULE auto_archive:\n  WHEN table.last_accessed > 90 days\n  AND table.is_production = false\n  THEN archive_to_cold_storage(table)\n  AND notify_owner(table.owner)\n\nRULE auto_scale:\n  WHEN query_queue_depth > 10\n  THEN increase_cluster_count()",
        "o": [
            "Automated actions triggered by metadata conditions",
            "Static metadata storage",
            "Manual archival processes",
            "No automation"
        ]
    },
    {
        "q": "_____ metadata drives automated responses based on data characteristics.",
        "type": "fill_blank",
        "answers": [
            "Active"
        ],
        "other_options": [
            "Passive",
            "Static",
            "Manual"
        ]
    },
    {
        "q": "Which capability distinguishes active metadata from traditional catalogs?",
        "type": "mcq",
        "o": [
            "Trigger actions based on metadata changes",
            "Store metadata only",
            "Manual intervention required",
            "No integration capabilities"
        ]
    },
    {
        "q": "Match the active metadata use case with its automation:",
        "type": "match",
        "left": [
            "Cost optimization",
            "Quality enforcement",
            "Access governance",
            "Lifecycle management"
        ],
        "right": [
            "Auto-suspend idle resources",
            "Block bad data",
            "Auto-provision access",
            "Archive stale data"
        ]
    },
    {
        "q": "Event-driven architecture enables real-time metadata-triggered actions.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this natural language query interface demonstrate?",
        "type": "mcq",
        "c": "USER: \"Show me top 10 customers by revenue last quarter\"\n\nAI-GENERATED SQL:\nSELECT customer_name, SUM(amount) as revenue\nFROM fact_orders o\nJOIN dim_customers c ON o.customer_id = c.id\nWHERE order_date BETWEEN '2024-01-01' AND '2024-03-31'\nGROUP BY customer_name\nORDER BY revenue DESC\nLIMIT 10",
        "o": [
            "AI-powered text-to-SQL query generation",
            "Manual SQL writing",
            "Pre-built reports only",
            "No query capability"
        ]
    },
    {
        "q": "_____ analytics uses AI to convert natural language questions to SQL.",
        "type": "fill_blank",
        "answers": [
            "Conversational"
        ],
        "other_options": [
            "Traditional",
            "Manual",
            "Batch"
        ]
    },
    {
        "q": "Which technology enables business users to query data using natural language?",
        "type": "mcq",
        "o": [
            "Large language models with semantic layer context",
            "Command line interfaces",
            "Complex SQL editors",
            "Static reports"
        ]
    },
    {
        "q": "Match the AI analytics capability with its function:",
        "type": "match",
        "left": [
            "Text-to-SQL",
            "Auto-visualization",
            "Anomaly explanation",
            "Insight generation"
        ],
        "right": [
            "Query generation",
            "Chart recommendation",
            "Root cause analysis",
            "Pattern discovery"
        ]
    },
    {
        "q": "Rearrange the AI-assisted analytics maturity levels:",
        "type": "rearrange",
        "words": [
            "SQL Generation",
            "Visualization Suggest",
            "Insight Discovery",
            "Autonomous Analysis"
        ]
    },
    {
        "q": "Semantic layer metadata improves AI-generated SQL accuracy.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this vector embedding integration enable?",
        "type": "mcq",
        "c": "CREATE TABLE product_embeddings (\n  product_id BIGINT,\n  embedding VECTOR(768),\n  PRIMARY KEY (product_id)\n)\n\nSELECT p.product_name, p.description\nFROM products p\nJOIN product_embeddings e ON p.id = e.product_id\nORDER BY VECTOR_COSINE_SIMILARITY(e.embedding, :query_embedding)\nLIMIT 10",
        "o": [
            "Semantic search using vector similarity in the warehouse",
            "Traditional keyword search",
            "Exact match queries",
            "No search capability"
        ]
    },
    {
        "q": "_____ search finds semantically similar items using vector embeddings.",
        "type": "fill_blank",
        "answers": [
            "Semantic"
        ],
        "other_options": [
            "Keyword",
            "Exact",
            "Pattern"
        ]
    },
    {
        "q": "Which index type optimizes vector similarity searches?",
        "type": "mcq",
        "o": [
            "Approximate nearest neighbor (ANN) index",
            "B-tree index",
            "Hash index",
            "Bitmap index"
        ]
    },
    {
        "q": "Match the vector operation with its use case:",
        "type": "match",
        "left": [
            "Cosine similarity",
            "Euclidean distance",
            "Dot product",
            "Manhattan distance"
        ],
        "right": [
            "Text similarity",
            "Spatial proximity",
            "Recommendation",
            "Grid distance"
        ]
    },
    {
        "q": "Vector databases integrated with warehouses enable unified analytics and AI.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this data product observability track?",
        "type": "mcq",
        "c": "DATA_PRODUCT customer_360\n  SLO:\n    freshness: < 1 hour (99.9%)\n    completeness: > 99.5%\n    accuracy: > 99%\n  METRICS:\n    - query_count: 15,234/day\n    - unique_consumers: 45\n    - avg_latency: 2.3s\n  ALERTS:\n    - freshness_breach: 2 in last 30 days",
        "o": [
            "Data product health, usage, and SLO compliance",
            "Storage costs only",
            "User activity logs",
            "Query syntax"
        ]
    },
    {
        "q": "Service level _____ define target metrics for data product reliability.",
        "type": "fill_blank",
        "answers": [
            "objectives"
        ],
        "other_options": [
            "agreements",
            "indicators",
            "indexes"
        ]
    },
    {
        "q": "Which metric indicates data product adoption success?",
        "type": "mcq",
        "o": [
            "Number of unique consumers and query volume",
            "Storage size",
            "Column count",
            "Table age"
        ]
    },
    {
        "q": "Match the data product metric with its category:",
        "type": "match",
        "left": [
            "Freshness",
            "Query latency",
            "Consumer count",
            "Error rate"
        ],
        "right": [
            "Data quality",
            "Performance",
            "Adoption",
            "Reliability"
        ]
    },
    {
        "q": "Rearrange the data product lifecycle stages:",
        "type": "rearrange",
        "words": [
            "Discovery",
            "Development",
            "Deployment",
            "Consumption",
            "Deprecation"
        ]
    },
    {
        "q": "Error budgets track allowed SLO violations before requiring intervention.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this cost attribution model calculate?",
        "type": "mcq",
        "c": "SELECT \n  department,\n  SUM(compute_credits) as compute_cost,\n  SUM(storage_bytes) / 1e12 as storage_tb,\n  SUM(query_count) as queries,\n  compute_cost / queries as cost_per_query\nFROM usage_attribution\nWHERE date = CURRENT_DATE - 1\nGROUP BY department\nORDER BY compute_cost DESC",
        "o": [
            "Departmental chargeback based on actual resource consumption",
            "Total company costs only",
            "User login attempts",
            "Schema change count"
        ]
    },
    {
        "q": "_____ attribution allocates cloud costs to business units based on usage.",
        "type": "fill_blank",
        "answers": [
            "Cost"
        ],
        "other_options": [
            "Credit",
            "Time",
            "Storage"
        ]
    },
    {
        "q": "Which approach enables departmental accountability for data costs?",
        "type": "mcq",
        "o": [
            "Resource tagging with usage-based chargeback",
            "Equal cost splitting",
            "No cost tracking",
            "IT absorbs all costs"
        ]
    },
    {
        "q": "Match the cost optimization lever with its action:",
        "type": "match",
        "left": [
            "Right-sizing",
            "Auto-suspend",
            "Result caching",
            "Compression"
        ],
        "right": [
            "Match resources to needs",
            "Stop idle compute",
            "Avoid re-computation",
            "Reduce storage"
        ]
    },
    {
        "q": "FinOps practices optimize cloud data warehouse spending.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this zero-downtime migration implement?",
        "type": "mcq",
        "c": "-- Phase 1: Dual-write to old and new systems\n-- Phase 2: Shadow read comparison\n-- Phase 3: Gradual traffic shift (10% -> 50% -> 100%)\n-- Phase 4: Decommission old system\n\nROUTING_POLICY:\n  IF percent_new < 100:\n    write_to(old_warehouse, new_warehouse)\n    compare_results(old_result, new_result)\n  ELSE:\n    write_to(new_warehouse)",
        "o": [
            "Gradual migration with validation before full cutover",
            "Big bang migration",
            "No migration strategy",
            "Manual data copying"
        ]
    },
    {
        "q": "_____ deployment pattern allows gradual traffic shifting during migrations.",
        "type": "fill_blank",
        "answers": [
            "Canary"
        ],
        "other_options": [
            "Big bang",
            "Waterfall",
            "Manual"
        ]
    },
    {
        "q": "Which validation ensures data consistency during warehouse migration?",
        "type": "mcq",
        "o": [
            "Shadow comparison of query results between systems",
            "No validation needed",
            "UI comparison only",
            "Schema comparison only"
        ]
    },
    {
        "q": "Match the migration phase with its activity:",
        "type": "match",
        "left": [
            "Preparation",
            "Parallel run",
            "Cutover",
            "Decommission"
        ],
        "right": [
            "Setup new system",
            "Validate parity",
            "Switch traffic",
            "Retire old system"
        ]
    },
    {
        "q": "Rearrange the migration risk mitigation strategies by impact:",
        "type": "rearrange",
        "words": [
            "Rollback Plan",
            "Data Validation",
            "Performance Testing",
            "User Training",
            "Documentation"
        ]
    },
    {
        "q": "Feature flags enable controlled rollout of new warehouse capabilities.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What optimization does this query routing policy implement?",
        "type": "mcq",
        "c": "ROUTING POLICY workload_isolation:\n  WHEN query.type = 'reporting' AND query.size = 'large'\n    ROUTE TO warehouse_xl WITH priority LOW\n  WHEN query.type = 'dashboard' AND query.source = 'executive'\n    ROUTE TO warehouse_xl WITH priority HIGH\n  WHEN query.type = 'ad_hoc'\n    ROUTE TO warehouse_m WITH timeout 300s\n  DEFAULT\n    ROUTE TO warehouse_s",
        "o": [
            "Intelligent query routing based on workload classification",
            "Random query distribution",
            "Single warehouse for all queries",
            "Manual query assignment"
        ]
    },
    {
        "q": "_____ routing directs queries to appropriate resources based on characteristics.",
        "type": "fill_blank",
        "answers": [
            "Intelligent"
        ],
        "other_options": [
            "Random",
            "Static",
            "Manual"
        ]
    },
    {
        "q": "Which workload management feature prevents resource contention?",
        "type": "mcq",
        "o": [
            "Separate compute pools with query classification",
            "Single shared resource pool",
            "No resource management",
            "First-come-first-served only"
        ]
    },
    {
        "q": "Match the query classification with its routing strategy:",
        "type": "match",
        "left": [
            "Interactive dashboards",
            "Large batch reports",
            "Ad-hoc exploration",
            "ML training"
        ],
        "right": [
            "High priority dedicated",
            "Low priority large",
            "Medium timeout",
            "Isolated compute"
        ]
    },
    {
        "q": "Query queuing prevents system overload when resources are exhausted.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this data quality circuit breaker implement?",
        "type": "mcq",
        "c": "QUALITY_GATE staging_to_production:\n  CHECKS:\n    - completeness(critical_columns) >= 99%\n    - freshness(max_timestamp) < 2 hours\n    - row_count_variance < 20%\n  ON_FAILURE:\n    - halt_pipeline()\n    - notify_oncall()\n    - create_incident(severity=HIGH)\n  ON_SUCCESS:\n    - promote_to_production()",
        "o": [
            "Automated pipeline halt when quality thresholds are breached",
            "Always promote regardless of quality",
            "Manual quality review",
            "Silent failure logging"
        ]
    },
    {
        "q": "Quality _____ stop pipelines when data fails validation checks.",
        "type": "fill_blank",
        "answers": [
            "gates"
        ],
        "other_options": [
            "logs",
            "reports",
            "views"
        ]
    },
    {
        "q": "Which approach prevents bad data from reaching production systems?",
        "type": "mcq",
        "o": [
            "Pre-promotion quality gates with automated blocking",
            "Post-production monitoring only",
            "No quality checks",
            "Manual inspection"
        ]
    },
    {
        "q": "Match the quality gate outcome with its action:",
        "type": "match",
        "left": [
            "Pass",
            "Warn",
            "Fail",
            "Critical"
        ],
        "right": [
            "Promote data",
            "Log and continue",
            "Block promotion",
            "Page on-call"
        ]
    },
    {
        "q": "Rearrange the data pipeline reliability patterns by implementation complexity:",
        "type": "rearrange",
        "words": [
            "Logging",
            "Alerting",
            "Quality Gates",
            "Auto-remediation",
            "Self-healing"
        ]
    },
    {
        "q": "Self-healing pipelines can automatically retry or correct common failure patterns.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What capability does this real-time feature serving enable?",
        "type": "mcq",
        "c": "FEATURE_STORE:\n  ENTITY customer\n  FEATURES:\n    - customer_lifetime_value (batch, daily refresh)\n    - recent_purchase_count (streaming, 5-min window)\n    - risk_score (on-demand, ML model)\n  SERVING:\n    - online_store: Redis (p99 < 10ms)\n    - offline_store: Delta Lake (historical training)",
        "o": [
            "Low-latency feature access for real-time ML inference",
            "Batch-only feature access",
            "No ML integration",
            "Manual feature calculation"
        ]
    },
    {
        "q": "_____ serving provides sub-second feature access for production ML models.",
        "type": "fill_blank",
        "answers": [
            "Online"
        ],
        "other_options": [
            "Batch",
            "Offline",
            "Manual"
        ]
    },
    {
        "q": "Which architecture ensures training-serving feature consistency?",
        "type": "mcq",
        "o": [
            "Unified feature store with offline and online serving",
            "Separate feature calculation for training and serving",
            "No feature management",
            "Manual feature pipelines"
        ]
    },
    {
        "q": "Match the feature store component with its purpose:",
        "type": "match",
        "left": [
            "Offline store",
            "Online store",
            "Feature computation",
            "Feature registry"
        ],
        "right": [
            "Training data",
            "Real-time serving",
            "Transform raw data",
            "Metadata management"
        ]
    },
    {
        "q": "Point-in-time correct feature retrieval prevents data leakage in ML training.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this GitOps-style infrastructure pattern implement?",
        "type": "mcq",
        "c": "# warehouse_config.yaml\nversion: 2.1\nwarehouses:\n  analytics:\n    size: MEDIUM\n    auto_scale: true\n    min_clusters: 1\n    max_clusters: 5\n    auto_suspend: 300\n    tags:\n      department: analytics\n      cost_center: CC-1234\n\n# CI/CD pipeline applies changes on merge",
        "o": [
            "Version-controlled infrastructure configuration with automated deployment",
            "Manual console configurations",
            "No configuration management",
            "Ad-hoc changes"
        ]
    },
    {
        "q": "Infrastructure as _____ manages warehouse resources through version control.",
        "type": "fill_blank",
        "answers": [
            "Code"
        ],
        "other_options": [
            "Manual",
            "Console",
            "Script"
        ]
    },
    {
        "q": "Which practice enables reproducible warehouse environment management?",
        "type": "mcq",
        "o": [
            "Declarative configuration with CI/CD automation",
            "Manual point-and-click setup",
            "Undocumented changes",
            "No environment management"
        ]
    },
    {
        "q": "Match the DataOps practice with its benefit:",
        "type": "match",
        "left": [
            "Version control",
            "CI/CD pipelines",
            "Automated testing",
            "Environment parity"
        ],
        "right": [
            "Change tracking",
            "Automated deployment",
            "Quality assurance",
            "Consistent behavior"
        ]
    },
    {
        "q": "Rearrange the DataOps maturity stages:",
        "type": "rearrange",
        "words": [
            "Manual Processes",
            "Basic Automation",
            "CI/CD Pipelines",
            "Full GitOps",
            "Self-service Platform"
        ]
    },
    {
        "q": "Drift detection identifies unauthorized changes to warehouse configurations.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this data contract enforcement implement?",
        "type": "mcq",
        "c": "CONTRACT orders_contract:\n  SCHEMA:\n    order_id: BIGINT NOT NULL\n    amount: DECIMAL(10,2) CHECK amount > 0\n    status: STRING IN ['pending', 'completed', 'cancelled']\n  SLA:\n    freshness: < 1 hour\n    completeness: >= 99.5%\n  ON_VIOLATION:\n    BLOCK_PUBLISH AND ALERT",
        "o": [
            "Schema validation and quality enforcement with blocking on violations",
            "Optional documentation only",
            "No enforcement mechanism",
            "Manual review process"
        ]
    },
    {
        "q": "Data _____ testing validates schema compatibility between versions.",
        "type": "fill_blank",
        "answers": [
            "contract"
        ],
        "other_options": [
            "unit",
            "integration",
            "performance"
        ]
    },
    {
        "q": "Which approach ensures backward compatibility in data schema changes?",
        "type": "mcq",
        "o": [
            "Semantic versioning with compatibility rules",
            "Breaking changes without notice",
            "No version control",
            "Random schema updates"
        ]
    },
    {
        "q": "Match the schema change type with its compatibility status:",
        "type": "match",
        "left": [
            "Add optional column",
            "Remove column",
            "Change type",
            "Rename column"
        ],
        "right": [
            "Backward compatible",
            "Breaking change",
            "Requires migration",
            "Alias mapping needed"
        ]
    },
    {
        "q": "Rearrange the data contract adoption stages:",
        "type": "rearrange",
        "words": [
            "Define Schema",
            "Add Quality Rules",
            "Implement Enforcement",
            "Monitor Compliance",
            "Iterate"
        ]
    },
    {
        "q": "Contract-first development defines data interfaces before implementation.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this multi-tenant architecture implement?",
        "type": "mcq",
        "c": "CREATE DATABASE tenant_${tenant_id}\nCREATE SCHEMA shared_analytics\nCREATE ROW POLICY tenant_isolation\n  ON all_tables\n  FILTER tenant_id = CURRENT_TENANT()\n\n-- Each tenant sees only their data",
        "o": [
            "Isolated tenant data with shared infrastructure",
            "Single tenant deployment",
            "No data isolation",
            "Manual data separation"
        ]
    },
    {
        "q": "_____ isolation uses row-level filters to separate tenant data.",
        "type": "fill_blank",
        "answers": [
            "Logical"
        ],
        "other_options": [
            "Physical",
            "Complete",
            "Manual"
        ]
    },
    {
        "q": "Which multi-tenancy model provides the strongest data isolation?",
        "type": "mcq",
        "o": [
            "Separate databases per tenant",
            "Shared tables with tenant column",
            "No isolation",
            "Shared schemas"
        ]
    },
    {
        "q": "Match the multi-tenant model with its tradeoff:",
        "type": "match",
        "left": [
            "Separate database",
            "Separate schema",
            "Shared table",
            "Shared schema"
        ],
        "right": [
            "Highest isolation cost",
            "Moderate isolation",
            "Lowest cost",
            "Simple queries"
        ]
    },
    {
        "q": "Tenant-aware caching prevents data leakage between organizations.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What optimization does this distributed query achieve?",
        "type": "mcq",
        "c": "SELECT /*+ COLOCATED_JOIN */\n  o.order_id, c.customer_name\nFROM orders o\nJOIN customers c ON o.customer_id = c.id\nWHERE o.region = c.region\n\n-- Data is partitioned by region on both tables",
        "o": [
            "Local join execution without data shuffling",
            "Full data redistribution",
            "Sequential processing",
            "No optimization"
        ]
    },
    {
        "q": "_____ joins execute locally when data is partitioned on the join key.",
        "type": "fill_blank",
        "answers": [
            "Collocated"
        ],
        "other_options": [
            "Distributed",
            "Remote",
            "Shuffled"
        ]
    },
    {
        "q": "Which data organization enables network-free distributed joins?",
        "type": "mcq",
        "o": [
            "Co-partitioning tables on join keys",
            "Random data distribution",
            "No partitioning",
            "Single-node storage"
        ]
    },
    {
        "q": "Match the distributed processing optimization with its benefit:",
        "type": "match",
        "left": [
            "Partition alignment",
            "Broadcast table",
            "Local aggregation",
            "Predicate pushdown"
        ],
        "right": [
            "Avoid shuffles",
            "Small table replication",
            "Reduce transfer",
            "Early filtering"
        ]
    },
    {
        "q": "Rearrange the distributed query optimization priorities:",
        "type": "rearrange",
        "words": [
            "Minimize Data Movement",
            "Parallel Execution",
            "Local Processing",
            "Result Aggregation"
        ]
    },
    {
        "q": "Exchange operators in query plans represent data redistribution.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this data versioning pattern enable?",
        "type": "mcq",
        "c": "SELECT * FROM orders VERSION AS OF 1234567890\nSELECT * FROM orders VERSION AS OF '2024-01-15 10:00:00'\nSELECT * FROM orders@v2.1.0\n\n-- Query any historical version of the data",
        "o": [
            "Point-in-time queries across any historical version",
            "Only current data access",
            "Delete historical versions",
            "No versioning support"
        ]
    },
    {
        "q": "Data _____ enables reproducible analytics by preserving historical states.",
        "type": "fill_blank",
        "answers": [
            "versioning"
        ],
        "other_options": [
            "compression",
            "encryption",
            "partitioning"
        ]
    },
    {
        "q": "Which feature enables debugging pipelines with historical data?",
        "type": "mcq",
        "o": [
            "Time travel queries to reproduce past states",
            "Current data only",
            "Log file analysis",
            "Manual data restoration"
        ]
    },
    {
        "q": "Match the versioning use case with its benefit:",
        "type": "match",
        "left": [
            "Audit compliance",
            "Pipeline debugging",
            "ML reproducibility",
            "Rollback recovery"
        ],
        "right": [
            "Point-in-time reports",
            "Historical data access",
            "Training data snapshots",
            "Undo bad changes"
        ]
    },
    {
        "q": "Retention policies automatically expire old data versions.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this AI-powered optimization implement?",
        "type": "mcq",
        "c": "AUTONOMOUS_OPTIMIZATION:\n  ANALYZE query_patterns\n  RECOMMEND:\n    - Create index on (customer_id, order_date)\n    - Add clustering on region\n    - Materialize frequently joined subquery\n  AUTO_APPLY: cost_savings > $1000/month",
        "o": [
            "ML-driven automatic performance tuning based on query patterns",
            "Manual index creation",
            "No optimization",
            "Static configuration"
        ]
    },
    {
        "q": "_____ tuning uses machine learning to optimize database performance.",
        "type": "fill_blank",
        "answers": [
            "Autonomous"
        ],
        "other_options": [
            "Manual",
            "Static",
            "Scheduled"
        ]
    },
    {
        "q": "Which capability distinguishes autonomous from traditional databases?",
        "type": "mcq",
        "o": [
            "Self-tuning based on workload analysis",
            "Manual DBA intervention required",
            "Fixed configurations",
            "No performance optimization"
        ]
    },
    {
        "q": "Match the autonomous capability with its optimization area:",
        "type": "match",
        "left": [
            "Auto-indexing",
            "Auto-clustering",
            "Auto-scaling",
            "Auto-tiering"
        ],
        "right": [
            "Query acceleration",
            "Data layout",
            "Resource sizing",
            "Storage optimization"
        ]
    },
    {
        "q": "Rearrange the autonomous database evolution stages:",
        "type": "rearrange",
        "words": [
            "Self-monitoring",
            "Self-diagnosing",
            "Self-tuning",
            "Self-healing",
            "Self-securing"
        ]
    },
    {
        "q": "Workload management automation adjusts resources based on demand patterns.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What compliance requirement does this data residency implement?",
        "type": "mcq",
        "c": "REGION_POLICY eu_data:\n  STORAGE_LOCATION: eu-west-1\n  PROCESSING_LOCATION: eu-*\n  REPLICATION: eu-central-1 (disaster recovery)\n  BLOCKED_REGIONS: us-*, asia-*\n  AUDIT_LOG: enabled",
        "o": [
            "Geographic data sovereignty with processing restrictions",
            "Global data replication",
            "No location restrictions",
            "Manual data placement"
        ]
    },
    {
        "q": "Data _____ requirements restrict where data can be stored and processed.",
        "type": "fill_blank",
        "answers": [
            "residency"
        ],
        "other_options": [
            "compression",
            "encryption",
            "versioning"
        ]
    },
    {
        "q": "Which regulation mandates specific geographic data storage?",
        "type": "mcq",
        "o": [
            "GDPR for EU personal data",
            "No geographic requirements",
            "Global storage is always allowed",
            "Data can be anywhere"
        ]
    },
    {
        "q": "Match the compliance requirement with its data handling:",
        "type": "match",
        "left": [
            "Data residency",
            "Data sovereignty",
            "Right to erasure",
            "Breach notification"
        ],
        "right": [
            "Geographic storage",
            "Local jurisdiction",
            "Delete on request",
            "Timely disclosure"
        ]
    },
    {
        "q": "Cross-border data transfers require legal mechanisms like Standard Contractual Clauses.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this real-time CDC streaming implement?",
        "type": "mcq",
        "c": "STREAMING_PIPELINE cdc_to_lakehouse:\n  SOURCE: debezium.postgres.orders\n  TRANSFORM:\n    - FLATTEN nested_json\n    - DEDUPE by primary_key ORDER BY _timestamp\n    - APPLY slowly_changing_dimension TYPE 2\n  TARGET: delta_lake.silver.orders\n  CHECKPOINT: every 1 minute",
        "o": [
            "Continuous data synchronization with SCD Type 2 history tracking",
            "Batch daily loads",
            "Manual data copying",
            "No change tracking"
        ]
    },
    {
        "q": "_____ CDC captures changes with minimal latency from source systems.",
        "type": "fill_blank",
        "answers": [
            "Streaming"
        ],
        "other_options": [
            "Batch",
            "Manual",
            "Scheduled"
        ]
    },
    {
        "q": "Which CDC implementation has the lowest source system impact?",
        "type": "mcq",
        "o": [
            "Log-based CDC reading transaction logs",
            "Trigger-based CDC",
            "Full table comparisons",
            "Application-level change tracking"
        ]
    },
    {
        "q": "Match the CDC component with its streaming function:",
        "type": "match",
        "left": [
            "Connector",
            "Topic",
            "Consumer",
            "Sink"
        ],
        "right": [
            "Capture changes",
            "Buffer events",
            "Transform data",
            "Write to target"
        ]
    },
    {
        "q": "Rearrange the CDC pipeline components by data flow:",
        "type": "rearrange",
        "words": [
            "Source Database",
            "CDC Connector",
            "Message Broker",
            "Stream Processor",
            "Target Storage"
        ]
    },
    {
        "q": "Exactly-once semantics prevent duplicate or lost records in streaming CDC.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does this unified analytics platform enable?",
        "type": "mcq",
        "c": "UNIFIED_PLATFORM:\n  DATA_INGESTION: batch, streaming, CDC\n  PROCESSING: SQL, Python, Spark, ML\n  STORAGE: lakehouse, warehouse, feature store\n  GOVERNANCE: catalog, lineage, access control\n  SERVING: BI, data science, applications",
        "o": [
            "End-to-end data platform for all analytics workloads",
            "Separate siloed tools",
            "BI-only platform",
            "Storage-only solution"
        ]
    },
    {
        "q": "Platform _____ integrates diverse data tools into a unified experience.",
        "type": "fill_blank",
        "answers": [
            "unification"
        ],
        "other_options": [
            "separation",
            "isolation",
            "fragmentation"
        ]
    },
    {
        "q": "Which benefit drives data platform consolidation?",
        "type": "mcq",
        "o": [
            "Reduced operational complexity and improved governance",
            "More tools to manage",
            "Increased data silos",
            "Higher integration costs"
        ]
    },
    {
        "q": "Match the platform capability with its value:",
        "type": "match",
        "left": [
            "Unified governance",
            "Shared compute",
            "Common catalog",
            "Integrated security"
        ],
        "right": [
            "Consistent policies",
            "Resource efficiency",
            "Data discovery",
            "Simplified access"
        ]
    },
    {
        "q": "Rearrange the data platform maturity levels:",
        "type": "rearrange",
        "words": [
            "Point Solutions",
            "Tool Integration",
            "Unified Platform",
            "Self-service Analytics",
            "AI-augmented Insights"
        ]
    },
    {
        "q": "Modern data platforms combine the best of data lakes and data warehouses.",
        "type": "true_false",
        "correct": "True"
    }
]
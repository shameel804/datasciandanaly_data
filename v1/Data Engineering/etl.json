[
    {
        "q": "What does ETL stand for?",
        "type": "mcq",
        "o": [
            "Extract, Transform, Load",
            "Export, Transfer, Load",
            "Extract, Transfer, Link",
            "Export, Transform, Link"
        ]
    },
    {
        "q": "Which phase of ETL involves pulling data from source systems?",
        "type": "mcq",
        "o": [
            "Extract",
            "Transform",
            "Load",
            "Validate"
        ]
    },
    {
        "q": "ETL processes are primarily used in data warehousing.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the main purpose of the Transform phase in ETL?",
        "type": "mcq",
        "o": [
            "Convert data into a suitable format",
            "Delete duplicate records",
            "Store data in the target system",
            "Extract data from sources"
        ]
    },
    {
        "q": "Which of the following is a common data source for ETL?",
        "type": "mcq",
        "o": [
            "Relational database",
            "Final report",
            "Dashboard",
            "User interface"
        ]
    },
    {
        "q": "The _____ phase moves processed data into the target database.",
        "type": "fill_blank",
        "answers": [
            "Load"
        ],
        "other_options": [
            "Extract",
            "Transform",
            "Validate"
        ]
    },
    {
        "q": "ETL can only work with structured data.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which type of extraction retrieves all data from the source?",
        "type": "mcq",
        "o": [
            "Full extraction",
            "Incremental extraction",
            "Partial extraction",
            "Delta extraction"
        ]
    },
    {
        "q": "What is a staging area in ETL?",
        "type": "mcq",
        "o": [
            "Temporary storage for data processing",
            "Final destination database",
            "Source database backup",
            "User interface component"
        ]
    },
    {
        "q": "Match the ETL phase with its function:",
        "type": "match",
        "left": [
            "Extract",
            "Transform",
            "Load",
            "Validate"
        ],
        "right": [
            "Pull data from sources",
            "Convert data format",
            "Store in target",
            "Check data quality"
        ]
    },
    {
        "q": "Which transformation removes duplicate records?",
        "type": "mcq",
        "o": [
            "Deduplication",
            "Aggregation",
            "Filtering",
            "Sorting"
        ]
    },
    {
        "q": "Data validation occurs only during the Extract phase.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What is data cleansing in ETL?",
        "type": "mcq",
        "o": [
            "Correcting errors and inconsistencies",
            "Deleting all data",
            "Backing up data",
            "Compressing data files"
        ]
    },
    {
        "q": "Which loading strategy completely replaces existing data?",
        "type": "mcq",
        "o": [
            "Full load",
            "Incremental load",
            "Delta load",
            "Streaming load"
        ]
    },
    {
        "q": "The _____ extraction method only retrieves changed data since the last run.",
        "type": "fill_blank",
        "answers": [
            "incremental"
        ],
        "other_options": [
            "full",
            "complete",
            "partial"
        ]
    },
    {
        "q": "ETL tools can schedule jobs to run automatically.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What does data profiling help identify?",
        "type": "mcq",
        "o": [
            "Data quality issues",
            "Network speed",
            "User permissions",
            "Storage capacity"
        ]
    },
    {
        "q": "Which of these is a popular ETL tool?",
        "type": "mcq",
        "o": [
            "Informatica PowerCenter",
            "Microsoft Word",
            "Adobe Photoshop",
            "Google Chrome"
        ]
    },
    {
        "q": "Rearrange the basic ETL workflow steps:",
        "type": "rearrange",
        "words": [
            "Extract",
            "Stage",
            "Transform",
            "Validate",
            "Load"
        ]
    },
    {
        "q": "What is the purpose of data mapping in ETL?",
        "type": "mcq",
        "o": [
            "Define how source fields correspond to target fields",
            "Create geographical maps",
            "Generate random data",
            "Delete old records"
        ]
    },
    {
        "q": "A lookup transformation retrieves values from a reference table.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which format is commonly used for flat file extraction?",
        "type": "mcq",
        "o": [
            "CSV",
            "MP3",
            "JPG",
            "EXE"
        ]
    },
    {
        "q": "What is the output of this Python ETL code?",
        "type": "mcq",
        "c": "data = ['apple', 'BANANA', 'Cherry']\nresult = [item.lower() for item in data]\nprint(result)",
        "o": [
            "['apple', 'banana', 'cherry']",
            "['APPLE', 'BANANA', 'CHERRY']",
            "['Apple', 'Banana', 'Cherry']",
            "Error"
        ]
    },
    {
        "q": "The _____ stores metadata about ETL processes.",
        "type": "fill_blank",
        "answers": [
            "repository"
        ],
        "other_options": [
            "database",
            "cache",
            "buffer"
        ]
    },
    {
        "q": "What is batch processing in ETL?",
        "type": "mcq",
        "o": [
            "Processing data in groups at scheduled times",
            "Processing one record at a time",
            "Real-time processing",
            "Manual data entry"
        ]
    },
    {
        "q": "ETL processes always run in real-time.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which database operation adds new records?",
        "type": "mcq",
        "o": [
            "INSERT",
            "UPDATE",
            "DELETE",
            "SELECT"
        ]
    },
    {
        "q": "What is the output of this transformation code?",
        "type": "mcq",
        "c": "value = '  Hello World  '\nresult = value.strip()\nprint(result)",
        "o": [
            "Hello World",
            "  Hello World  ",
            "HelloWorld",
            "hello world"
        ]
    },
    {
        "q": "Match the data quality dimension with its description:",
        "type": "match",
        "left": [
            "Accuracy",
            "Completeness",
            "Consistency",
            "Timeliness"
        ],
        "right": [
            "Data is correct",
            "No missing values",
            "Data matches across systems",
            "Data is up-to-date"
        ]
    },
    {
        "q": "Data truncation occurs when data is too long for the target field.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which SQL clause filters records during extraction?",
        "type": "mcq",
        "o": [
            "WHERE",
            "SELECT",
            "FROM",
            "INSERT"
        ]
    },
    {
        "q": "What does NULL represent in a database?",
        "type": "mcq",
        "o": [
            "Missing or unknown value",
            "Zero",
            "Empty string",
            "False"
        ]
    },
    {
        "q": "The _____ constraint ensures all values in a column are unique.",
        "type": "fill_blank",
        "answers": [
            "UNIQUE"
        ],
        "other_options": [
            "PRIMARY",
            "FOREIGN",
            "CHECK"
        ]
    },
    {
        "q": "What is the output of this data type conversion?",
        "type": "mcq",
        "c": "value = '123'\nresult = int(value) + 7\nprint(result)",
        "o": [
            "130",
            "1237",
            "123 + 7",
            "Error"
        ]
    },
    {
        "q": "Data normalization reduces data redundancy.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which connection type is used for database extraction?",
        "type": "mcq",
        "o": [
            "JDBC",
            "HTTP",
            "FTP",
            "SMTP"
        ]
    },
    {
        "q": "What is the purpose of error handling in ETL?",
        "type": "mcq",
        "o": [
            "Manage and log failed records",
            "Speed up processing",
            "Reduce storage space",
            "Generate reports"
        ]
    },
    {
        "q": "Rearrange the data validation steps:",
        "type": "rearrange",
        "words": [
            "Define rules",
            "Apply checks",
            "Log errors",
            "Report issues",
            "Fix data"
        ]
    },
    {
        "q": "What is the output of this null handling code?",
        "type": "mcq",
        "c": "value = None\nresult = value if value else 'N/A'\nprint(result)",
        "o": [
            "N/A",
            "None",
            "null",
            "Error"
        ]
    },
    {
        "q": "Primary keys can contain duplicate values.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which transformation combines data from multiple sources?",
        "type": "mcq",
        "o": [
            "Join",
            "Filter",
            "Sort",
            "Aggregate"
        ]
    },
    {
        "q": "The _____ key references a primary key in another table.",
        "type": "fill_blank",
        "answers": [
            "foreign"
        ],
        "other_options": [
            "primary",
            "unique",
            "composite"
        ]
    },
    {
        "q": "What is referential integrity?",
        "type": "mcq",
        "o": [
            "Ensuring foreign keys match primary keys",
            "Encrypting data",
            "Compressing files",
            "Backing up databases"
        ]
    },
    {
        "q": "Match the ETL tool with its vendor:",
        "type": "match",
        "left": [
            "SSIS",
            "DataStage",
            "Talend",
            "Informatica"
        ],
        "right": [
            "Microsoft",
            "IBM",
            "Open Source",
            "Informatica Corp"
        ]
    },
    {
        "q": "What is the output of this aggregation code?",
        "type": "mcq",
        "c": "numbers = [10, 20, 30, 40]\ntotal = sum(numbers)\nprint(total)",
        "o": [
            "100",
            "10",
            "40",
            "[10, 20, 30, 40]"
        ]
    },
    {
        "q": "ETL logging helps track process execution.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which operation updates existing records?",
        "type": "mcq",
        "o": [
            "UPDATE",
            "INSERT",
            "DELETE",
            "CREATE"
        ]
    },
    {
        "q": "What does SCD stand for in data warehousing?",
        "type": "mcq",
        "o": [
            "Slowly Changing Dimension",
            "System Configuration Data",
            "Source Control Database",
            "Structured Column Definition"
        ]
    },
    {
        "q": "The _____ loading method adds only new records without modifying existing ones.",
        "type": "fill_blank",
        "answers": [
            "append"
        ],
        "other_options": [
            "replace",
            "merge",
            "delete"
        ]
    },
    {
        "q": "What is the output of this filtering code?",
        "type": "mcq",
        "c": "data = [1, 2, 3, 4, 5]\nresult = [x for x in data if x > 3]\nprint(result)",
        "o": [
            "[4, 5]",
            "[1, 2, 3]",
            "[3, 4, 5]",
            "[1, 2]"
        ]
    },
    {
        "q": "Data warehouse is the typical target for ETL processes.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which format is XML?",
        "type": "mcq",
        "o": [
            "Extensible Markup Language",
            "Extra Massive Language",
            "External Model Language",
            "Execution Module Language"
        ]
    },
    {
        "q": "What is data aggregation?",
        "type": "mcq",
        "o": [
            "Summarizing data into groups",
            "Splitting data into parts",
            "Deleting data",
            "Encrypting data"
        ]
    },
    {
        "q": "Rearrange the incremental ETL steps:",
        "type": "rearrange",
        "words": [
            "Identify changes",
            "Extract delta",
            "Transform",
            "Merge",
            "Load"
        ]
    },
    {
        "q": "What is the output of this date parsing code?",
        "type": "mcq",
        "c": "from datetime import datetime\ndate_str = '2024-01-15'\ndt = datetime.strptime(date_str, '%Y-%m-%d')\nprint(dt.year)",
        "o": [
            "2024",
            "01",
            "15",
            "2024-01-15"
        ]
    },
    {
        "q": "The _____ phase checks if data meets quality standards.",
        "type": "fill_blank",
        "answers": [
            "validation"
        ],
        "other_options": [
            "extraction",
            "loading",
            "mapping"
        ]
    },
    {
        "q": "API extraction retrieves data through web services.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which HTTP method is commonly used to retrieve data from APIs?",
        "type": "mcq",
        "o": [
            "GET",
            "POST",
            "DELETE",
            "PUT"
        ]
    },
    {
        "q": "What is the output of this JSON parsing code?",
        "type": "mcq",
        "c": "import json\ndata = '{\"name\": \"John\", \"age\": 30}'\nobj = json.loads(data)\nprint(obj['name'])",
        "o": [
            "John",
            "30",
            "name",
            "{\"name\": \"John\"}"
        ]
    },
    {
        "q": "Match the transformation type with its purpose:",
        "type": "match",
        "left": [
            "Filter",
            "Sort",
            "Aggregate",
            "Join"
        ],
        "right": [
            "Remove unwanted rows",
            "Order data",
            "Summarize data",
            "Combine datasets"
        ]
    },
    {
        "q": "What is a data pipeline?",
        "type": "mcq",
        "o": [
            "Automated flow of data between systems",
            "Physical data cable",
            "Database table",
            "User interface"
        ]
    },
    {
        "q": "Parallel processing can speed up ETL jobs.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this string concatenation?",
        "type": "mcq",
        "c": "first = 'Data'\nlast = 'Engineering'\nresult = first + ' ' + last\nprint(result)",
        "o": [
            "Data Engineering",
            "DataEngineering",
            "Data + Engineering",
            "Error"
        ]
    },
    {
        "q": "Which clause orders SQL query results?",
        "type": "mcq",
        "o": [
            "ORDER BY",
            "GROUP BY",
            "WHERE",
            "HAVING"
        ]
    },
    {
        "q": "The _____ clause groups rows with the same values.",
        "type": "fill_blank",
        "answers": [
            "GROUP BY"
        ],
        "other_options": [
            "ORDER BY",
            "WHERE",
            "HAVING"
        ]
    },
    {
        "q": "What is the output of this splitting code?",
        "type": "mcq",
        "c": "text = 'apple,banana,cherry'\nparts = text.split(',')\nprint(parts[1])",
        "o": [
            "banana",
            "apple",
            "cherry",
            "apple,banana,cherry"
        ]
    },
    {
        "q": "Data masking hides sensitive information.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which transformation changes data types?",
        "type": "mcq",
        "o": [
            "Type casting",
            "Filtering",
            "Deduplication",
            "Aggregation"
        ]
    },
    {
        "q": "What is idempotency in ETL?",
        "type": "mcq",
        "o": [
            "Running the same job multiple times produces the same result",
            "Processing data in parallel",
            "Encrypting data",
            "Compressing files"
        ]
    },
    {
        "q": "Rearrange the data extraction methods by complexity (simplest first):",
        "type": "rearrange",
        "words": [
            "File copy",
            "Database query",
            "API call",
            "Web scraping",
            "Stream capture"
        ]
    },
    {
        "q": "What is the output of this dictionary access?",
        "type": "mcq",
        "c": "record = {'id': 1, 'name': 'Alice', 'city': 'NYC'}\nprint(record.get('city', 'Unknown'))",
        "o": [
            "NYC",
            "Unknown",
            "city",
            "Alice"
        ]
    },
    {
        "q": "The _____ operation combines rows from two tables based on a related column.",
        "type": "fill_blank",
        "answers": [
            "JOIN"
        ],
        "other_options": [
            "UNION",
            "INSERT",
            "SELECT"
        ]
    },
    {
        "q": "INNER JOIN returns all rows from both tables.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which JOIN type returns all rows from the left table?",
        "type": "mcq",
        "o": [
            "LEFT JOIN",
            "RIGHT JOIN",
            "INNER JOIN",
            "CROSS JOIN"
        ]
    },
    {
        "q": "What is the output of this list length check?",
        "type": "mcq",
        "c": "items = ['a', 'b', 'c', 'd', 'e']\ncount = len(items)\nprint(count)",
        "o": [
            "5",
            "4",
            "e",
            "['a', 'b', 'c', 'd', 'e']"
        ]
    },
    {
        "q": "Match the SQL function with its purpose:",
        "type": "match",
        "left": [
            "COUNT",
            "SUM",
            "AVG",
            "MAX"
        ],
        "right": [
            "Count rows",
            "Total values",
            "Average value",
            "Maximum value"
        ]
    },
    {
        "q": "What is data lineage?",
        "type": "mcq",
        "o": [
            "Tracking data origin and transformations",
            "Encrypting data",
            "Compressing files",
            "Deleting old records"
        ]
    },
    {
        "q": "Checkpoints help resume ETL jobs after failures.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this conditional transformation?",
        "type": "mcq",
        "c": "age = 25\ncategory = 'Adult' if age >= 18 else 'Minor'\nprint(category)",
        "o": [
            "Adult",
            "Minor",
            "25",
            "True"
        ]
    },
    {
        "q": "Which SQL aggregate function finds the lowest value?",
        "type": "mcq",
        "o": [
            "MIN",
            "MAX",
            "AVG",
            "SUM"
        ]
    },
    {
        "q": "The _____ function counts the number of rows in SQL.",
        "type": "fill_blank",
        "answers": [
            "COUNT"
        ],
        "other_options": [
            "SUM",
            "TOTAL",
            "NUMBER"
        ]
    },
    {
        "q": "What is the output of this rounding operation?",
        "type": "mcq",
        "c": "value = 3.7\nresult = round(value)\nprint(result)",
        "o": [
            "4",
            "3",
            "3.7",
            "3.0"
        ]
    },
    {
        "q": "Data profiling should be done before designing transformations.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which data format uses key-value pairs?",
        "type": "mcq",
        "o": [
            "JSON",
            "CSV",
            "TXT",
            "BIN"
        ]
    },
    {
        "q": "What is data enrichment?",
        "type": "mcq",
        "o": [
            "Adding additional information to existing data",
            "Deleting old records",
            "Compressing files",
            "Encrypting data"
        ]
    },
    {
        "q": "Rearrange the ETL testing phases:",
        "type": "rearrange",
        "words": [
            "Unit testing",
            "Integration testing",
            "Performance testing",
            "User acceptance"
        ]
    },
    {
        "q": "What is the output of this uppercase transformation?",
        "type": "mcq",
        "c": "text = 'hello world'\nresult = text.upper()\nprint(result)",
        "o": [
            "HELLO WORLD",
            "hello world",
            "Hello World",
            "Hello world"
        ]
    },
    {
        "q": "The _____ validates that row counts match between source and target.",
        "type": "fill_blank",
        "answers": [
            "reconciliation"
        ],
        "other_options": [
            "transformation",
            "extraction",
            "loading"
        ]
    },
    {
        "q": "TRUNCATE removes all rows from a table.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which SQL statement creates a new table?",
        "type": "mcq",
        "o": [
            "CREATE TABLE",
            "INSERT TABLE",
            "NEW TABLE",
            "ADD TABLE"
        ]
    },
    {
        "q": "What is the output of this replace operation?",
        "type": "mcq",
        "c": "text = 'Hello World'\nresult = text.replace('World', 'ETL')\nprint(result)",
        "o": [
            "Hello ETL",
            "Hello World",
            "World ETL",
            "ETL World"
        ]
    },
    {
        "q": "Match the loading strategy with its description:",
        "type": "match",
        "left": [
            "Full load",
            "Incremental",
            "Append",
            "Upsert"
        ],
        "right": [
            "Replace all data",
            "Load changes only",
            "Add new records",
            "Insert or update"
        ]
    },
    {
        "q": "What is an ETL workflow?",
        "type": "mcq",
        "o": [
            "Sequence of ETL tasks and dependencies",
            "Single database query",
            "User interface design",
            "Network configuration"
        ]
    },
    {
        "q": "Dependencies define the order of ETL task execution.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this boolean check?",
        "type": "mcq",
        "c": "value = 'ETL'\nis_empty = len(value) == 0\nprint(is_empty)",
        "o": [
            "False",
            "True",
            "0",
            "ETL"
        ]
    },
    {
        "q": "What is Change Data Capture (CDC)?",
        "type": "mcq",
        "o": [
            "Technique to identify and track data changes",
            "Method to compress data files",
            "Tool for data visualization",
            "Process to encrypt sensitive data"
        ]
    },
    {
        "q": "CDC eliminates the need for full data extraction.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which method uses database transaction logs for CDC?",
        "type": "mcq",
        "o": [
            "Log-based CDC",
            "Trigger-based CDC",
            "Timestamp-based CDC",
            "Snapshot-based CDC"
        ]
    },
    {
        "q": "What is the output of this timestamp comparison?",
        "type": "mcq",
        "c": "from datetime import datetime\nlast_run = datetime(2024, 1, 1)\ncurrent = datetime(2024, 1, 15)\ndiff = (current - last_run).days\nprint(diff)",
        "o": [
            "14",
            "15",
            "1",
            "Error"
        ]
    },
    {
        "q": "The _____ column is commonly used to track when records are modified.",
        "type": "fill_blank",
        "answers": [
            "timestamp"
        ],
        "other_options": [
            "id",
            "name",
            "status"
        ]
    },
    {
        "q": "Match the CDC method with its characteristic:",
        "type": "match",
        "left": [
            "Log-based",
            "Trigger-based",
            "Timestamp-based",
            "Diff-based"
        ],
        "right": [
            "Reads transaction logs",
            "Uses database triggers",
            "Compares modification dates",
            "Compares full datasets"
        ]
    },
    {
        "q": "What is a surrogate key?",
        "type": "mcq",
        "o": [
            "System-generated unique identifier",
            "Business key from source",
            "Foreign key reference",
            "Composite key"
        ]
    },
    {
        "q": "Natural keys are always preferred over surrogate keys.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What is the output of this hash generation?",
        "type": "mcq",
        "c": "import hashlib\ndata = 'ETL Process'\nhash_value = hashlib.md5(data.encode()).hexdigest()[:8]\nprint(len(hash_value))",
        "o": [
            "8",
            "32",
            "16",
            "Error"
        ]
    },
    {
        "q": "Which SCD type overwrites old data with new values?",
        "type": "mcq",
        "o": [
            "Type 1",
            "Type 2",
            "Type 3",
            "Type 4"
        ]
    },
    {
        "q": "SCD Type 2 maintains history by creating new records.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "The _____ flag indicates if a record is the current version in SCD Type 2.",
        "type": "fill_blank",
        "answers": [
            "current"
        ],
        "other_options": [
            "active",
            "valid",
            "latest"
        ]
    },
    {
        "q": "What is the output of this date formatting?",
        "type": "mcq",
        "c": "from datetime import datetime\ndt = datetime(2024, 3, 15)\nformatted = dt.strftime('%Y%m%d')\nprint(formatted)",
        "o": [
            "20240315",
            "2024-03-15",
            "15-03-2024",
            "03/15/2024"
        ]
    },
    {
        "q": "Which metadata describes the structure of data?",
        "type": "mcq",
        "o": [
            "Technical metadata",
            "Business metadata",
            "Operational metadata",
            "Process metadata"
        ]
    },
    {
        "q": "Rearrange the SCD Type 2 update process:",
        "type": "rearrange",
        "words": [
            "Find existing",
            "Set end date",
            "Insert new row",
            "Set current flag",
            "Commit"
        ]
    },
    {
        "q": "What is data partitioning?",
        "type": "mcq",
        "o": [
            "Dividing data into smaller manageable chunks",
            "Combining multiple tables",
            "Encrypting sensitive columns",
            "Compressing data files"
        ]
    },
    {
        "q": "Partitioning can improve query performance.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this partition key generation?",
        "type": "mcq",
        "c": "from datetime import datetime\ndt = datetime(2024, 6, 15)\npartition_key = f'{dt.year}_{dt.month:02d}'\nprint(partition_key)",
        "o": [
            "2024_06",
            "2024-06",
            "202406",
            "06_2024"
        ]
    },
    {
        "q": "Match the partitioning strategy with its use case:",
        "type": "match",
        "left": [
            "Range",
            "List",
            "Hash",
            "Composite"
        ],
        "right": [
            "Date-based data",
            "Category-based data",
            "Even distribution",
            "Multiple columns"
        ]
    },
    {
        "q": "The _____ extraction pattern pulls data at regular intervals.",
        "type": "fill_blank",
        "answers": [
            "scheduled"
        ],
        "other_options": [
            "random",
            "manual",
            "continuous"
        ]
    },
    {
        "q": "What is parallel loading?",
        "type": "mcq",
        "o": [
            "Loading data using multiple concurrent processes",
            "Loading data one record at a time",
            "Loading data in reverse order",
            "Loading data without validation"
        ]
    },
    {
        "q": "What is the output of this parallel processing simulation?",
        "type": "mcq",
        "c": "partitions = ['p1', 'p2', 'p3', 'p4']\nresult = len(partitions)\nprint(f'Processing {result} partitions')",
        "o": [
            "Processing 4 partitions",
            "Processing p1 partitions",
            "Processing ['p1', 'p2', 'p3', 'p4'] partitions",
            "Error"
        ]
    },
    {
        "q": "Bulk loading is faster than row-by-row insertion.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which SQL command loads data from a file?",
        "type": "mcq",
        "o": [
            "BULK INSERT",
            "FILE INSERT",
            "LOAD DATA",
            "IMPORT FILE"
        ]
    },
    {
        "q": "What is the purpose of a control file in ETL?",
        "type": "mcq",
        "o": [
            "Define data format and loading rules",
            "Store actual data records",
            "Generate reports",
            "Encrypt data files"
        ]
    },
    {
        "q": "The _____ validates that source and target record counts match.",
        "type": "fill_blank",
        "answers": [
            "reconciliation"
        ],
        "other_options": [
            "transformation",
            "extraction",
            "mapping"
        ]
    },
    {
        "q": "What is the output of this record counting?",
        "type": "mcq",
        "c": "source_count = 1000\ntarget_count = 998\ndiff = source_count - target_count\nprint(f'Missing: {diff}')",
        "o": [
            "Missing: 2",
            "Missing: 1000",
            "Missing: 998",
            "Missing: -2"
        ]
    },
    {
        "q": "Data type mismatch is a common ETL error.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the validation type with its check:",
        "type": "match",
        "left": [
            "Format",
            "Range",
            "Uniqueness",
            "Referential"
        ],
        "right": [
            "Pattern matching",
            "Value boundaries",
            "No duplicates",
            "Foreign key exists"
        ]
    },
    {
        "q": "What is a reject file in ETL?",
        "type": "mcq",
        "o": [
            "File containing records that failed validation",
            "File with deleted records",
            "Backup of source data",
            "Configuration file"
        ]
    },
    {
        "q": "Rearrange the error handling workflow:",
        "type": "rearrange",
        "words": [
            "Detect error",
            "Log details",
            "Route to reject",
            "Notify team",
            "Retry or skip"
        ]
    },
    {
        "q": "What is the output of this error logging code?",
        "type": "mcq",
        "c": "errors = []\ntry:\n    value = int('abc')\nexcept ValueError as e:\n    errors.append(str(e))\nprint(len(errors))",
        "o": [
            "1",
            "0",
            "abc",
            "Error"
        ]
    },
    {
        "q": "The _____ pattern retries failed operations with increasing delays.",
        "type": "fill_blank",
        "answers": [
            "exponential backoff"
        ],
        "other_options": [
            "linear retry",
            "immediate retry",
            "single retry"
        ]
    },
    {
        "q": "What is data drift?",
        "type": "mcq",
        "o": [
            "Gradual changes in data structure or values over time",
            "Data moving between systems",
            "Data compression",
            "Data encryption"
        ]
    },
    {
        "q": "Schema validation catches structural changes in source data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this schema check?",
        "type": "mcq",
        "c": "expected_cols = ['id', 'name', 'date']\nactual_cols = ['id', 'name', 'date', 'status']\nextra = set(actual_cols) - set(expected_cols)\nprint(list(extra))",
        "o": [
            "['status']",
            "['id', 'name', 'date']",
            "[]",
            "Error"
        ]
    },
    {
        "q": "Which technique handles missing values by filling with defaults?",
        "type": "mcq",
        "o": [
            "Imputation",
            "Deletion",
            "Aggregation",
            "Normalization"
        ]
    },
    {
        "q": "Match the null handling strategy with its approach:",
        "type": "match",
        "left": [
            "Drop",
            "Fill constant",
            "Fill mean",
            "Forward fill"
        ],
        "right": [
            "Remove null rows",
            "Use fixed value",
            "Use average",
            "Use previous value"
        ]
    },
    {
        "q": "What is the output of this null replacement?",
        "type": "mcq",
        "c": "data = [1, None, 3, None, 5]\nresult = [x if x else 0 for x in data]\nprint(result)",
        "o": [
            "[1, 0, 3, 0, 5]",
            "[1, None, 3, None, 5]",
            "[1, 3, 5]",
            "Error"
        ]
    },
    {
        "q": "The _____ removes leading and trailing whitespace from strings.",
        "type": "fill_blank",
        "answers": [
            "trim"
        ],
        "other_options": [
            "cut",
            "remove",
            "delete"
        ]
    },
    {
        "q": "Data standardization converts values to a common format.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this standardization?",
        "type": "mcq",
        "c": "phone = '(123) 456-7890'\nclean = ''.join(c for c in phone if c.isdigit())\nprint(clean)",
        "o": [
            "1234567890",
            "(123) 456-7890",
            "123-456-7890",
            "Error"
        ]
    },
    {
        "q": "Which transformation converts text to a consistent case?",
        "type": "mcq",
        "o": [
            "Case normalization",
            "Data encryption",
            "Compression",
            "Aggregation"
        ]
    },
    {
        "q": "Rearrange the data cleaning steps:",
        "type": "rearrange",
        "words": [
            "Profile data",
            "Handle nulls",
            "Standardize",
            "Validate",
            "Document"
        ]
    },
    {
        "q": "What is the output of this case normalization?",
        "type": "mcq",
        "c": "names = ['JOHN', 'jane', 'BoB']\nresult = [n.title() for n in names]\nprint(result)",
        "o": [
            "['John', 'Jane', 'Bob']",
            "['JOHN', 'JANE', 'BOB']",
            "['john', 'jane', 'bob']",
            "Error"
        ]
    },
    {
        "q": "Match the data quality metric with its measurement:",
        "type": "match",
        "left": [
            "Completeness",
            "Accuracy",
            "Consistency",
            "Validity"
        ],
        "right": [
            "Percent non-null",
            "Correct values",
            "Same across systems",
            "Meets format rules"
        ]
    },
    {
        "q": "The _____ identifies patterns and anomalies in source data.",
        "type": "fill_blank",
        "answers": [
            "profiling"
        ],
        "other_options": [
            "loading",
            "extracting",
            "mapping"
        ]
    },
    {
        "q": "What is a lookup table?",
        "type": "mcq",
        "o": [
            "Reference table for translating codes to descriptions",
            "Main transaction table",
            "Temporary staging table",
            "Error log table"
        ]
    },
    {
        "q": "Lookup transformations can slow down ETL if not indexed.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this lookup simulation?",
        "type": "mcq",
        "c": "lookup = {'US': 'United States', 'UK': 'United Kingdom'}\ncode = 'US'\nresult = lookup.get(code, 'Unknown')\nprint(result)",
        "o": [
            "United States",
            "US",
            "Unknown",
            "Error"
        ]
    },
    {
        "q": "Which join returns only matching records from both tables?",
        "type": "mcq",
        "o": [
            "INNER JOIN",
            "LEFT JOIN",
            "RIGHT JOIN",
            "FULL JOIN"
        ]
    },
    {
        "q": "A FULL OUTER JOIN includes all records from both tables.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this merge operation?",
        "type": "mcq",
        "c": "set1 = {1, 2, 3}\nset2 = {2, 3, 4}\nresult = set1.intersection(set2)\nprint(result)",
        "o": [
            "{2, 3}",
            "{1, 2, 3, 4}",
            "{1, 4}",
            "{1, 2, 3}"
        ]
    },
    {
        "q": "The _____ combines two datasets with the same structure vertically.",
        "type": "fill_blank",
        "answers": [
            "UNION"
        ],
        "other_options": [
            "JOIN",
            "MERGE",
            "CROSS"
        ]
    },
    {
        "q": "Match the join type with its result:",
        "type": "match",
        "left": [
            "INNER",
            "LEFT",
            "RIGHT",
            "CROSS"
        ],
        "right": [
            "Only matches",
            "All left plus matches",
            "All right plus matches",
            "Cartesian product"
        ]
    },
    {
        "q": "What is a fact table in dimensional modeling?",
        "type": "mcq",
        "o": [
            "Table containing business metrics and measures",
            "Table with descriptive attributes",
            "Temporary staging table",
            "Configuration table"
        ]
    },
    {
        "q": "Dimension tables contain descriptive attributes.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this aggregation?",
        "type": "mcq",
        "c": "sales = [100, 200, 150, 250]\navg_sale = sum(sales) / len(sales)\nprint(avg_sale)",
        "o": [
            "175.0",
            "700",
            "100",
            "4"
        ]
    },
    {
        "q": "Rearrange the dimensional modeling steps:",
        "type": "rearrange",
        "words": [
            "Identify facts",
            "Define dimensions",
            "Design schema",
            "Map sources",
            "Validate"
        ]
    },
    {
        "q": "Which schema type has a central fact table surrounded by dimensions?",
        "type": "mcq",
        "o": [
            "Star schema",
            "Snowflake schema",
            "Galaxy schema",
            "Flat schema"
        ]
    },
    {
        "q": "The _____ schema normalizes dimension tables into sub-dimensions.",
        "type": "fill_blank",
        "answers": [
            "snowflake"
        ],
        "other_options": [
            "star",
            "galaxy",
            "flat"
        ]
    },
    {
        "q": "What is the output of this grouping operation?",
        "type": "mcq",
        "c": "from collections import Counter\ndata = ['A', 'B', 'A', 'C', 'A', 'B']\ncounts = Counter(data)\nprint(counts['A'])",
        "o": [
            "3",
            "2",
            "1",
            "6"
        ]
    },
    {
        "q": "Match the dimensional concept with its description:",
        "type": "match",
        "left": [
            "Fact",
            "Dimension",
            "Grain",
            "Hierarchy"
        ],
        "right": [
            "Measures",
            "Descriptors",
            "Level of detail",
            "Parent-child levels"
        ]
    },
    {
        "q": "What is grain in dimensional modeling?",
        "type": "mcq",
        "o": [
            "Level of detail represented by a fact row",
            "Size of the database",
            "Number of dimensions",
            "Type of aggregation"
        ]
    },
    {
        "q": "Conformed dimensions are shared across multiple fact tables.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this pivot simulation?",
        "type": "mcq",
        "c": "data = {'Q1': 100, 'Q2': 200, 'Q3': 150}\ntotal = sum(data.values())\nprint(total)",
        "o": [
            "450",
            "3",
            "Q1Q2Q3",
            "Error"
        ]
    },
    {
        "q": "Which ETL approach processes data as it arrives?",
        "type": "mcq",
        "o": [
            "Real-time ETL",
            "Batch ETL",
            "Micro-batch ETL",
            "Manual ETL"
        ]
    },
    {
        "q": "The _____ processes small batches of data at frequent intervals.",
        "type": "fill_blank",
        "answers": [
            "micro-batch"
        ],
        "other_options": [
            "batch",
            "real-time",
            "stream"
        ]
    },
    {
        "q": "Streaming ETL has lower latency than batch ETL.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this time window calculation?",
        "type": "mcq",
        "c": "window_minutes = 5\nrecords_per_minute = 100\ntotal = window_minutes * records_per_minute\nprint(f'{total} records')",
        "o": [
            "500 records",
            "5 records",
            "100 records",
            "Error"
        ]
    },
    {
        "q": "Match the processing type with its latency:",
        "type": "match",
        "left": [
            "Batch",
            "Micro-batch",
            "Near real-time",
            "Real-time"
        ],
        "right": [
            "Hours",
            "Minutes",
            "Seconds",
            "Milliseconds"
        ]
    },
    {
        "q": "What is data denormalization?",
        "type": "mcq",
        "o": [
            "Adding redundancy to improve query performance",
            "Removing all duplicates",
            "Encrypting data",
            "Compressing data"
        ]
    },
    {
        "q": "Rearrange the ETL monitoring priorities:",
        "type": "rearrange",
        "words": [
            "Job status",
            "Row counts",
            "Error rates",
            "Duration",
            "Resource usage"
        ]
    },
    {
        "q": "What is the output of this duration calculation?",
        "type": "mcq",
        "c": "from datetime import datetime, timedelta\nstart = datetime(2024, 1, 1, 10, 0)\nend = datetime(2024, 1, 1, 10, 45)\nduration = (end - start).seconds // 60\nprint(f'{duration} minutes')",
        "o": [
            "45 minutes",
            "10 minutes",
            "0 minutes",
            "Error"
        ]
    },
    {
        "q": "The _____ metric measures records processed per unit time.",
        "type": "fill_blank",
        "answers": [
            "throughput"
        ],
        "other_options": [
            "latency",
            "accuracy",
            "completeness"
        ]
    },
    {
        "q": "ETL audit trails help with compliance requirements.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is an ETL audit table?",
        "type": "mcq",
        "o": [
            "Table recording job execution details",
            "Table with business data",
            "Temporary staging table",
            "Configuration table"
        ]
    },
    {
        "q": "Match the audit field with its purpose:",
        "type": "match",
        "left": [
            "Job ID",
            "Start time",
            "Row count",
            "Status"
        ],
        "right": [
            "Unique identifier",
            "When job began",
            "Records processed",
            "Success or failure"
        ]
    },
    {
        "q": "What is the output of this status check?",
        "type": "mcq",
        "c": "job_status = 'COMPLETED'\nerror_count = 0\nis_success = job_status == 'COMPLETED' and error_count == 0\nprint(is_success)",
        "o": [
            "True",
            "False",
            "COMPLETED",
            "0"
        ]
    },
    {
        "q": "Which tool is used for ETL job scheduling?",
        "type": "mcq",
        "o": [
            "cron",
            "grep",
            "sed",
            "awk"
        ]
    },
    {
        "q": "The _____ expression '0 6 * * *' runs a job daily at 6 AM.",
        "type": "fill_blank",
        "answers": [
            "cron"
        ],
        "other_options": [
            "bash",
            "shell",
            "sql"
        ]
    },
    {
        "q": "Job dependencies ensure tasks run in correct order.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this dependency check?",
        "type": "mcq",
        "c": "dependencies = ['extract', 'transform']\ncompleted = ['extract']\nready = all(d in completed for d in dependencies)\nprint(ready)",
        "o": [
            "False",
            "True",
            "['extract']",
            "Error"
        ]
    },
    {
        "q": "Rearrange the job scheduling workflow:",
        "type": "rearrange",
        "words": [
            "Define schedule",
            "Set dependencies",
            "Configure alerts",
            "Deploy",
            "Monitor"
        ]
    },
    {
        "q": "What is data latency in ETL?",
        "type": "mcq",
        "o": [
            "Time delay between source update and target availability",
            "Size of data files",
            "Number of transformations",
            "Database connection speed"
        ]
    },
    {
        "q": "Match the scheduling interval with its frequency:",
        "type": "match",
        "left": [
            "Hourly",
            "Daily",
            "Weekly",
            "Monthly"
        ],
        "right": [
            "Every 60 minutes",
            "Once per day",
            "Once per week",
            "Once per month"
        ]
    },
    {
        "q": "What is the output of this interval calculation?",
        "type": "mcq",
        "c": "runs_per_day = 24\ndays = 7\ntotal_runs = runs_per_day * days\nprint(f'{total_runs} runs per week')",
        "o": [
            "168 runs per week",
            "24 runs per week",
            "7 runs per week",
            "31 runs per week"
        ]
    },
    {
        "q": "The _____ handles ETL failures by sending notifications.",
        "type": "fill_blank",
        "answers": [
            "alerting"
        ],
        "other_options": [
            "logging",
            "scheduling",
            "mapping"
        ]
    },
    {
        "q": "Dead letter queues store failed messages for later processing.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this retry logic?",
        "type": "mcq",
        "c": "max_retries = 3\nattempts = 0\nwhile attempts < max_retries:\n    attempts += 1\nprint(f'Attempted {attempts} times')",
        "o": [
            "Attempted 3 times",
            "Attempted 0 times",
            "Attempted 1 times",
            "Error"
        ]
    },
    {
        "q": "Which pattern ensures exactly-once processing?",
        "type": "mcq",
        "o": [
            "Idempotency",
            "Parallelism",
            "Caching",
            "Compression"
        ]
    },
    {
        "q": "Match the recovery strategy with its action:",
        "type": "match",
        "left": [
            "Retry",
            "Skip",
            "Abort",
            "Manual"
        ],
        "right": [
            "Try again",
            "Ignore and continue",
            "Stop processing",
            "Human intervention"
        ]
    },
    {
        "q": "What is the output of this file validation?",
        "type": "mcq",
        "c": "import os\nfile_path = '/data/input.csv'\nexists = os.path.exists(file_path) if False else 'Simulated'\nprint(exists)",
        "o": [
            "Simulated",
            "True",
            "False",
            "/data/input.csv"
        ]
    },
    {
        "q": "The _____ table stores intermediate ETL results.",
        "type": "fill_blank",
        "answers": [
            "staging"
        ],
        "other_options": [
            "fact",
            "dimension",
            "audit"
        ]
    },
    {
        "q": "Staging tables are typically truncated before each ETL run.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this table naming convention?",
        "type": "mcq",
        "c": "source = 'customers'\nprefix = 'stg_'\ntable_name = f'{prefix}{source}'\nprint(table_name)",
        "o": [
            "stg_customers",
            "customers_stg",
            "customersprefix",
            "Error"
        ]
    },
    {
        "q": "Rearrange the staging table lifecycle:",
        "type": "rearrange",
        "words": [
            "Truncate",
            "Load raw",
            "Transform",
            "Validate",
            "Move to target"
        ]
    },
    {
        "q": "What is the output of this metadata extraction?",
        "type": "mcq",
        "c": "import json\nschema = {'columns': ['id', 'name', 'date'], 'types': ['int', 'str', 'date']}\ncolumn_count = len(schema['columns'])\nprint(f'{column_count} columns')",
        "o": [
            "3 columns",
            "2 columns",
            "{'columns': ['id', 'name', 'date']}",
            "Error"
        ]
    },
    {
        "q": "Which ETL pattern extracts data to a data lake before transformation?",
        "type": "mcq",
        "o": [
            "ELT",
            "ETL",
            "ETLT",
            "TEL"
        ]
    },
    {
        "q": "ELT leverages the processing power of the target system.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the ETL pattern with its characteristic:",
        "type": "match",
        "left": [
            "ETL",
            "ELT",
            "ETLT",
            "Streaming"
        ],
        "right": [
            "Transform before load",
            "Transform after load",
            "Hybrid approach",
            "Continuous processing"
        ]
    },
    {
        "q": "The _____ approach is preferred when the target system has strong compute capabilities.",
        "type": "fill_blank",
        "answers": [
            "ELT"
        ],
        "other_options": [
            "ETL",
            "ETLT",
            "batch"
        ]
    },
    {
        "q": "What is the output of this SQL window function simulation?",
        "type": "mcq",
        "c": "values = [100, 200, 150, 300]\nrunning_sum = [sum(values[:i+1]) for i in range(len(values))]\nprint(running_sum[-1])",
        "o": [
            "750",
            "300",
            "[100, 300, 450, 750]",
            "Error"
        ]
    },
    {
        "q": "Which technique identifies matching records across multiple sources?",
        "type": "mcq",
        "o": [
            "Record linkage",
            "Data compression",
            "Data encryption",
            "Data deletion"
        ]
    },
    {
        "q": "Fuzzy matching allows for approximate string comparisons.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this fuzzy matching simulation?",
        "type": "mcq",
        "c": "name1 = 'John Smith'\nname2 = 'Jon Smith'\nmatching_chars = sum(a == b for a, b in zip(name1, name2))\nprint(matching_chars)",
        "o": [
            "8",
            "10",
            "9",
            "0"
        ]
    },
    {
        "q": "Rearrange the master data management process:",
        "type": "rearrange",
        "words": [
            "Identify sources",
            "Define golden record",
            "Match records",
            "Merge data",
            "Publish master"
        ]
    },
    {
        "q": "What is a golden record?",
        "type": "mcq",
        "o": [
            "Single authoritative version of a data entity",
            "Encrypted backup copy",
            "Compressed data file",
            "Temporary staging record"
        ]
    },
    {
        "q": "The _____ algorithm calculates edit distance between strings.",
        "type": "fill_blank",
        "answers": [
            "Levenshtein"
        ],
        "other_options": [
            "Binary",
            "Linear",
            "Quicksort"
        ]
    },
    {
        "q": "What is the output of this deduplication logic?",
        "type": "mcq",
        "c": "records = ['A', 'B', 'A', 'C', 'B', 'D']\nunique = list(dict.fromkeys(records))\nprint(len(unique))",
        "o": [
            "4",
            "6",
            "3",
            "2"
        ]
    },
    {
        "q": "Match the data quality rule with its implementation:",
        "type": "match",
        "left": [
            "Format mask",
            "Range check",
            "Lookup validation",
            "Cross-field"
        ],
        "right": [
            "Pattern matching",
            "Min-max bounds",
            "Reference table",
            "Dependent fields"
        ]
    },
    {
        "q": "Data quality rules can be applied during extraction.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this format validation?",
        "type": "mcq",
        "c": "import re\nemail = 'user@example.com'\npattern = r'^[\\w.-]+@[\\w.-]+\\.\\w+$'\nis_valid = bool(re.match(pattern, email))\nprint(is_valid)",
        "o": [
            "True",
            "False",
            "user@example.com",
            "Error"
        ]
    },
    {
        "q": "Which transformation pivots rows into columns?",
        "type": "mcq",
        "o": [
            "Pivot",
            "Unpivot",
            "Transpose",
            "Rotate"
        ]
    },
    {
        "q": "The _____ transformation converts columns back to rows.",
        "type": "fill_blank",
        "answers": [
            "unpivot"
        ],
        "other_options": [
            "pivot",
            "rotate",
            "flip"
        ]
    },
    {
        "q": "What is the output of this transpose simulation?",
        "type": "mcq",
        "c": "data = {'A': [1, 2], 'B': [3, 4]}\nkeys = list(data.keys())\nprint(keys)",
        "o": [
            "['A', 'B']",
            "[1, 2, 3, 4]",
            "{'A': [1, 2], 'B': [3, 4]}",
            "Error"
        ]
    },
    {
        "q": "Rearrange the data quality assessment steps:",
        "type": "rearrange",
        "words": [
            "Define metrics",
            "Profile data",
            "Identify issues",
            "Remediate",
            "Monitor"
        ]
    },
    {
        "q": "What is data provenance?",
        "type": "mcq",
        "o": [
            "Documentation of data origin and history",
            "Data compression technique",
            "Encryption method",
            "Storage format"
        ]
    },
    {
        "q": "Match the data lineage level with its scope:",
        "type": "match",
        "left": [
            "Column-level",
            "Table-level",
            "Process-level",
            "System-level"
        ],
        "right": [
            "Individual fields",
            "Full tables",
            "ETL jobs",
            "Entire pipeline"
        ]
    },
    {
        "q": "Impact analysis uses lineage to identify downstream effects.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this lineage tracking?",
        "type": "mcq",
        "c": "lineage = {'target': 'sales_fact', 'sources': ['orders', 'products', 'customers']}\nsource_count = len(lineage['sources'])\nprint(f'{source_count} source tables')",
        "o": [
            "3 source tables",
            "1 source tables",
            "sales_fact",
            "Error"
        ]
    },
    {
        "q": "Which technique detects outliers in numerical data?",
        "type": "mcq",
        "o": [
            "Statistical analysis",
            "String matching",
            "Date parsing",
            "File compression"
        ]
    },
    {
        "q": "The _____ method uses standard deviation to identify outliers.",
        "type": "fill_blank",
        "answers": [
            "z-score"
        ],
        "other_options": [
            "median",
            "mode",
            "range"
        ]
    },
    {
        "q": "What is the output of this outlier detection?",
        "type": "mcq",
        "c": "import statistics\ndata = [10, 12, 11, 13, 100, 12, 11]\nmean = statistics.mean(data)\nprint(round(mean, 1))",
        "o": [
            "24.1",
            "12.0",
            "100",
            "11"
        ]
    },
    {
        "q": "Data profiling should identify value distributions.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the profiling metric with its purpose:",
        "type": "match",
        "left": [
            "Cardinality",
            "Nullability",
            "Distribution",
            "Patterns"
        ],
        "right": [
            "Unique values",
            "Missing values",
            "Value frequency",
            "Data formats"
        ]
    },
    {
        "q": "What is the output of this cardinality calculation?",
        "type": "mcq",
        "c": "column = ['A', 'B', 'A', 'C', 'B', 'A', 'D']\ncardinality = len(set(column))\nprint(f'Cardinality: {cardinality}')",
        "o": [
            "Cardinality: 4",
            "Cardinality: 7",
            "Cardinality: 3",
            "Error"
        ]
    },
    {
        "q": "Rearrange the ETL development lifecycle:",
        "type": "rearrange",
        "words": [
            "Requirements",
            "Design",
            "Develop",
            "Test",
            "Deploy"
        ]
    },
    {
        "q": "Which testing type verifies data accuracy?",
        "type": "mcq",
        "o": [
            "Data validation testing",
            "Performance testing",
            "Security testing",
            "Usability testing"
        ]
    },
    {
        "q": "The _____ test compares source and target data values.",
        "type": "fill_blank",
        "answers": [
            "reconciliation"
        ],
        "other_options": [
            "unit",
            "integration",
            "system"
        ]
    },
    {
        "q": "What is the output of this test assertion?",
        "type": "mcq",
        "c": "source_sum = 10000\ntarget_sum = 10000\nmatch = source_sum == target_sum\nprint(f'Totals match: {match}')",
        "o": [
            "Totals match: True",
            "Totals match: False",
            "10000",
            "Error"
        ]
    },
    {
        "q": "Regression testing ensures new changes do not break existing functionality.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the test type with its focus:",
        "type": "match",
        "left": [
            "Unit",
            "Integration",
            "System",
            "UAT"
        ],
        "right": [
            "Single component",
            "Component interaction",
            "Full pipeline",
            "Business requirements"
        ]
    },
    {
        "q": "What is the output of this test case generation?",
        "type": "mcq",
        "c": "test_cases = ['null_handling', 'duplicates', 'data_types', 'boundaries']\ntotal = len(test_cases)\nprint(f'{total} test cases defined')",
        "o": [
            "4 test cases defined",
            "0 test cases defined",
            "test_cases",
            "Error"
        ]
    },
    {
        "q": "Which transformation handles slowly changing dimensions?",
        "type": "mcq",
        "o": [
            "SCD component",
            "Filter component",
            "Sort component",
            "Aggregate component"
        ]
    },
    {
        "q": "SCD Type 3 stores current and previous values in separate columns.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "The _____ date marks when a record version became inactive.",
        "type": "fill_blank",
        "answers": [
            "end"
        ],
        "other_options": [
            "start",
            "create",
            "modify"
        ]
    },
    {
        "q": "What is the output of this SCD date calculation?",
        "type": "mcq",
        "c": "from datetime import datetime, timedelta\nstart_date = datetime(2024, 1, 1)\ndays_active = 90\nend_date = start_date + timedelta(days=days_active)\nprint(end_date.strftime('%Y-%m-%d'))",
        "o": [
            "2024-03-31",
            "2024-01-01",
            "2024-04-01",
            "Error"
        ]
    },
    {
        "q": "Rearrange the incremental load validation steps:",
        "type": "rearrange",
        "words": [
            "Get watermark",
            "Extract delta",
            "Validate counts",
            "Load target",
            "Update watermark"
        ]
    },
    {
        "q": "What is a watermark in incremental ETL?",
        "type": "mcq",
        "o": [
            "Marker indicating last processed point",
            "Data quality score",
            "Encryption key",
            "Compression ratio"
        ]
    },
    {
        "q": "Match the watermark type with its use:",
        "type": "match",
        "left": [
            "Timestamp",
            "Sequence",
            "Hash",
            "Offset"
        ],
        "right": [
            "Date-based changes",
            "ID-based changes",
            "Content changes",
            "Position-based"
        ]
    },
    {
        "q": "High watermark indicates the maximum processed value.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this watermark update?",
        "type": "mcq",
        "c": "current_watermark = 1000\nnew_records = [1001, 1002, 1003, 1004, 1005]\nnew_watermark = max(new_records)\nprint(f'New watermark: {new_watermark}')",
        "o": [
            "New watermark: 1005",
            "New watermark: 1000",
            "New watermark: 1001",
            "Error"
        ]
    },
    {
        "q": "Which constraint ensures data integrity across tables?",
        "type": "mcq",
        "o": [
            "Foreign key",
            "Primary key",
            "Unique key",
            "Check constraint"
        ]
    },
    {
        "q": "The _____ constraint limits values to a predefined set.",
        "type": "fill_blank",
        "answers": [
            "check"
        ],
        "other_options": [
            "primary",
            "foreign",
            "unique"
        ]
    },
    {
        "q": "What is the output of this constraint validation?",
        "type": "mcq",
        "c": "allowed_statuses = ['active', 'inactive', 'pending']\nvalue = 'active'\nis_valid = value in allowed_statuses\nprint(is_valid)",
        "o": [
            "True",
            "False",
            "active",
            "Error"
        ]
    },
    {
        "q": "Referential integrity prevents orphan records.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the constraint with its enforcement:",
        "type": "match",
        "left": [
            "NOT NULL",
            "UNIQUE",
            "PRIMARY KEY",
            "FOREIGN KEY"
        ],
        "right": [
            "No missing values",
            "No duplicates",
            "Unique identifier",
            "Reference parent"
        ]
    },
    {
        "q": "What is the output of this null constraint check?",
        "type": "mcq",
        "c": "required_fields = ['id', 'name', 'date']\nrecord = {'id': 1, 'name': 'Test', 'date': None}\nmissing = [f for f in required_fields if record.get(f) is None]\nprint(missing)",
        "o": [
            "['date']",
            "[]",
            "['id', 'name', 'date']",
            "Error"
        ]
    },
    {
        "q": "Rearrange the data archival process:",
        "type": "rearrange",
        "words": [
            "Define policy",
            "Identify data",
            "Archive records",
            "Verify archive",
            "Delete source"
        ]
    },
    {
        "q": "What is data archival in ETL?",
        "type": "mcq",
        "o": [
            "Moving historical data to long-term storage",
            "Deleting all old data",
            "Encrypting sensitive data",
            "Compressing active data"
        ]
    },
    {
        "q": "The _____ determines how long data should be retained.",
        "type": "fill_blank",
        "answers": [
            "retention policy"
        ],
        "other_options": [
            "backup policy",
            "security policy",
            "access policy"
        ]
    },
    {
        "q": "What is the output of this retention calculation?",
        "type": "mcq",
        "c": "from datetime import datetime, timedelta\nrecord_date = datetime(2020, 6, 15)\nretention_years = 3\nexpiry_date = datetime(record_date.year + retention_years, record_date.month, record_date.day)\nprint(expiry_date.year)",
        "o": [
            "2023",
            "2020",
            "2017",
            "Error"
        ]
    },
    {
        "q": "Data purging permanently removes data from the system.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the data lifecycle stage with its action:",
        "type": "match",
        "left": [
            "Creation",
            "Active use",
            "Archive",
            "Purge"
        ],
        "right": [
            "Initial load",
            "Regular access",
            "Long-term storage",
            "Permanent deletion"
        ]
    },
    {
        "q": "What is the output of this age calculation?",
        "type": "mcq",
        "c": "from datetime import datetime\nrecord_date = datetime(2022, 1, 1)\ncurrent = datetime(2024, 7, 1)\nage_days = (current - record_date).days\nprint(f'{age_days} days old')",
        "o": [
            "912 days old",
            "365 days old",
            "730 days old",
            "Error"
        ]
    },
    {
        "q": "Which ETL component handles parallel data streams?",
        "type": "mcq",
        "o": [
            "Union transformation",
            "Filter transformation",
            "Sort transformation",
            "Lookup transformation"
        ]
    },
    {
        "q": "The _____ splits a single data stream into multiple outputs.",
        "type": "fill_blank",
        "answers": [
            "router"
        ],
        "other_options": [
            "merger",
            "combiner",
            "joiner"
        ]
    },
    {
        "q": "What is the output of this stream splitting?",
        "type": "mcq",
        "c": "records = [{'type': 'A', 'val': 1}, {'type': 'B', 'val': 2}, {'type': 'A', 'val': 3}]\nstream_a = [r for r in records if r['type'] == 'A']\nprint(len(stream_a))",
        "o": [
            "2",
            "3",
            "1",
            "0"
        ]
    },
    {
        "q": "Conditional routing directs records based on business rules.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the parallel processing steps:",
        "type": "rearrange",
        "words": [
            "Partition data",
            "Process parallel",
            "Merge results",
            "Validate output",
            "Complete job"
        ]
    },
    {
        "q": "What is data partitioning for parallel processing?",
        "type": "mcq",
        "o": [
            "Splitting data into chunks for concurrent processing",
            "Encrypting data in parts",
            "Compressing data segments",
            "Archiving data portions"
        ]
    },
    {
        "q": "Match the parallelism type with its description:",
        "type": "match",
        "left": [
            "Pipeline",
            "Partition",
            "Component",
            "Data"
        ],
        "right": [
            "Sequential stages",
            "Split by key",
            "Independent tasks",
            "Replicate processing"
        ]
    },
    {
        "q": "What is the output of this partition assignment?",
        "type": "mcq",
        "c": "total_records = 1000\nnum_partitions = 4\nrecords_per_partition = total_records // num_partitions\nprint(f'{records_per_partition} per partition')",
        "o": [
            "250 per partition",
            "1000 per partition",
            "4 per partition",
            "Error"
        ]
    },
    {
        "q": "The _____ ensures all partitions complete before proceeding.",
        "type": "fill_blank",
        "answers": [
            "barrier"
        ],
        "other_options": [
            "filter",
            "router",
            "merger"
        ]
    },
    {
        "q": "Load balancing distributes work evenly across parallel processes.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this load calculation?",
        "type": "mcq",
        "c": "workers = 8\ntotal_tasks = 100\ntasks_per_worker = total_tasks / workers\nprint(f'{tasks_per_worker} tasks each')",
        "o": [
            "12.5 tasks each",
            "100 tasks each",
            "8 tasks each",
            "Error"
        ]
    },
    {
        "q": "Which technique reduces ETL processing time?",
        "type": "mcq",
        "o": [
            "Indexing",
            "Logging",
            "Auditing",
            "Commenting"
        ]
    },
    {
        "q": "Match the optimization technique with its benefit:",
        "type": "match",
        "left": [
            "Indexing",
            "Caching",
            "Partitioning",
            "Compression"
        ],
        "right": [
            "Faster lookups",
            "Reduced reads",
            "Query pruning",
            "Smaller storage"
        ]
    },
    {
        "q": "Bulk operations are more efficient than row-by-row processing.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this batch size calculation?",
        "type": "mcq",
        "c": "total_rows = 10000\nbatch_size = 1000\nnum_batches = total_rows // batch_size\nprint(f'{num_batches} batches')",
        "o": [
            "10 batches",
            "1000 batches",
            "1 batches",
            "Error"
        ]
    },
    {
        "q": "Rearrange the performance tuning steps:",
        "type": "rearrange",
        "words": [
            "Identify bottleneck",
            "Analyze metrics",
            "Apply fix",
            "Test change",
            "Monitor result"
        ]
    },
    {
        "q": "The _____ identifies slow-running ETL components.",
        "type": "fill_blank",
        "answers": [
            "profiler"
        ],
        "other_options": [
            "loader",
            "extractor",
            "validator"
        ]
    },
    {
        "q": "What is the output of this timing measurement?",
        "type": "mcq",
        "c": "import time\nstart = 100.0\nend = 145.5\nduration = end - start\nprint(f'{duration} seconds')",
        "o": [
            "45.5 seconds",
            "100.0 seconds",
            "145.5 seconds",
            "Error"
        ]
    },
    {
        "q": "Which metric measures ETL job resource consumption?",
        "type": "mcq",
        "o": [
            "Memory usage",
            "Data accuracy",
            "User count",
            "File size"
        ]
    },
    {
        "q": "Match the resource metric with its unit:",
        "type": "match",
        "left": [
            "CPU",
            "Memory",
            "Disk I/O",
            "Network"
        ],
        "right": [
            "Percentage",
            "Megabytes",
            "IOPS",
            "Mbps"
        ]
    },
    {
        "q": "Resource contention can slow down ETL jobs.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this memory calculation?",
        "type": "mcq",
        "c": "used_mb = 2048\ntotal_mb = 8192\nusage_percent = (used_mb / total_mb) * 100\nprint(f'{usage_percent}% used')",
        "o": [
            "25.0% used",
            "100% used",
            "2048% used",
            "Error"
        ]
    },
    {
        "q": "The _____ limits concurrent ETL job execution.",
        "type": "fill_blank",
        "answers": [
            "throttling"
        ],
        "other_options": [
            "caching",
            "indexing",
            "logging"
        ]
    },
    {
        "q": "Connection pooling reduces database connection overhead.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this connection pool check?",
        "type": "mcq",
        "c": "max_connections = 10\nactive = 7\navailable = max_connections - active\nprint(f'{available} connections available')",
        "o": [
            "3 connections available",
            "10 connections available",
            "7 connections available",
            "Error"
        ]
    },
    {
        "q": "Rearrange the connection management steps:",
        "type": "rearrange",
        "words": [
            "Get connection",
            "Execute query",
            "Process results",
            "Release connection",
            "Handle errors"
        ]
    },
    {
        "q": "What is the output of this complex join operation?",
        "type": "mcq",
        "c": "orders = [{'id': 1, 'customer_id': 101}, {'id': 2, 'customer_id': 102}]\ncustomers = [{'id': 101, 'name': 'Alice'}, {'id': 103, 'name': 'Bob'}]\nmatched = [o for o in orders if o['customer_id'] in [c['id'] for c in customers]]\nprint(len(matched))",
        "o": [
            "1",
            "2",
            "0",
            "3"
        ]
    },
    {
        "q": "Which ETL architecture supports both batch and streaming processing?",
        "type": "mcq",
        "o": [
            "Lambda architecture",
            "Traditional ETL",
            "Simple batch",
            "Manual processing"
        ]
    },
    {
        "q": "Kappa architecture uses a single stream processing layer.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the architecture with its characteristic:",
        "type": "match",
        "left": [
            "Lambda",
            "Kappa",
            "Delta",
            "Medallion"
        ],
        "right": [
            "Dual batch and stream",
            "Stream only",
            "ACID transactions",
            "Bronze-Silver-Gold"
        ]
    },
    {
        "q": "The _____ layer in medallion architecture contains raw data.",
        "type": "fill_blank",
        "answers": [
            "bronze"
        ],
        "other_options": [
            "silver",
            "gold",
            "platinum"
        ]
    },
    {
        "q": "What is the output of this late arriving data simulation?",
        "type": "mcq",
        "c": "from datetime import datetime, timedelta\nevent_time = datetime(2024, 1, 1, 10, 0)\nprocess_time = datetime(2024, 1, 1, 12, 30)\ndelay_hours = (process_time - event_time).seconds / 3600\nprint(f'{delay_hours} hours late')",
        "o": [
            "2.5 hours late",
            "10 hours late",
            "12.5 hours late",
            "Error"
        ]
    },
    {
        "q": "Which technique handles out-of-order event processing?",
        "type": "mcq",
        "o": [
            "Watermarking",
            "Compression",
            "Encryption",
            "Caching"
        ]
    },
    {
        "q": "Late arriving data can trigger reprocessing of aggregations.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the event time processing steps:",
        "type": "rearrange",
        "words": [
            "Receive event",
            "Extract timestamp",
            "Check watermark",
            "Process or buffer",
            "Update state"
        ]
    },
    {
        "q": "What is the output of this window aggregation?",
        "type": "mcq",
        "c": "events = [{'time': 1, 'value': 10}, {'time': 2, 'value': 20}, {'time': 5, 'value': 30}]\nwindow_size = 3\nwindow_1 = [e['value'] for e in events if e['time'] <= window_size]\nprint(sum(window_1))",
        "o": [
            "30",
            "60",
            "10",
            "Error"
        ]
    },
    {
        "q": "The _____ window type has fixed boundaries.",
        "type": "fill_blank",
        "answers": [
            "tumbling"
        ],
        "other_options": [
            "sliding",
            "session",
            "global"
        ]
    },
    {
        "q": "Match the window type with its behavior:",
        "type": "match",
        "left": [
            "Tumbling",
            "Sliding",
            "Session",
            "Global"
        ],
        "right": [
            "Non-overlapping fixed",
            "Overlapping intervals",
            "Activity-based",
            "Entire stream"
        ]
    },
    {
        "q": "What is exactly-once semantics in streaming ETL?",
        "type": "mcq",
        "o": [
            "Each event is processed exactly one time",
            "Events may be processed multiple times",
            "Events may be skipped",
            "All events are batched"
        ]
    },
    {
        "q": "At-least-once processing may produce duplicate results.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this deduplication check?",
        "type": "mcq",
        "c": "processed_ids = {1, 2, 3}\nnew_event_id = 2\nis_duplicate = new_event_id in processed_ids\nprint(f'Duplicate: {is_duplicate}')",
        "o": [
            "Duplicate: True",
            "Duplicate: False",
            "2",
            "Error"
        ]
    },
    {
        "q": "Which pattern ensures events are not lost during failures?",
        "type": "mcq",
        "o": [
            "Checkpointing",
            "Compression",
            "Caching",
            "Commenting"
        ]
    },
    {
        "q": "The _____ stores the current processing state for recovery.",
        "type": "fill_blank",
        "answers": [
            "checkpoint"
        ],
        "other_options": [
            "buffer",
            "cache",
            "log"
        ]
    },
    {
        "q": "Rearrange the failure recovery steps:",
        "type": "rearrange",
        "words": [
            "Detect failure",
            "Load checkpoint",
            "Replay events",
            "Continue processing",
            "Update checkpoint"
        ]
    },
    {
        "q": "What is the output of this state management simulation?",
        "type": "mcq",
        "c": "state = {'count': 100, 'sum': 5000}\nnew_value = 50\nstate['count'] += 1\nstate['sum'] += new_value\nprint(state['sum'])",
        "o": [
            "5050",
            "5000",
            "50",
            "Error"
        ]
    },
    {
        "q": "Match the delivery guarantee with its trade-off:",
        "type": "match",
        "left": [
            "At-most-once",
            "At-least-once",
            "Exactly-once"
        ],
        "right": [
            "May lose data",
            "May duplicate",
            "Complex implementation"
        ]
    },
    {
        "q": "Schema evolution allows adding new columns to existing tables.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this schema comparison?",
        "type": "mcq",
        "c": "old_schema = ['id', 'name', 'date']\nnew_schema = ['id', 'name', 'date', 'status', 'amount']\nadded = set(new_schema) - set(old_schema)\nprint(len(added))",
        "o": [
            "2",
            "5",
            "3",
            "0"
        ]
    },
    {
        "q": "Which schema change is backward compatible?",
        "type": "mcq",
        "o": [
            "Adding optional column",
            "Removing required column",
            "Changing data type",
            "Renaming primary key"
        ]
    },
    {
        "q": "The _____ prevents breaking changes in data schemas.",
        "type": "fill_blank",
        "answers": [
            "schema registry"
        ],
        "other_options": [
            "data catalog",
            "metadata store",
            "config file"
        ]
    },
    {
        "q": "Match the schema change with its impact:",
        "type": "match",
        "left": [
            "Add column",
            "Remove column",
            "Rename column",
            "Change type"
        ],
        "right": [
            "Usually safe",
            "Breaking",
            "Breaking",
            "Potentially breaking"
        ]
    },
    {
        "q": "Rearrange the schema migration steps:",
        "type": "rearrange",
        "words": [
            "Analyze impact",
            "Create migration",
            "Test changes",
            "Deploy update",
            "Validate data"
        ]
    },
    {
        "q": "What is the output of this type coercion?",
        "type": "mcq",
        "c": "value = '123.45'\nconverted = float(value)\nrounded = round(converted, 1)\nprint(rounded)",
        "o": [
            "123.4",
            "123.45",
            "123",
            "Error"
        ]
    },
    {
        "q": "Data contracts define expected data format between systems.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this contract validation?",
        "type": "mcq",
        "c": "contract = {'required': ['id', 'name'], 'optional': ['email']}\ndata = {'id': 1, 'name': 'Test'}\nmissing = [f for f in contract['required'] if f not in data]\nprint(f'Missing: {missing}')",
        "o": [
            "Missing: []",
            "Missing: ['id', 'name']",
            "Missing: ['email']",
            "Error"
        ]
    },
    {
        "q": "Which tool manages data pipeline dependencies?",
        "type": "mcq",
        "o": [
            "Orchestrator",
            "Compressor",
            "Encryptor",
            "Formatter"
        ]
    },
    {
        "q": "The _____ graph represents task dependencies in ETL.",
        "type": "fill_blank",
        "answers": [
            "DAG"
        ],
        "other_options": [
            "tree",
            "list",
            "stack"
        ]
    },
    {
        "q": "Match the orchestration concept with its purpose:",
        "type": "match",
        "left": [
            "Task",
            "DAG",
            "Trigger",
            "Sensor"
        ],
        "right": [
            "Unit of work",
            "Workflow graph",
            "Start condition",
            "Wait for event"
        ]
    },
    {
        "q": "What is the output of this dependency resolution?",
        "type": "mcq",
        "c": "tasks = {'extract': [], 'transform': ['extract'], 'load': ['transform']}\ndef can_run(task):\n    return all(dep in completed for dep in tasks[task])\ncompleted = ['extract']\nprint(can_run('transform'))",
        "o": [
            "True",
            "False",
            "['extract']",
            "Error"
        ]
    },
    {
        "q": "Sensors wait for external conditions before proceeding.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the pipeline deployment steps:",
        "type": "rearrange",
        "words": [
            "Develop locally",
            "Test staging",
            "Review code",
            "Deploy production",
            "Monitor"
        ]
    },
    {
        "q": "What is the output of this trigger evaluation?",
        "type": "mcq",
        "c": "from datetime import datetime\nschedule_hour = 6\ncurrent_hour = datetime(2024, 1, 1, 6, 0).hour\nshould_run = current_hour == schedule_hour\nprint(should_run)",
        "o": [
            "True",
            "False",
            "6",
            "Error"
        ]
    },
    {
        "q": "Which pattern handles ETL job failures gracefully?",
        "type": "mcq",
        "o": [
            "Circuit breaker",
            "Data compression",
            "File encryption",
            "Code commenting"
        ]
    },
    {
        "q": "The _____ pattern prevents cascading failures.",
        "type": "fill_blank",
        "answers": [
            "circuit breaker"
        ],
        "other_options": [
            "retry",
            "timeout",
            "cache"
        ]
    },
    {
        "q": "Match the resilience pattern with its behavior:",
        "type": "match",
        "left": [
            "Retry",
            "Timeout",
            "Fallback",
            "Bulkhead"
        ],
        "right": [
            "Try again",
            "Limit wait",
            "Use default",
            "Isolate failures"
        ]
    },
    {
        "q": "What is the output of this circuit breaker simulation?",
        "type": "mcq",
        "c": "failure_count = 5\nthreshold = 3\ncircuit_open = failure_count >= threshold\nprint(f'Circuit open: {circuit_open}')",
        "o": [
            "Circuit open: True",
            "Circuit open: False",
            "5",
            "Error"
        ]
    },
    {
        "q": "Backpressure prevents overwhelming downstream systems.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the incident response steps:",
        "type": "rearrange",
        "words": [
            "Detect issue",
            "Assess impact",
            "Implement fix",
            "Verify resolution",
            "Document"
        ]
    },
    {
        "q": "What is the output of this rate limiting check?",
        "type": "mcq",
        "c": "requests_per_second = 100\nmax_rate = 50\nis_throttled = requests_per_second > max_rate\nprint(f'Throttled: {is_throttled}')",
        "o": [
            "Throttled: True",
            "Throttled: False",
            "100",
            "Error"
        ]
    },
    {
        "q": "Which technique improves ETL observability?",
        "type": "mcq",
        "o": [
            "Distributed tracing",
            "Data compression",
            "File encryption",
            "Code minification"
        ]
    },
    {
        "q": "The _____ provides end-to-end visibility across services.",
        "type": "fill_blank",
        "answers": [
            "trace"
        ],
        "other_options": [
            "log",
            "metric",
            "alert"
        ]
    },
    {
        "q": "Match the observability pillar with its purpose:",
        "type": "match",
        "left": [
            "Logs",
            "Metrics",
            "Traces",
            "Alerts"
        ],
        "right": [
            "Event records",
            "Measurements",
            "Request flow",
            "Notifications"
        ]
    },
    {
        "q": "What is the output of this span duration calculation?",
        "type": "mcq",
        "c": "span_start = 1000\nspan_end = 1450\nduration_ms = span_end - span_start\nprint(f'{duration_ms}ms')",
        "o": [
            "450ms",
            "1000ms",
            "1450ms",
            "Error"
        ]
    },
    {
        "q": "SLA monitoring tracks service level commitments.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the alerting workflow:",
        "type": "rearrange",
        "words": [
            "Define threshold",
            "Monitor metric",
            "Trigger alert",
            "Notify team",
            "Acknowledge"
        ]
    },
    {
        "q": "What is the output of this SLA calculation?",
        "type": "mcq",
        "c": "successful_runs = 95\ntotal_runs = 100\nsla_percent = (successful_runs / total_runs) * 100\nprint(f'{sla_percent}% SLA')",
        "o": [
            "95.0% SLA",
            "100% SLA",
            "95 SLA",
            "Error"
        ]
    },
    {
        "q": "Which technique reduces data transfer in ETL?",
        "type": "mcq",
        "o": [
            "Compression",
            "Logging",
            "Commenting",
            "Formatting"
        ]
    },
    {
        "q": "The _____ format is optimized for columnar data storage.",
        "type": "fill_blank",
        "answers": [
            "Parquet"
        ],
        "other_options": [
            "CSV",
            "JSON",
            "XML"
        ]
    },
    {
        "q": "Match the file format with its characteristic:",
        "type": "match",
        "left": [
            "Parquet",
            "Avro",
            "ORC",
            "JSON"
        ],
        "right": [
            "Columnar analytics",
            "Row-based with schema",
            "Hive optimized",
            "Human readable"
        ]
    },
    {
        "q": "What is the output of this compression ratio calculation?",
        "type": "mcq",
        "c": "original_size = 1000\ncompressed_size = 250\nratio = original_size / compressed_size\nprint(f'{ratio}x compression')",
        "o": [
            "4.0x compression",
            "0.25x compression",
            "1000x compression",
            "Error"
        ]
    },
    {
        "q": "Columnar formats are efficient for aggregation queries.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the data serialization steps:",
        "type": "rearrange",
        "words": [
            "Define schema",
            "Serialize data",
            "Compress bytes",
            "Write to storage",
            "Generate metadata"
        ]
    },
    {
        "q": "What is the output of this predicate pushdown simulation?",
        "type": "mcq",
        "c": "total_partitions = 100\nfiltered_partitions = 5\nskipped = total_partitions - filtered_partitions\nprint(f'Skipped {skipped} partitions')",
        "o": [
            "Skipped 95 partitions",
            "Skipped 5 partitions",
            "Skipped 100 partitions",
            "Error"
        ]
    },
    {
        "q": "Which optimization pushes filters to the data source?",
        "type": "mcq",
        "o": [
            "Predicate pushdown",
            "Data caching",
            "Schema evolution",
            "Late binding"
        ]
    },
    {
        "q": "The _____ optimization reads only required columns from columnar files.",
        "type": "fill_blank",
        "answers": [
            "projection pushdown"
        ],
        "other_options": [
            "predicate pushdown",
            "partition pruning",
            "bucket pruning"
        ]
    },
    {
        "q": "Match the optimization with its benefit:",
        "type": "match",
        "left": [
            "Predicate pushdown",
            "Projection pushdown",
            "Partition pruning",
            "Caching"
        ],
        "right": [
            "Filter at source",
            "Read fewer columns",
            "Skip partitions",
            "Reuse data"
        ]
    },
    {
        "q": "What is the output of this column projection?",
        "type": "mcq",
        "c": "all_columns = ['id', 'name', 'date', 'amount', 'status', 'notes']\nneeded_columns = ['id', 'amount']\nreduction = len(all_columns) - len(needed_columns)\nprint(f'Skipped {reduction} columns')",
        "o": [
            "Skipped 4 columns",
            "Skipped 2 columns",
            "Skipped 6 columns",
            "Error"
        ]
    },
    {
        "q": "Partition pruning skips irrelevant data partitions.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the query optimization steps:",
        "type": "rearrange",
        "words": [
            "Parse query",
            "Analyze plan",
            "Apply optimizations",
            "Generate physical plan",
            "Execute"
        ]
    },
    {
        "q": "What is the output of this cost estimation?",
        "type": "mcq",
        "c": "rows = 1000000\nrows_per_second = 50000\nestimated_seconds = rows / rows_per_second\nprint(f'{estimated_seconds} seconds')",
        "o": [
            "20.0 seconds",
            "1000000 seconds",
            "50000 seconds",
            "Error"
        ]
    },
    {
        "q": "Which technique distributes data across multiple nodes?",
        "type": "mcq",
        "o": [
            "Sharding",
            "Compression",
            "Encryption",
            "Formatting"
        ]
    },
    {
        "q": "The _____ determines which shard stores a record.",
        "type": "fill_blank",
        "answers": [
            "shard key"
        ],
        "other_options": [
            "primary key",
            "foreign key",
            "unique key"
        ]
    },
    {
        "q": "Match the data distribution strategy with its approach:",
        "type": "match",
        "left": [
            "Hash",
            "Range",
            "Round-robin",
            "Directory"
        ],
        "right": [
            "Hash function",
            "Value ranges",
            "Sequential",
            "Lookup table"
        ]
    },
    {
        "q": "What is the output of this shard assignment?",
        "type": "mcq",
        "c": "record_id = 12345\nnum_shards = 4\nshard = record_id % num_shards\nprint(f'Shard: {shard}')",
        "o": [
            "Shard: 1",
            "Shard: 0",
            "Shard: 4",
            "Error"
        ]
    },
    {
        "q": "Hot spots occur when data is unevenly distributed.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the rebalancing steps:",
        "type": "rearrange",
        "words": [
            "Detect imbalance",
            "Plan migration",
            "Move data",
            "Update routing",
            "Verify balance"
        ]
    },
    {
        "q": "What is the output of this skew detection?",
        "type": "mcq",
        "c": "shard_sizes = [1000, 1100, 5000, 900]\nmax_size = max(shard_sizes)\nmin_size = min(shard_sizes)\nskew_ratio = max_size / min_size\nprint(f'Skew: {round(skew_ratio, 1)}x')",
        "o": [
            "Skew: 5.6x",
            "Skew: 1.0x",
            "Skew: 5000x",
            "Error"
        ]
    },
    {
        "q": "Which approach handles slowly changing reference data?",
        "type": "mcq",
        "o": [
            "Broadcast variables",
            "Full table scan",
            "Nested loops",
            "Cartesian product"
        ]
    },
    {
        "q": "The _____ join broadcasts small tables to all nodes.",
        "type": "fill_blank",
        "answers": [
            "broadcast"
        ],
        "other_options": [
            "shuffle",
            "sort-merge",
            "nested loop"
        ]
    },
    {
        "q": "Match the join strategy with its use case:",
        "type": "match",
        "left": [
            "Broadcast",
            "Shuffle",
            "Sort-merge",
            "Hash"
        ],
        "right": [
            "Small table",
            "Large tables",
            "Pre-sorted data",
            "Equi-joins"
        ]
    },
    {
        "q": "What is the output of this join size estimation?",
        "type": "mcq",
        "c": "left_rows = 1000000\nright_rows = 100\nbroadcast_threshold = 10000\nuse_broadcast = right_rows < broadcast_threshold\nprint(f'Use broadcast: {use_broadcast}')",
        "o": [
            "Use broadcast: True",
            "Use broadcast: False",
            "100",
            "Error"
        ]
    },
    {
        "q": "Shuffle operations are expensive in distributed systems.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the shuffle join steps:",
        "type": "rearrange",
        "words": [
            "Hash keys",
            "Partition data",
            "Shuffle to nodes",
            "Join locally",
            "Collect results"
        ]
    },
    {
        "q": "What is the output of this partition count calculation?",
        "type": "mcq",
        "c": "data_size_gb = 100\ntarget_partition_size_gb = 1\npartitions = data_size_gb // target_partition_size_gb\nprint(f'{partitions} partitions')",
        "o": [
            "100 partitions",
            "1 partitions",
            "1000 partitions",
            "Error"
        ]
    },
    {
        "q": "What is the output of this complex aggregation pipeline?",
        "type": "mcq",
        "c": "from collections import defaultdict\ndata = [{'region': 'NA', 'sales': 100}, {'region': 'EU', 'sales': 200}, {'region': 'NA', 'sales': 150}]\nby_region = defaultdict(int)\nfor d in data:\n    by_region[d['region']] += d['sales']\nprint(by_region['NA'])",
        "o": [
            "250",
            "100",
            "150",
            "450"
        ]
    },
    {
        "q": "Which technique enables time-travel queries in data lakes?",
        "type": "mcq",
        "o": [
            "Delta Lake versioning",
            "File compression",
            "Data encryption",
            "Index creation"
        ]
    },
    {
        "q": "ACID transactions are supported by Delta Lake.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the data lake format with its feature:",
        "type": "match",
        "left": [
            "Delta Lake",
            "Apache Hudi",
            "Apache Iceberg",
            "Lake Formation"
        ],
        "right": [
            "ACID on Spark",
            "Upserts streaming",
            "Hidden partitioning",
            "AWS governance"
        ]
    },
    {
        "q": "The _____ log tracks all changes in Delta Lake.",
        "type": "fill_blank",
        "answers": [
            "transaction"
        ],
        "other_options": [
            "error",
            "audit",
            "system"
        ]
    },
    {
        "q": "What is the output of this version query simulation?",
        "type": "mcq",
        "c": "versions = [0, 1, 2, 3, 4]\ncurrent_version = max(versions)\nrollback_to = current_version - 2\nprint(f'Rollback to version {rollback_to}')",
        "o": [
            "Rollback to version 2",
            "Rollback to version 4",
            "Rollback to version 0",
            "Error"
        ]
    },
    {
        "q": "Which operation compacts small files in Delta Lake?",
        "type": "mcq",
        "o": [
            "OPTIMIZE",
            "COMPACT",
            "MERGE",
            "VACUUM"
        ]
    },
    {
        "q": "Z-ordering improves query performance by co-locating related data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the Delta Lake optimization steps:",
        "type": "rearrange",
        "words": [
            "Analyze patterns",
            "Run OPTIMIZE",
            "Apply Z-ORDER",
            "VACUUM old files",
            "Verify performance"
        ]
    },
    {
        "q": "What is the output of this file count calculation?",
        "type": "mcq",
        "c": "files_before = 1000\nafter_optimize = 50\nreduction = ((files_before - after_optimize) / files_before) * 100\nprint(f'{reduction}% reduction')",
        "o": [
            "95.0% reduction",
            "50% reduction",
            "5% reduction",
            "Error"
        ]
    },
    {
        "q": "The _____ command removes old versions from Delta Lake.",
        "type": "fill_blank",
        "answers": [
            "VACUUM"
        ],
        "other_options": [
            "DELETE",
            "PURGE",
            "CLEAN"
        ]
    },
    {
        "q": "Match the maintenance operation with its purpose:",
        "type": "match",
        "left": [
            "OPTIMIZE",
            "VACUUM",
            "ANALYZE",
            "Z-ORDER"
        ],
        "right": [
            "Compact files",
            "Remove old versions",
            "Update statistics",
            "Cluster data"
        ]
    },
    {
        "q": "What is schema-on-read?",
        "type": "mcq",
        "o": [
            "Schema applied when data is queried",
            "Schema applied before data is stored",
            "Schema generated automatically",
            "Schema ignored entirely"
        ]
    },
    {
        "q": "Schema-on-write validates data at ingestion time.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this schema inference?",
        "type": "mcq",
        "c": "sample = {'id': 1, 'name': 'test', 'amount': 99.99, 'active': True}\ntypes = {k: type(v).__name__ for k, v in sample.items()}\nprint(types['amount'])",
        "o": [
            "float",
            "int",
            "str",
            "bool"
        ]
    },
    {
        "q": "Which approach separates storage from compute?",
        "type": "mcq",
        "o": [
            "Lakehouse architecture",
            "Traditional data warehouse",
            "Local file system",
            "Single-node database"
        ]
    },
    {
        "q": "The _____ pattern unifies data lake and warehouse capabilities.",
        "type": "fill_blank",
        "answers": [
            "lakehouse"
        ],
        "other_options": [
            "warehouse",
            "lake",
            "mart"
        ]
    },
    {
        "q": "Match the architecture with its storage approach:",
        "type": "match",
        "left": [
            "Data warehouse",
            "Data lake",
            "Lakehouse",
            "Data mart"
        ],
        "right": [
            "Structured only",
            "Raw files",
            "Unified storage",
            "Department subset"
        ]
    },
    {
        "q": "Rearrange the lakehouse data flow:",
        "type": "rearrange",
        "words": [
            "Ingest raw",
            "Store bronze",
            "Curate silver",
            "Aggregate gold",
            "Serve BI"
        ]
    },
    {
        "q": "What is the output of this tier classification?",
        "type": "mcq",
        "c": "tiers = {'raw': 'bronze', 'cleaned': 'silver', 'aggregated': 'gold'}\ndata_state = 'cleaned'\ntier = tiers.get(data_state, 'unknown')\nprint(f'Data tier: {tier}')",
        "o": [
            "Data tier: silver",
            "Data tier: bronze",
            "Data tier: gold",
            "Error"
        ]
    },
    {
        "q": "Federated queries access data across multiple sources.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this federated query simulation?",
        "type": "mcq",
        "c": "sources = ['postgres', 's3', 'snowflake']\nquery_sources = len(sources)\nprint(f'Querying {query_sources} sources')",
        "o": [
            "Querying 3 sources",
            "Querying 1 sources",
            "Querying postgres sources",
            "Error"
        ]
    },
    {
        "q": "Which technique virtualizes data access without movement?",
        "type": "mcq",
        "o": [
            "Data virtualization",
            "Data replication",
            "Data backup",
            "Data deletion"
        ]
    },
    {
        "q": "The _____ layer provides unified access to distributed data.",
        "type": "fill_blank",
        "answers": [
            "virtualization"
        ],
        "other_options": [
            "storage",
            "compute",
            "network"
        ]
    },
    {
        "q": "Match the data access pattern with its characteristic:",
        "type": "match",
        "left": [
            "ETL",
            "ELT",
            "Virtualization",
            "Replication"
        ],
        "right": [
            "Extract first",
            "Load first",
            "No movement",
            "Copy data"
        ]
    },
    {
        "q": "What is the output of this query routing logic?",
        "type": "mcq",
        "c": "table_locations = {'customers': 'postgres', 'orders': 's3', 'products': 'mysql'}\ntable = 'orders'\nsource = table_locations.get(table, 'unknown')\nprint(f'Route to: {source}')",
        "o": [
            "Route to: s3",
            "Route to: postgres",
            "Route to: mysql",
            "Error"
        ]
    },
    {
        "q": "Data mesh promotes decentralized data ownership.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the data mesh principles:",
        "type": "rearrange",
        "words": [
            "Domain ownership",
            "Data as product",
            "Self-serve platform",
            "Federated governance"
        ]
    },
    {
        "q": "What is the output of this domain assignment?",
        "type": "mcq",
        "c": "domains = {'orders': 'sales', 'inventory': 'operations', 'customers': 'marketing'}\ntable = 'customers'\ndomain = domains.get(table)\nprint(f'Owner: {domain}')",
        "o": [
            "Owner: marketing",
            "Owner: sales",
            "Owner: operations",
            "Error"
        ]
    },
    {
        "q": "Which role is responsible for data product quality?",
        "type": "mcq",
        "o": [
            "Data product owner",
            "Database administrator",
            "Network engineer",
            "Security analyst"
        ]
    },
    {
        "q": "The _____ provides common data infrastructure in data mesh.",
        "type": "fill_blank",
        "answers": [
            "platform"
        ],
        "other_options": [
            "domain",
            "product",
            "governance"
        ]
    },
    {
        "q": "Match the data mesh role with its responsibility:",
        "type": "match",
        "left": [
            "Domain team",
            "Platform team",
            "Governance",
            "Consumers"
        ],
        "right": [
            "Own data products",
            "Build infrastructure",
            "Set standards",
            "Use data"
        ]
    },
    {
        "q": "What is the output of this SLA tracking?",
        "type": "mcq",
        "c": "data_product = {'name': 'sales_summary', 'freshness_hours': 1, 'quality_score': 0.99}\nsla_met = data_product['quality_score'] >= 0.95\nprint(f'SLA met: {sla_met}')",
        "o": [
            "SLA met: True",
            "SLA met: False",
            "0.99",
            "Error"
        ]
    },
    {
        "q": "Semantic layer provides business-friendly data access.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the semantic layer components:",
        "type": "rearrange",
        "words": [
            "Physical tables",
            "Logical models",
            "Metrics definitions",
            "Business terms",
            "Access layer"
        ]
    },
    {
        "q": "What is the output of this metric calculation?",
        "type": "mcq",
        "c": "metrics = {'revenue': 1000000, 'costs': 600000}\nprofit = metrics['revenue'] - metrics['costs']\nmargin = (profit / metrics['revenue']) * 100\nprint(f'{margin}% margin')",
        "o": [
            "40.0% margin",
            "60.0% margin",
            "100% margin",
            "Error"
        ]
    },
    {
        "q": "Which technique ensures consistent metric definitions?",
        "type": "mcq",
        "o": [
            "Metrics layer",
            "File compression",
            "Data encryption",
            "Network routing"
        ]
    },
    {
        "q": "The _____ prevents multiple definitions of the same metric.",
        "type": "fill_blank",
        "answers": [
            "single source of truth"
        ],
        "other_options": [
            "multiple sources",
            "backup",
            "cache"
        ]
    },
    {
        "q": "Match the semantic concept with its purpose:",
        "type": "match",
        "left": [
            "Entity",
            "Measure",
            "Dimension",
            "Hierarchy"
        ],
        "right": [
            "Business object",
            "Numeric value",
            "Category",
            "Drill path"
        ]
    },
    {
        "q": "What is the output of this dimension lookup?",
        "type": "mcq",
        "c": "dimensions = {'date': ['year', 'quarter', 'month', 'day'], 'geo': ['country', 'state', 'city']}\nlevels = len(dimensions['date'])\nprint(f'{levels} levels')",
        "o": [
            "4 levels",
            "3 levels",
            "2 levels",
            "Error"
        ]
    },
    {
        "q": "Data observability detects data quality issues proactively.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the data observability pillars:",
        "type": "rearrange",
        "words": [
            "Freshness",
            "Volume",
            "Schema",
            "Distribution",
            "Lineage"
        ]
    },
    {
        "q": "What is the output of this freshness check?",
        "type": "mcq",
        "c": "from datetime import datetime, timedelta\nlast_update = datetime(2024, 1, 1, 10, 0)\ncurrent = datetime(2024, 1, 1, 14, 0)\nage_hours = (current - last_update).seconds / 3600\nprint(f'{age_hours} hours old')",
        "o": [
            "4.0 hours old",
            "10 hours old",
            "14 hours old",
            "Error"
        ]
    },
    {
        "q": "Which metric detects unexpected changes in data volume?",
        "type": "mcq",
        "o": [
            "Row count anomaly",
            "Schema change",
            "Null percentage",
            "Duplicate count"
        ]
    },
    {
        "q": "The _____ alert fires when data is older than expected.",
        "type": "fill_blank",
        "answers": [
            "freshness"
        ],
        "other_options": [
            "volume",
            "schema",
            "quality"
        ]
    },
    {
        "q": "Match the observability metric with its detection:",
        "type": "match",
        "left": [
            "Freshness",
            "Volume",
            "Schema",
            "Distribution"
        ],
        "right": [
            "Stale data",
            "Missing rows",
            "Column changes",
            "Value drift"
        ]
    },
    {
        "q": "What is the output of this volume anomaly detection?",
        "type": "mcq",
        "c": "expected_rows = 10000\nactual_rows = 5000\ndeviation = abs(actual_rows - expected_rows) / expected_rows * 100\nprint(f'{deviation}% deviation')",
        "o": [
            "50.0% deviation",
            "100% deviation",
            "5000% deviation",
            "Error"
        ]
    },
    {
        "q": "Change data capture from logs is less intrusive than triggers.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the CDC pipeline stages:",
        "type": "rearrange",
        "words": [
            "Capture changes",
            "Publish to stream",
            "Transform events",
            "Apply to target",
            "Confirm"
        ]
    },
    {
        "q": "What is the output of this CDC offset tracking?",
        "type": "mcq",
        "c": "current_offset = 1000\nevents_processed = 250\nnew_offset = current_offset + events_processed\nprint(f'Offset: {new_offset}')",
        "o": [
            "Offset: 1250",
            "Offset: 1000",
            "Offset: 250",
            "Error"
        ]
    },
    {
        "q": "Which tool captures database changes via log mining?",
        "type": "mcq",
        "o": [
            "Debezium",
            "Pandas",
            "NumPy",
            "Matplotlib"
        ]
    },
    {
        "q": "The _____ connector enables real-time CDC from databases.",
        "type": "fill_blank",
        "answers": [
            "Debezium"
        ],
        "other_options": [
            "Pandas",
            "Spark",
            "Airflow"
        ]
    },
    {
        "q": "Match the CDC source with its log type:",
        "type": "match",
        "left": [
            "PostgreSQL",
            "MySQL",
            "SQL Server",
            "Oracle"
        ],
        "right": [
            "WAL",
            "Binlog",
            "Transaction log",
            "Redo log"
        ]
    },
    {
        "q": "What is the output of this event ordering check?",
        "type": "mcq",
        "c": "events = [{'seq': 1, 'op': 'INSERT'}, {'seq': 2, 'op': 'UPDATE'}, {'seq': 3, 'op': 'DELETE'}]\nis_ordered = all(events[i]['seq'] < events[i+1]['seq'] for i in range(len(events)-1))\nprint(f'Ordered: {is_ordered}')",
        "o": [
            "Ordered: True",
            "Ordered: False",
            "[1, 2, 3]",
            "Error"
        ]
    },
    {
        "q": "Tombstone records signal deletions in CDC streams.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the event sourcing pattern:",
        "type": "rearrange",
        "words": [
            "Capture event",
            "Store in log",
            "Project state",
            "Handle queries",
            "Replay if needed"
        ]
    },
    {
        "q": "What is the output of this event replay?",
        "type": "mcq",
        "c": "events = [{'type': 'credit', 'amount': 100}, {'type': 'debit', 'amount': 30}, {'type': 'credit', 'amount': 50}]\nbalance = 0\nfor e in events:\n    balance += e['amount'] if e['type'] == 'credit' else -e['amount']\nprint(balance)",
        "o": [
            "120",
            "180",
            "80",
            "Error"
        ]
    },
    {
        "q": "Which pattern stores all changes as immutable events?",
        "type": "mcq",
        "o": [
            "Event sourcing",
            "CRUD operations",
            "File deletion",
            "Data truncation"
        ]
    },
    {
        "q": "The _____ recreates current state from event history.",
        "type": "fill_blank",
        "answers": [
            "projection"
        ],
        "other_options": [
            "extraction",
            "deletion",
            "compression"
        ]
    },
    {
        "q": "Match the event pattern with its use:",
        "type": "match",
        "left": [
            "Event store",
            "Snapshot",
            "Projection",
            "Saga"
        ],
        "right": [
            "Append-only log",
            "State checkpoint",
            "Read model",
            "Distributed transaction"
        ]
    },
    {
        "q": "What is the output of this snapshot creation?",
        "type": "mcq",
        "c": "events_since_snapshot = 1000\nsnapshot_threshold = 100\nneed_snapshot = events_since_snapshot >= snapshot_threshold\nprint(f'Create snapshot: {need_snapshot}')",
        "o": [
            "Create snapshot: True",
            "Create snapshot: False",
            "1000",
            "Error"
        ]
    },
    {
        "q": "Sagas handle distributed transactions across services.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the saga pattern steps:",
        "type": "rearrange",
        "words": [
            "Start transaction",
            "Execute steps",
            "Handle failure",
            "Compensate",
            "Complete or rollback"
        ]
    },
    {
        "q": "What is the output of this compensation check?",
        "type": "mcq",
        "c": "steps_completed = ['step1', 'step2']\nfailed_step = 'step3'\ncompensations_needed = len(steps_completed)\nprint(f'{compensations_needed} compensations')",
        "o": [
            "2 compensations",
            "3 compensations",
            "0 compensations",
            "Error"
        ]
    },
    {
        "q": "Which orchestration type uses a central coordinator?",
        "type": "mcq",
        "o": [
            "Orchestration-based saga",
            "Choreography-based saga",
            "Event-driven saga",
            "Stateless saga"
        ]
    },
    {
        "q": "The _____ saga relies on events without central control.",
        "type": "fill_blank",
        "answers": [
            "choreography"
        ],
        "other_options": [
            "orchestration",
            "coordination",
            "synchronization"
        ]
    },
    {
        "q": "Match the saga type with its characteristic:",
        "type": "match",
        "left": [
            "Orchestration",
            "Choreography",
            "Timeout",
            "Compensation"
        ],
        "right": [
            "Central controller",
            "Event-driven",
            "Time limit",
            "Undo action"
        ]
    },
    {
        "q": "What is the output of this idempotency key check?",
        "type": "mcq",
        "c": "processed_keys = {'key1', 'key2', 'key3'}\nnew_key = 'key2'\nis_duplicate = new_key in processed_keys\nprint(f'Skip processing: {is_duplicate}')",
        "o": [
            "Skip processing: True",
            "Skip processing: False",
            "key2",
            "Error"
        ]
    },
    {
        "q": "Idempotency keys prevent duplicate processing of requests.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the idempotent operation flow:",
        "type": "rearrange",
        "words": [
            "Receive request",
            "Check key",
            "Process if new",
            "Store key",
            "Return result"
        ]
    },
    {
        "q": "What is the output of this transaction isolation simulation?",
        "type": "mcq",
        "c": "isolation_levels = ['READ_UNCOMMITTED', 'READ_COMMITTED', 'REPEATABLE_READ', 'SERIALIZABLE']\nstrictest = isolation_levels[-1]\nprint(f'Strictest: {strictest}')",
        "o": [
            "Strictest: SERIALIZABLE",
            "Strictest: READ_UNCOMMITTED",
            "Strictest: READ_COMMITTED",
            "Error"
        ]
    },
    {
        "q": "Which consistency model allows eventual convergence?",
        "type": "mcq",
        "o": [
            "Eventual consistency",
            "Strong consistency",
            "Immediate consistency",
            "No consistency"
        ]
    },
    {
        "q": "The _____ theorem states that distributed systems cannot guarantee all three: Consistency, Availability, Partition tolerance.",
        "type": "fill_blank",
        "answers": [
            "CAP"
        ],
        "other_options": [
            "ACID",
            "BASE",
            "SOLID"
        ]
    },
    {
        "q": "Match the consistency model with its guarantee:",
        "type": "match",
        "left": [
            "Strong",
            "Eventual",
            "Causal",
            "Read-your-writes"
        ],
        "right": [
            "Immediate",
            "Converges over time",
            "Order preserved",
            "See own writes"
        ]
    },
    {
        "q": "What is the output of this replication lag calculation?",
        "type": "mcq",
        "c": "primary_position = 10000\nreplica_position = 9500\nlag = primary_position - replica_position\nprint(f'Lag: {lag} records')",
        "o": [
            "Lag: 500 records",
            "Lag: 10000 records",
            "Lag: 9500 records",
            "Error"
        ]
    },
    {
        "q": "What is the output of this distributed consensus simulation?",
        "type": "mcq",
        "c": "votes = {'node1': True, 'node2': True, 'node3': False, 'node4': True, 'node5': True}\nmajority = sum(votes.values()) > len(votes) / 2\nprint(f'Consensus: {majority}')",
        "o": [
            "Consensus: True",
            "Consensus: False",
            "4",
            "Error"
        ]
    },
    {
        "q": "Which algorithm provides distributed consensus?",
        "type": "mcq",
        "o": [
            "Raft",
            "Quicksort",
            "Binary search",
            "Bubble sort"
        ]
    },
    {
        "q": "Paxos and Raft are both consensus algorithms.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the consensus algorithm with its characteristic:",
        "type": "match",
        "left": [
            "Paxos",
            "Raft",
            "ZAB",
            "2PC"
        ],
        "right": [
            "Original consensus",
            "Understandable",
            "ZooKeeper",
            "Two-phase commit"
        ]
    },
    {
        "q": "The _____ ensures leader election in distributed systems.",
        "type": "fill_blank",
        "answers": [
            "consensus"
        ],
        "other_options": [
            "caching",
            "compression",
            "encryption"
        ]
    },
    {
        "q": "What is the output of this quorum calculation?",
        "type": "mcq",
        "c": "nodes = 5\nquorum = (nodes // 2) + 1\nprint(f'Quorum: {quorum}')",
        "o": [
            "Quorum: 3",
            "Quorum: 2",
            "Quorum: 5",
            "Error"
        ]
    },
    {
        "q": "Which pattern handles cross-datacenter replication?",
        "type": "mcq",
        "o": [
            "Geo-replication",
            "Local backup",
            "Single-node storage",
            "Memory cache"
        ]
    },
    {
        "q": "Active-active replication allows writes in multiple locations.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the disaster recovery priorities:",
        "type": "rearrange",
        "words": [
            "Detect failure",
            "Failover",
            "Restore service",
            "Sync data",
            "Verify recovery"
        ]
    },
    {
        "q": "What is the output of this RTO calculation?",
        "type": "mcq",
        "c": "outage_start = 0\nrecovery_time = 15\nrto_minutes = recovery_time - outage_start\nprint(f'RTO: {rto_minutes} minutes')",
        "o": [
            "RTO: 15 minutes",
            "RTO: 0 minutes",
            "RTO: -15 minutes",
            "Error"
        ]
    },
    {
        "q": "The _____ defines acceptable data loss in disaster recovery.",
        "type": "fill_blank",
        "answers": [
            "RPO"
        ],
        "other_options": [
            "RTO",
            "SLA",
            "KPI"
        ]
    },
    {
        "q": "Match the recovery metric with its definition:",
        "type": "match",
        "left": [
            "RTO",
            "RPO",
            "MTTR",
            "MTBF"
        ],
        "right": [
            "Time to recover",
            "Data loss tolerance",
            "Mean repair time",
            "Failure interval"
        ]
    },
    {
        "q": "What is split-brain in distributed systems?",
        "type": "mcq",
        "o": [
            "Multiple leaders operating independently",
            "Efficient data partitioning",
            "Optimal load balancing",
            "Fast network connection"
        ]
    },
    {
        "q": "Fencing prevents split-brain scenarios.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this leader check?",
        "type": "mcq",
        "c": "nodes = {'node1': 'leader', 'node2': 'follower', 'node3': 'follower'}\nleaders = [n for n, r in nodes.items() if r == 'leader']\nprint(f'{len(leaders)} leader(s)')",
        "o": [
            "1 leader(s)",
            "3 leader(s)",
            "0 leader(s)",
            "Error"
        ]
    },
    {
        "q": "Which technique handles network partitions?",
        "type": "mcq",
        "o": [
            "Partition tolerance",
            "File compression",
            "Data encryption",
            "Code formatting"
        ]
    },
    {
        "q": "The _____ problem describes unreliable message delivery.",
        "type": "fill_blank",
        "answers": [
            "Byzantine generals"
        ],
        "other_options": [
            "Dining philosophers",
            "Producer-consumer",
            "Reader-writer"
        ]
    },
    {
        "q": "Rearrange the partition handling steps:",
        "type": "rearrange",
        "words": [
            "Detect partition",
            "Isolate affected",
            "Continue processing",
            "Heal partition",
            "Reconcile data"
        ]
    },
    {
        "q": "What is the output of this partition detection?",
        "type": "mcq",
        "c": "cluster = {'region1': ['a', 'b'], 'region2': ['c', 'd', 'e']}\npartitioned = len(cluster) > 1 and all(len(v) > 0 for v in cluster.values())\nprint(f'Partitioned: {partitioned}')",
        "o": [
            "Partitioned: True",
            "Partitioned: False",
            "2",
            "Error"
        ]
    },
    {
        "q": "Match the failure mode with its behavior:",
        "type": "match",
        "left": [
            "Fail-stop",
            "Byzantine",
            "Crash",
            "Omission"
        ],
        "right": [
            "Clean failure",
            "Arbitrary behavior",
            "Process stops",
            "Lost messages"
        ]
    },
    {
        "q": "Vector clocks track causal ordering of events.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this vector clock update?",
        "type": "mcq",
        "c": "clock = {'node1': 3, 'node2': 5, 'node3': 2}\nclock['node1'] += 1\nprint(clock['node1'])",
        "o": [
            "4",
            "3",
            "5",
            "Error"
        ]
    },
    {
        "q": "Which structure resolves concurrent updates?",
        "type": "mcq",
        "o": [
            "CRDT",
            "CSV",
            "JSON",
            "XML"
        ]
    },
    {
        "q": "The _____ data type converges automatically without coordination.",
        "type": "fill_blank",
        "answers": [
            "CRDT"
        ],
        "other_options": [
            "ACID",
            "BASE",
            "REST"
        ]
    },
    {
        "q": "Match the CRDT type with its operation:",
        "type": "match",
        "left": [
            "G-Counter",
            "PN-Counter",
            "LWW-Register",
            "OR-Set"
        ],
        "right": [
            "Increment only",
            "Inc and dec",
            "Last write wins",
            "Add and remove"
        ]
    },
    {
        "q": "What is the output of this CRDT merge?",
        "type": "mcq",
        "c": "counter1 = {'node1': 5, 'node2': 3}\ncounter2 = {'node1': 4, 'node2': 6}\nmerged = {k: max(counter1.get(k, 0), counter2.get(k, 0)) for k in set(counter1) | set(counter2)}\nprint(sum(merged.values()))",
        "o": [
            "11",
            "8",
            "10",
            "Error"
        ]
    },
    {
        "q": "Rearrange the conflict resolution strategies by complexity:",
        "type": "rearrange",
        "words": [
            "Last write wins",
            "Merge function",
            "CRDT",
            "Manual resolution",
            "Application logic"
        ]
    },
    {
        "q": "What is optimistic locking?",
        "type": "mcq",
        "o": [
            "Check for conflicts at commit time",
            "Lock resources immediately",
            "Prevent all concurrent access",
            "Ignore all conflicts"
        ]
    },
    {
        "q": "Pessimistic locking acquires locks before operations.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output of this version check?",
        "type": "mcq",
        "c": "stored_version = 5\nread_version = 5\ncan_update = stored_version == read_version\nprint(f'Update allowed: {can_update}')",
        "o": [
            "Update allowed: True",
            "Update allowed: False",
            "5",
            "Error"
        ]
    },
    {
        "q": "The _____ strategy retries on conflict detection.",
        "type": "fill_blank",
        "answers": [
            "optimistic"
        ],
        "other_options": [
            "pessimistic",
            "blocking",
            "waiting"
        ]
    },
    {
        "q": "Match the locking strategy with its trade-off:",
        "type": "match",
        "left": [
            "Optimistic",
            "Pessimistic",
            "No locking",
            "Distributed"
        ],
        "right": [
            "Retry on conflict",
            "Wait for lock",
            "Data corruption",
            "Coordination overhead"
        ]
    },
    {
        "q": "What is the output of this lock timeout?",
        "type": "mcq",
        "c": "lock_acquired_at = 100\ncurrent_time = 130\ntimeout_seconds = 30\nexpired = (current_time - lock_acquired_at) >= timeout_seconds\nprint(f'Lock expired: {expired}')",
        "o": [
            "Lock expired: True",
            "Lock expired: False",
            "30",
            "Error"
        ]
    },
    {
        "q": "Deadlock occurs when processes wait for each other indefinitely.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the deadlock prevention strategies:",
        "type": "rearrange",
        "words": [
            "Acquire in order",
            "Use timeouts",
            "Detect cycles",
            "Release and retry",
            "Abort transactions"
        ]
    },
    {
        "q": "What is the output of this cycle detection?",
        "type": "mcq",
        "c": "waits_for = {'A': 'B', 'B': 'C', 'C': 'A'}\nchain = ['A']\ncurrent = 'A'\nfor _ in range(len(waits_for)):\n    current = waits_for.get(current)\n    chain.append(current)\nhas_cycle = chain[0] == chain[-1]\nprint(f'Deadlock: {has_cycle}')",
        "o": [
            "Deadlock: True",
            "Deadlock: False",
            "['A', 'B', 'C', 'A']",
            "Error"
        ]
    },
    {
        "q": "Which technique prevents write skew anomalies?",
        "type": "mcq",
        "o": [
            "Serializable isolation",
            "Read uncommitted",
            "No isolation",
            "Simple caching"
        ]
    },
    {
        "q": "The _____ anomaly occurs when concurrent transactions violate constraints.",
        "type": "fill_blank",
        "answers": [
            "write skew"
        ],
        "other_options": [
            "dirty read",
            "lost update",
            "phantom read"
        ]
    },
    {
        "q": "Match the isolation level with its protection:",
        "type": "match",
        "left": [
            "Read uncommitted",
            "Read committed",
            "Repeatable read",
            "Serializable"
        ],
        "right": [
            "None",
            "Dirty reads",
            "Non-repeatable",
            "All anomalies"
        ]
    },
    {
        "q": "What is the output of this snapshot isolation check?",
        "type": "mcq",
        "c": "transaction_start = 100\ndata_version = 95\nread_allowed = data_version <= transaction_start\nprint(f'Read consistent: {read_allowed}')",
        "o": [
            "Read consistent: True",
            "Read consistent: False",
            "100",
            "Error"
        ]
    },
    {
        "q": "MVCC maintains multiple versions of data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the transaction lifecycle:",
        "type": "rearrange",
        "words": [
            "Begin",
            "Read data",
            "Write changes",
            "Validate",
            "Commit or abort"
        ]
    },
    {
        "q": "What is the output of this MVCC visibility check?",
        "type": "mcq",
        "c": "versions = [{'version': 1, 'value': 'A'}, {'version': 5, 'value': 'B'}, {'version': 10, 'value': 'C'}]\ntxn_snapshot = 7\nvisible = [v for v in versions if v['version'] <= txn_snapshot]\nprint(visible[-1]['value'])",
        "o": [
            "B",
            "A",
            "C",
            "Error"
        ]
    },
    {
        "q": "Which technique garbage collects old versions?",
        "type": "mcq",
        "o": [
            "Version pruning",
            "Data compression",
            "Schema evolution",
            "Index creation"
        ]
    },
    {
        "q": "The _____ determines when old versions can be removed.",
        "type": "fill_blank",
        "answers": [
            "low watermark"
        ],
        "other_options": [
            "high watermark",
            "threshold",
            "limit"
        ]
    },
    {
        "q": "Match the MVCC concept with its purpose:",
        "type": "match",
        "left": [
            "Snapshot",
            "Version chain",
            "Visibility",
            "Garbage collection"
        ],
        "right": [
            "Point-in-time view",
            "Data history",
            "Transaction access",
            "Clean old data"
        ]
    },
    {
        "q": "What is the output of this garbage collection eligibility?",
        "type": "mcq",
        "c": "oldest_active_txn = 100\nversion_timestamp = 50\ncan_gc = version_timestamp < oldest_active_txn\nprint(f'Can GC: {can_gc}')",
        "o": [
            "Can GC: True",
            "Can GC: False",
            "50",
            "Error"
        ]
    },
    {
        "q": "Write-ahead logging ensures durability.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the WAL recovery steps:",
        "type": "rearrange",
        "words": [
            "Read log",
            "Redo committed",
            "Undo uncommitted",
            "Checkpoint",
            "Resume operations"
        ]
    },
    {
        "q": "What is the output of this LSN calculation?",
        "type": "mcq",
        "c": "current_lsn = 1000\nlog_record_size = 128\nnext_lsn = current_lsn + log_record_size\nprint(f'Next LSN: {next_lsn}')",
        "o": [
            "Next LSN: 1128",
            "Next LSN: 1000",
            "Next LSN: 128",
            "Error"
        ]
    },
    {
        "q": "Which operation flushes WAL to stable storage?",
        "type": "mcq",
        "o": [
            "fsync",
            "malloc",
            "free",
            "fork"
        ]
    },
    {
        "q": "The _____ record marks the end of a committed transaction.",
        "type": "fill_blank",
        "answers": [
            "commit"
        ],
        "other_options": [
            "begin",
            "abort",
            "checkpoint"
        ]
    },
    {
        "q": "Match the log record type with its purpose:",
        "type": "match",
        "left": [
            "Begin",
            "Update",
            "Commit",
            "Abort"
        ],
        "right": [
            "Start transaction",
            "Data change",
            "Complete success",
            "Rollback"
        ]
    },
    {
        "q": "What is the output of this checkpoint creation?",
        "type": "mcq",
        "c": "active_transactions = [101, 102, 103]\ncheckpoint_lsn = 5000\nrecovery_start = min([checkpoint_lsn] + active_transactions)\nprint(f'Recovery from: {recovery_start}')",
        "o": [
            "Recovery from: 101",
            "Recovery from: 5000",
            "Recovery from: 103",
            "Error"
        ]
    },
    {
        "q": "Checkpoints reduce recovery time.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the crash recovery phases:",
        "type": "rearrange",
        "words": [
            "Analysis",
            "Redo",
            "Undo",
            "Verify",
            "Complete"
        ]
    },
    {
        "q": "What is the output of this redo analysis?",
        "type": "mcq",
        "c": "log_records = [{'lsn': 100, 'type': 'update'}, {'lsn': 200, 'type': 'commit'}, {'lsn': 300, 'type': 'update'}]\nredo_needed = [r for r in log_records if r['type'] == 'update']\nprint(len(redo_needed))",
        "o": [
            "2",
            "3",
            "1",
            "Error"
        ]
    },
    {
        "q": "Which storage engine uses LSM trees?",
        "type": "mcq",
        "o": [
            "RocksDB",
            "PostgreSQL",
            "Oracle",
            "SQL Server"
        ]
    },
    {
        "q": "The _____ compacts SSTables in LSM trees.",
        "type": "fill_blank",
        "answers": [
            "compaction"
        ],
        "other_options": [
            "indexing",
            "caching",
            "buffering"
        ]
    },
    {
        "q": "Match the storage structure with its use:",
        "type": "match",
        "left": [
            "B-tree",
            "LSM tree",
            "Hash index",
            "Bitmap"
        ],
        "right": [
            "Read-heavy",
            "Write-heavy",
            "Exact match",
            "Low cardinality"
        ]
    },
    {
        "q": "What is the output of this compaction simulation?",
        "type": "mcq",
        "c": "levels = [10, 5, 2, 1]\ntotal_files = sum(levels)\nprint(f'{total_files} files to compact')",
        "o": [
            "18 files to compact",
            "10 files to compact",
            "4 files to compact",
            "Error"
        ]
    },
    {
        "q": "Write amplification is a concern in LSM trees.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the LSM write path:",
        "type": "rearrange",
        "words": [
            "Write memtable",
            "Flush to disk",
            "Create SSTable",
            "Compact levels",
            "Merge files"
        ]
    },
    {
        "q": "What is the output of this amplification calculation?",
        "type": "mcq",
        "c": "bytes_written_disk = 10000\nbytes_written_app = 1000\namplification = bytes_written_disk / bytes_written_app\nprint(f'{amplification}x amplification')",
        "o": [
            "10.0x amplification",
            "1.0x amplification",
            "1000x amplification",
            "Error"
        ]
    },
    {
        "q": "Which technique improves read performance in LSM?",
        "type": "mcq",
        "o": [
            "Bloom filter",
            "Full table scan",
            "Sequential write",
            "Memory flush"
        ]
    },
    {
        "q": "The _____ filter quickly checks if a key might exist.",
        "type": "fill_blank",
        "answers": [
            "Bloom"
        ],
        "other_options": [
            "Hash",
            "Tree",
            "Index"
        ]
    },
    {
        "q": "Match the optimization with its target:",
        "type": "match",
        "left": [
            "Bloom filter",
            "Block cache",
            "Compression",
            "Prefix seek"
        ],
        "right": [
            "Skip non-existent",
            "Hot data",
            "Storage size",
            "Range queries"
        ]
    },
    {
        "q": "What is the output of this bloom filter check?",
        "type": "mcq",
        "c": "bloom_positives = {'key1', 'key2', 'key3'}\nquery_key = 'key2'\nmight_exist = query_key in bloom_positives\nprint(f'Might exist: {might_exist}')",
        "o": [
            "Might exist: True",
            "Might exist: False",
            "key2",
            "Error"
        ]
    },
    {
        "q": "Bloom filters can have false positives but not false negatives.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the data lake zones:",
        "type": "rearrange",
        "words": [
            "Landing",
            "Raw",
            "Curated",
            "Enriched",
            "Consumption"
        ]
    },
    {
        "q": "What is the output of this zone classification?",
        "type": "mcq",
        "c": "zones = {'raw': 1, 'curated': 2, 'consumption': 3}\ndata_quality = 'curated'\nlevel = zones.get(data_quality, 0)\nprint(f'Quality level: {level}')",
        "o": [
            "Quality level: 2",
            "Quality level: 1",
            "Quality level: 3",
            "Error"
        ]
    },
    {
        "q": "Which format supports schema evolution natively?",
        "type": "mcq",
        "o": [
            "Avro",
            "CSV",
            "Fixed-width",
            "Plain text"
        ]
    },
    {
        "q": "The _____ catalog manages metadata in data lakes.",
        "type": "fill_blank",
        "answers": [
            "data"
        ],
        "other_options": [
            "file",
            "system",
            "network"
        ]
    },
    {
        "q": "Match the data lake tool with its function:",
        "type": "match",
        "left": [
            "Glue Catalog",
            "Atlas",
            "DataHub",
            "Amundsen"
        ],
        "right": [
            "AWS metadata",
            "Apache governance",
            "LinkedIn discovery",
            "Lyft discovery"
        ]
    },
    {
        "q": "What is the output of this partition path generation?",
        "type": "mcq",
        "c": "year = 2024\nmonth = 6\nday = 15\npath = f'/data/year={year}/month={month:02d}/day={day:02d}'\nprint(path)",
        "o": [
            "/data/year=2024/month=06/day=15",
            "/data/2024/06/15",
            "/data/year=2024/month=6/day=15",
            "Error"
        ]
    },
    {
        "q": "Hive-style partitioning uses key=value directory names.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the data governance pillars:",
        "type": "rearrange",
        "words": [
            "Data quality",
            "Data security",
            "Data privacy",
            "Data catalog",
            "Data stewardship"
        ]
    },
    {
        "q": "What is the output of this access control check?",
        "type": "mcq",
        "c": "user_roles = {'alice': ['analyst', 'viewer'], 'bob': ['admin']}\nrequired_role = 'admin'\nuser = 'bob'\nhas_access = required_role in user_roles.get(user, [])\nprint(f'Access granted: {has_access}')",
        "o": [
            "Access granted: True",
            "Access granted: False",
            "admin",
            "Error"
        ]
    },
    {
        "q": "Which compliance regulation protects EU citizen data?",
        "type": "mcq",
        "o": [
            "GDPR",
            "HIPAA",
            "SOX",
            "PCI-DSS"
        ]
    },
    {
        "q": "The _____ right allows individuals to request data deletion.",
        "type": "fill_blank",
        "answers": [
            "erasure"
        ],
        "other_options": [
            "access",
            "portability",
            "rectification"
        ]
    },
    {
        "q": "Match the regulation with its scope:",
        "type": "match",
        "left": [
            "GDPR",
            "HIPAA",
            "CCPA",
            "PCI-DSS"
        ],
        "right": [
            "EU data protection",
            "US healthcare",
            "California privacy",
            "Payment cards"
        ]
    },
    {
        "q": "What is the output of this PII detection?",
        "type": "mcq",
        "c": "record = {'name': 'John Doe', 'ssn': '123-45-6789', 'city': 'NYC'}\npii_fields = ['ssn', 'name']\nfound_pii = [f for f in pii_fields if f in record]\nprint(len(found_pii))",
        "o": [
            "2",
            "3",
            "1",
            "Error"
        ]
    },
    {
        "q": "Data masking protects sensitive information in non-production.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the data classification levels:",
        "type": "rearrange",
        "words": [
            "Public",
            "Internal",
            "Confidential",
            "Restricted",
            "Top Secret"
        ]
    },
    {
        "q": "What is the output of this classification assignment?",
        "type": "mcq",
        "c": "classification_rules = {'ssn': 'restricted', 'email': 'confidential', 'city': 'internal'}\nfield = 'ssn'\nlevel = classification_rules.get(field, 'public')\nprint(f'Classification: {level}')",
        "o": [
            "Classification: restricted",
            "Classification: public",
            "Classification: confidential",
            "Error"
        ]
    },
    {
        "q": "Which technique anonymizes data while preserving utility?",
        "type": "mcq",
        "o": [
            "K-anonymity",
            "Full deletion",
            "Plain encryption",
            "Data duplication"
        ]
    },
    {
        "q": "The _____ ensures that at least k individuals share the same attributes.",
        "type": "fill_blank",
        "answers": [
            "k-anonymity"
        ],
        "other_options": [
            "l-diversity",
            "t-closeness",
            "differential privacy"
        ]
    },
    {
        "q": "Match the privacy technique with its approach:",
        "type": "match",
        "left": [
            "Generalization",
            "Suppression",
            "Perturbation",
            "Synthetic"
        ],
        "right": [
            "Reduce precision",
            "Remove values",
            "Add noise",
            "Generate fake"
        ]
    },
    {
        "q": "What is the output of this age generalization?",
        "type": "mcq",
        "c": "age = 37\nbucket_size = 10\ngeneralized = (age // bucket_size) * bucket_size\nprint(f'Age bucket: {generalized}-{generalized + bucket_size - 1}')",
        "o": [
            "Age bucket: 30-39",
            "Age bucket: 37-46",
            "Age bucket: 40-49",
            "Error"
        ]
    },
    {
        "q": "Differential privacy adds controlled noise to query results.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the privacy-preserving data pipeline:",
        "type": "rearrange",
        "words": [
            "Identify PII",
            "Classify data",
            "Apply masking",
            "Validate output",
            "Audit access"
        ]
    },
    {
        "q": "What is the output of this noise addition?",
        "type": "mcq",
        "c": "true_count = 1000\nepsilon = 1.0\n# Simulated noise\nnoise = 50\nnoisy_count = true_count + noise\nprint(f'Noisy result: {noisy_count}')",
        "o": [
            "Noisy result: 1050",
            "Noisy result: 1000",
            "Noisy result: 50",
            "Error"
        ]
    },
    {
        "q": "Which tool automates ETL infrastructure deployment?",
        "type": "mcq",
        "o": [
            "Terraform",
            "Excel",
            "Word",
            "PowerPoint"
        ]
    },
    {
        "q": "The _____ pattern manages infrastructure as code.",
        "type": "fill_blank",
        "answers": [
            "IaC"
        ],
        "other_options": [
            "ETL",
            "ELT",
            "CDC"
        ]
    },
    {
        "q": "Match the IaC tool with its provider:",
        "type": "match",
        "left": [
            "Terraform",
            "CloudFormation",
            "Pulumi",
            "Ansible"
        ],
        "right": [
            "HashiCorp",
            "AWS",
            "Code-first",
            "Red Hat"
        ]
    },
    {
        "q": "What is the output of this resource count?",
        "type": "mcq",
        "c": "resources = {'s3_buckets': 3, 'lambda_functions': 5, 'databases': 2}\ntotal = sum(resources.values())\nprint(f'{total} resources')",
        "o": [
            "10 resources",
            "3 resources",
            "5 resources",
            "Error"
        ]
    },
    {
        "q": "GitOps uses Git as the source of truth for deployments.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the CI/CD pipeline stages:",
        "type": "rearrange",
        "words": [
            "Build",
            "Test",
            "Security scan",
            "Deploy staging",
            "Deploy production"
        ]
    },
    {
        "q": "What is the output of this test coverage check?",
        "type": "mcq",
        "c": "covered_lines = 850\ntotal_lines = 1000\ncoverage = (covered_lines / total_lines) * 100\nprint(f'{coverage}% coverage')",
        "o": [
            "85.0% coverage",
            "850% coverage",
            "100% coverage",
            "Error"
        ]
    },
    {
        "q": "Blue-green deployment minimizes downtime.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the deployment strategy with its approach:",
        "type": "match",
        "left": [
            "Blue-green",
            "Canary",
            "Rolling",
            "A/B testing"
        ],
        "right": [
            "Full switch",
            "Gradual rollout",
            "Incremental",
            "Feature comparison"
        ]
    },
    {
        "q": "The _____ deployment tests new versions with a subset of traffic.",
        "type": "fill_blank",
        "answers": [
            "canary"
        ],
        "other_options": [
            "blue-green",
            "rolling",
            "immediate"
        ]
    },
    {
        "q": "What is the output of this traffic split calculation?",
        "type": "mcq",
        "c": "total_requests = 10000\ncanary_percent = 5\ncanary_requests = total_requests * (canary_percent / 100)\nprint(f'{int(canary_requests)} canary requests')",
        "o": [
            "500 canary requests",
            "10000 canary requests",
            "5 canary requests",
            "Error"
        ]
    },
    {
        "q": "What is the output of this feature flag check?",
        "type": "mcq",
        "c": "features = {'new_pipeline': True, 'legacy_mode': False}\nfeature = 'new_pipeline'\nenabled = features.get(feature, False)\nprint(f'Feature enabled: {enabled}')",
        "o": [
            "Feature enabled: True",
            "Feature enabled: False",
            "new_pipeline",
            "Error"
        ]
    },
    {
        "q": "Which technique enables gradual feature rollout?",
        "type": "mcq",
        "o": [
            "Feature flags",
            "Full deployment",
            "Immediate release",
            "Hard coding"
        ]
    },
    {
        "q": "Feature toggles allow runtime configuration changes.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "The _____ pattern enables testing in production safely.",
        "type": "fill_blank",
        "answers": [
            "dark launch"
        ],
        "other_options": [
            "hard launch",
            "full launch",
            "immediate launch"
        ]
    },
    {
        "q": "Match the release strategy with its risk level:",
        "type": "match",
        "left": [
            "Big bang",
            "Phased",
            "Canary",
            "Shadow"
        ],
        "right": [
            "Highest risk",
            "Medium risk",
            "Low risk",
            "No user impact"
        ]
    },
    {
        "q": "Rearrange the rollback procedure:",
        "type": "rearrange",
        "words": [
            "Detect issue",
            "Assess severity",
            "Trigger rollback",
            "Verify rollback",
            "Post-mortem"
        ]
    },
    {
        "q": "What is the output of this rollback decision?",
        "type": "mcq",
        "c": "error_rate = 0.15\nthreshold = 0.05\nshould_rollback = error_rate > threshold\nprint(f'Rollback: {should_rollback}')",
        "o": [
            "Rollback: True",
            "Rollback: False",
            "0.15",
            "Error"
        ]
    },
    {
        "q": "Which metric indicates ETL pipeline health?",
        "type": "mcq",
        "o": [
            "Latency",
            "Font size",
            "Color scheme",
            "Button style"
        ]
    },
    {
        "q": "SLIs measure the quality of service provided.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "The _____ defines the acceptable threshold for service quality.",
        "type": "fill_blank",
        "answers": [
            "SLO"
        ],
        "other_options": [
            "SLI",
            "SLA",
            "KPI"
        ]
    },
    {
        "q": "Match the metric type with its measurement:",
        "type": "match",
        "left": [
            "Latency",
            "Throughput",
            "Error rate",
            "Availability"
        ],
        "right": [
            "Response time",
            "Records per second",
            "Failed requests",
            "Uptime percentage"
        ]
    },
    {
        "q": "What is the output of this error budget calculation?",
        "type": "mcq",
        "c": "slo_target = 0.999\nactual_availability = 0.997\nerror_budget = slo_target - actual_availability\nprint(f'Budget remaining: {error_budget * 100:.2f}%')",
        "o": [
            "Budget remaining: 0.20%",
            "Budget remaining: 99.70%",
            "Budget remaining: 99.90%",
            "Error"
        ]
    },
    {
        "q": "Error budgets encourage calculated risk-taking.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the SRE incident response levels:",
        "type": "rearrange",
        "words": [
            "P0 Critical",
            "P1 High",
            "P2 Medium",
            "P3 Low",
            "P4 Informational"
        ]
    },
    {
        "q": "What is the output of this toil calculation?",
        "type": "mcq",
        "c": "manual_tasks_hours = 20\ntotal_hours = 40\ntoil_percent = (manual_tasks_hours / total_hours) * 100\nprint(f'{toil_percent}% toil')",
        "o": [
            "50.0% toil",
            "20% toil",
            "40% toil",
            "Error"
        ]
    },
    {
        "q": "Which approach reduces manual operational work?",
        "type": "mcq",
        "o": [
            "Automation",
            "More meetings",
            "Paper documentation",
            "Manual processes"
        ]
    },
    {
        "q": "The _____ eliminates repetitive manual tasks.",
        "type": "fill_blank",
        "answers": [
            "automation"
        ],
        "other_options": [
            "documentation",
            "meetings",
            "reviews"
        ]
    },
    {
        "q": "Match the automation level with its maturity:",
        "type": "match",
        "left": [
            "Manual",
            "Scripted",
            "Scheduled",
            "Self-healing"
        ],
        "right": [
            "Level 0",
            "Level 1",
            "Level 2",
            "Level 3"
        ]
    },
    {
        "q": "What is the output of this automation ROI?",
        "type": "mcq",
        "c": "hours_saved_per_week = 10\nweeks_per_year = 52\nhourly_rate = 50\nannual_savings = hours_saved_per_week * weeks_per_year * hourly_rate\nprint(f'${annual_savings} saved')",
        "o": [
            "$26000 saved",
            "$520 saved",
            "$500 saved",
            "Error"
        ]
    },
    {
        "q": "Self-healing systems recover automatically from failures.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the automation maturity ladder:",
        "type": "rearrange",
        "words": [
            "Manual execution",
            "Script library",
            "Automated triggers",
            "Self-service portal",
            "AI-driven ops"
        ]
    },
    {
        "q": "What is the output of this capacity forecast?",
        "type": "mcq",
        "c": "current_usage_gb = 500\ngrowth_rate = 0.10\nmonths = 12\nforecasted = current_usage_gb * ((1 + growth_rate) ** (months / 12))\nprint(f'{forecasted:.0f} GB')",
        "o": [
            "550 GB",
            "500 GB",
            "600 GB",
            "Error"
        ]
    },
    {
        "q": "Which planning ensures adequate resources?",
        "type": "mcq",
        "o": [
            "Capacity planning",
            "Random allocation",
            "Ignoring growth",
            "Reactive scaling"
        ]
    },
    {
        "q": "The _____ model predicts future resource needs.",
        "type": "fill_blank",
        "answers": [
            "forecasting"
        ],
        "other_options": [
            "historical",
            "random",
            "static"
        ]
    },
    {
        "q": "Match the scaling strategy with its trigger:",
        "type": "match",
        "left": [
            "Scheduled",
            "Reactive",
            "Predictive",
            "Manual"
        ],
        "right": [
            "Time-based",
            "Threshold-based",
            "ML-based",
            "Human decision"
        ]
    },
    {
        "q": "What is the output of this scaling decision?",
        "type": "mcq",
        "c": "cpu_usage = 85\nscale_up_threshold = 80\nscale_down_threshold = 30\naction = 'scale_up' if cpu_usage > scale_up_threshold else 'scale_down' if cpu_usage < scale_down_threshold else 'maintain'\nprint(action)",
        "o": [
            "scale_up",
            "scale_down",
            "maintain",
            "Error"
        ]
    },
    {
        "q": "Horizontal scaling adds more instances.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the cost optimization priorities:",
        "type": "rearrange",
        "words": [
            "Right-size resources",
            "Use spot instances",
            "Implement autoscaling",
            "Reserved capacity",
            "Review regularly"
        ]
    },
    {
        "q": "What is the output of this cost calculation?",
        "type": "mcq",
        "c": "on_demand_cost = 1000\nreserved_cost = 600\nsavings = ((on_demand_cost - reserved_cost) / on_demand_cost) * 100\nprint(f'{savings}% savings')",
        "o": [
            "40.0% savings",
            "60% savings",
            "100% savings",
            "Error"
        ]
    },
    {
        "q": "Which instance type offers lowest cost at risk of interruption?",
        "type": "mcq",
        "o": [
            "Spot instance",
            "On-demand",
            "Reserved",
            "Dedicated"
        ]
    },
    {
        "q": "The _____ pricing model offers discounts for commitment.",
        "type": "fill_blank",
        "answers": [
            "reserved"
        ],
        "other_options": [
            "on-demand",
            "spot",
            "pay-as-you-go"
        ]
    },
    {
        "q": "Match the cost optimization technique with its benefit:",
        "type": "match",
        "left": [
            "Right-sizing",
            "Spot instances",
            "Autoscaling",
            "Data tiering"
        ],
        "right": [
            "Eliminate waste",
            "Cheap capacity",
            "Match demand",
            "Storage costs"
        ]
    },
    {
        "q": "What is the output of this data tiering calculation?",
        "type": "mcq",
        "c": "hot_storage_cost = 0.10\ncold_storage_cost = 0.01\ndata_gb = 1000\ncold_ratio = 0.8\ntotal = (data_gb * (1 - cold_ratio) * hot_storage_cost) + (data_gb * cold_ratio * cold_storage_cost)\nprint(f'${total:.0f} monthly')",
        "o": [
            "$28 monthly",
            "$100 monthly",
            "$10 monthly",
            "Error"
        ]
    },
    {
        "q": "Data lifecycle policies automate tier transitions.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the data lifecycle stages:",
        "type": "rearrange",
        "words": [
            "Active",
            "Warm",
            "Cold",
            "Archive",
            "Delete"
        ]
    },
    {
        "q": "What is the output of this retention check?",
        "type": "mcq",
        "c": "from datetime import datetime, timedelta\ncreated = datetime(2024, 1, 1)\nnow = datetime(2024, 7, 1)\nretention_days = 90\nage_days = (now - created).days\nshould_delete = age_days > retention_days\nprint(f'Delete: {should_delete}')",
        "o": [
            "Delete: True",
            "Delete: False",
            "90",
            "Error"
        ]
    },
    {
        "q": "Which approach optimizes storage costs long-term?",
        "type": "mcq",
        "o": [
            "Data lifecycle management",
            "Keep all data forever",
            "Store only in hot tier",
            "Ignore storage costs"
        ]
    },
    {
        "q": "The _____ policy defines when data should be moved or deleted.",
        "type": "fill_blank",
        "answers": [
            "retention"
        ],
        "other_options": [
            "creation",
            "access",
            "modification"
        ]
    },
    {
        "q": "Match the storage tier with its access pattern:",
        "type": "match",
        "left": [
            "Hot",
            "Warm",
            "Cold",
            "Archive"
        ],
        "right": [
            "Frequent access",
            "Occasional access",
            "Rare access",
            "Compliance only"
        ]
    },
    {
        "q": "What is the output of this compression savings?",
        "type": "mcq",
        "c": "original_tb = 100\ncompressed_tb = 25\nstorage_cost_per_tb = 20\nsavings = (original_tb - compressed_tb) * storage_cost_per_tb\nprint(f'${savings} saved')",
        "o": [
            "$1500 saved",
            "$500 saved",
            "$2000 saved",
            "Error"
        ]
    },
    {
        "q": "Columnar compression achieves higher ratios for analytics.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the cost allocation process:",
        "type": "rearrange",
        "words": [
            "Tag resources",
            "Collect costs",
            "Allocate to teams",
            "Generate reports",
            "Optimize spend"
        ]
    },
    {
        "q": "What is the output of this chargeback calculation?",
        "type": "mcq",
        "c": "total_cost = 10000\nteam_usage = {'eng': 0.6, 'data': 0.3, 'ops': 0.1}\ndata_charge = total_cost * team_usage['data']\nprint(f'Data team: ${data_charge}')",
        "o": [
            "Data team: $3000.0",
            "Data team: $6000.0",
            "Data team: $1000.0",
            "Error"
        ]
    },
    {
        "q": "Which practice promotes cost accountability?",
        "type": "mcq",
        "o": [
            "Chargeback",
            "Free resources",
            "Hidden costs",
            "Central buying"
        ]
    },
    {
        "q": "The _____ model allocates costs to consuming teams.",
        "type": "fill_blank",
        "answers": [
            "chargeback"
        ],
        "other_options": [
            "showback",
            "sunkcost",
            "overhead"
        ]
    },
    {
        "q": "Match the FinOps practice with its goal:",
        "type": "match",
        "left": [
            "Visibility",
            "Optimization",
            "Governance",
            "Forecasting"
        ],
        "right": [
            "See spending",
            "Reduce waste",
            "Set controls",
            "Predict costs"
        ]
    },
    {
        "q": "What is the output of this budget alert?",
        "type": "mcq",
        "c": "budget = 10000\nspent = 8500\nalert_threshold = 0.80\nalert_triggered = spent >= (budget * alert_threshold)\nprint(f'Alert: {alert_triggered}')",
        "o": [
            "Alert: True",
            "Alert: False",
            "8500",
            "Error"
        ]
    },
    {
        "q": "FinOps promotes collaboration between finance and engineering.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the FinOps lifecycle:",
        "type": "rearrange",
        "words": [
            "Inform",
            "Optimize",
            "Operate",
            "Iterate",
            "Mature"
        ]
    },
    {
        "q": "What is the output of this unit cost calculation?",
        "type": "mcq",
        "c": "total_cost = 5000\nrecords_processed = 1000000000\ncost_per_million = (total_cost / records_processed) * 1000000\nprint(f'${cost_per_million:.2f} per million')",
        "o": [
            "$5.00 per million",
            "$0.005 per million",
            "$5000 per million",
            "Error"
        ]
    },
    {
        "q": "Which metric measures efficiency of data processing?",
        "type": "mcq",
        "o": [
            "Cost per record",
            "Total spend",
            "Team size",
            "Meeting count"
        ]
    },
    {
        "q": "The _____ ratio compares business value to cost.",
        "type": "fill_blank",
        "answers": [
            "ROI"
        ],
        "other_options": [
            "TCO",
            "NPV",
            "IRR"
        ]
    },
    {
        "q": "Match the efficiency metric with its formula:",
        "type": "match",
        "left": [
            "Throughput",
            "Latency",
            "Utilization",
            "Cost efficiency"
        ],
        "right": [
            "Records/second",
            "Time to process",
            "Used/Total",
            "Value/Cost"
        ]
    },
    {
        "q": "What is the output of this efficiency check?",
        "type": "mcq",
        "c": "capacity = 1000\nused = 750\nutilization = (used / capacity) * 100\nprint(f'{utilization}% utilized')",
        "o": [
            "75.0% utilized",
            "25% utilized",
            "100% utilized",
            "Error"
        ]
    },
    {
        "q": "High utilization may indicate capacity risk.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the performance tuning process:",
        "type": "rearrange",
        "words": [
            "Baseline",
            "Identify bottleneck",
            "Tune configuration",
            "Measure impact",
            "Document changes"
        ]
    },
    {
        "q": "What is the output of this bottleneck detection?",
        "type": "mcq",
        "c": "stage_times = {'extract': 10, 'transform': 45, 'load': 5}\nbottleneck = max(stage_times, key=stage_times.get)\nprint(f'Bottleneck: {bottleneck}')",
        "o": [
            "Bottleneck: transform",
            "Bottleneck: extract",
            "Bottleneck: load",
            "Error"
        ]
    },
    {
        "q": "Which law states that speedup is limited by the sequential portion?",
        "type": "mcq",
        "o": [
            "Amdahl's law",
            "Moore's law",
            "Murphy's law",
            "Pareto principle"
        ]
    },
    {
        "q": "The _____ identifies the slowest component in a pipeline.",
        "type": "fill_blank",
        "answers": [
            "bottleneck"
        ],
        "other_options": [
            "optimizer",
            "accelerator",
            "cache"
        ]
    },
    {
        "q": "Match the optimization technique with its target:",
        "type": "match",
        "left": [
            "Indexing",
            "Caching",
            "Parallelism",
            "Batching"
        ],
        "right": [
            "Query speed",
            "Repeated reads",
            "CPU usage",
            "I/O efficiency"
        ]
    },
    {
        "q": "What is the output of this parallel speedup?",
        "type": "mcq",
        "c": "sequential_time = 100\nparallel_time = 25\nspeedup = sequential_time / parallel_time\nprint(f'{speedup}x speedup')",
        "o": [
            "4.0x speedup",
            "25x speedup",
            "100x speedup",
            "Error"
        ]
    },
    {
        "q": "Micro-batching balances latency and throughput.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the optimization priorities:",
        "type": "rearrange",
        "words": [
            "Measure baseline",
            "Identify hotspots",
            "Apply fixes",
            "Verify improvement",
            "Monitor continuously"
        ]
    },
    {
        "q": "What is the output of this memory calculation?",
        "type": "mcq",
        "c": "records = 1000000\nbytes_per_record = 1024\ntotal_mb = (records * bytes_per_record) / (1024 * 1024)\nprint(f'{total_mb:.0f} MB')",
        "o": [
            "976 MB",
            "1024 MB",
            "1000 MB",
            "Error"
        ]
    },
    {
        "q": "Which technique reduces memory pressure in streaming?",
        "type": "mcq",
        "o": [
            "Windowing",
            "Full history",
            "Unlimited buffer",
            "No eviction"
        ]
    },
    {
        "q": "The _____ strategy limits memory by discarding old events.",
        "type": "fill_blank",
        "answers": [
            "eviction"
        ],
        "other_options": [
            "accumulation",
            "retention",
            "storage"
        ]
    },
    {
        "q": "Match the memory management technique with its use:",
        "type": "match",
        "left": [
            "Spilling",
            "Eviction",
            "Compression",
            "Pooling"
        ],
        "right": [
            "Disk overflow",
            "Remove old",
            "Reduce size",
            "Reuse objects"
        ]
    },
    {
        "q": "What is the output of this spill threshold check?",
        "type": "mcq",
        "c": "memory_used_mb = 7500\nmemory_limit_mb = 8000\nspill_threshold = 0.90\nshould_spill = memory_used_mb >= (memory_limit_mb * spill_threshold)\nprint(f'Spill to disk: {should_spill}')",
        "o": [
            "Spill to disk: True",
            "Spill to disk: False",
            "7200",
            "Error"
        ]
    },
    {
        "q": "Object pooling reduces garbage collection overhead.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the memory optimization steps:",
        "type": "rearrange",
        "words": [
            "Profile usage",
            "Identify leaks",
            "Tune GC",
            "Implement pooling",
            "Monitor production"
        ]
    },
    {
        "q": "What is the output of this GC overhead calculation?",
        "type": "mcq",
        "c": "total_time_ms = 10000\ngc_time_ms = 500\ngc_overhead = (gc_time_ms / total_time_ms) * 100\nprint(f'{gc_overhead}% GC overhead')",
        "o": [
            "5.0% GC overhead",
            "50% GC overhead",
            "500% GC overhead",
            "Error"
        ]
    },
    {
        "q": "Which serialization format is most memory efficient?",
        "type": "mcq",
        "o": [
            "Protocol Buffers",
            "XML",
            "Pretty JSON",
            "YAML"
        ]
    },
    {
        "q": "The _____ format minimizes serialization overhead.",
        "type": "fill_blank",
        "answers": [
            "binary"
        ],
        "other_options": [
            "text",
            "XML",
            "verbose"
        ]
    },
    {
        "q": "Match the serialization format with its characteristic:",
        "type": "match",
        "left": [
            "Protobuf",
            "JSON",
            "Avro",
            "MessagePack"
        ],
        "right": [
            "Schema required",
            "Human readable",
            "Schema embedded",
            "Binary JSON"
        ]
    },
    {
        "q": "What is the output of this serialization comparison?",
        "type": "mcq",
        "c": "json_bytes = 1000\nprotobuf_bytes = 400\nsavings = ((json_bytes - protobuf_bytes) / json_bytes) * 100\nprint(f'{savings}% smaller')",
        "o": [
            "60.0% smaller",
            "40% smaller",
            "100% smaller",
            "Error"
        ]
    },
    {
        "q": "Schema-less formats have higher parsing overhead.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the network optimization priorities:",
        "type": "rearrange",
        "words": [
            "Reduce payload",
            "Use compression",
            "Batch requests",
            "Cache responses",
            "Use CDN"
        ]
    },
    {
        "q": "What is the output of this bandwidth calculation?",
        "type": "mcq",
        "c": "records_per_second = 10000\nbytes_per_record = 500\nbandwidth_mbps = (records_per_second * bytes_per_record * 8) / (1024 * 1024)\nprint(f'{bandwidth_mbps:.1f} Mbps')",
        "o": [
            "38.1 Mbps",
            "5000 Mbps",
            "40 Mbps",
            "Error"
        ]
    },
    {
        "q": "Which technique reduces network latency in distributed ETL?",
        "type": "mcq",
        "o": [
            "Data locality",
            "Remote processing",
            "Cross-region shuffles",
            "Maximum hops"
        ]
    },
    {
        "q": "The _____ principle moves computation to data.",
        "type": "fill_blank",
        "answers": [
            "data locality"
        ],
        "other_options": [
            "data movement",
            "remote execution",
            "centralization"
        ]
    },
    {
        "q": "Match the network optimization with its benefit:",
        "type": "match",
        "left": [
            "Compression",
            "Batching",
            "Locality",
            "Connection pooling"
        ],
        "right": [
            "Reduce size",
            "Fewer calls",
            "Reduce transfer",
            "Reuse connections"
        ]
    },
    {
        "q": "What is the output of this latency budget?",
        "type": "mcq",
        "c": "stages = {'extract': 100, 'transform': 200, 'load': 50, 'network': 150}\ntotal_ms = sum(stages.values())\nbudget_ms = 600\nwithin_budget = total_ms <= budget_ms\nprint(f'Within budget: {within_budget}')",
        "o": [
            "Within budget: True",
            "Within budget: False",
            "500",
            "Error"
        ]
    },
    {
        "q": "Connection pooling reduces connection establishment overhead.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the end-to-end latency components:",
        "type": "rearrange",
        "words": [
            "Request parsing",
            "Processing",
            "Serialization",
            "Network transfer",
            "Response handling"
        ]
    },
    {
        "q": "What is the output of this connection pool size calculation?",
        "type": "mcq",
        "c": "concurrent_tasks = 50\nconnections_per_task = 2\npool_size = concurrent_tasks * connections_per_task\nprint(f'Pool size: {pool_size}')",
        "o": [
            "Pool size: 100",
            "Pool size: 50",
            "Pool size: 52",
            "Error"
        ]
    },
    {
        "q": "Which pattern manages database connection reuse?",
        "type": "mcq",
        "o": [
            "Connection pooling",
            "Connection creation",
            "Connection deletion",
            "Connection blocking"
        ]
    },
    {
        "q": "Connection pools improve performance by reusing connections.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "The _____ controls maximum pool connections.",
        "type": "fill_blank",
        "answers": [
            "max_size"
        ],
        "other_options": [
            "min_size",
            "avg_size",
            "total_size"
        ]
    },
    {
        "q": "Match the connection pool parameter with its purpose:",
        "type": "match",
        "left": [
            "Max size",
            "Min size",
            "Timeout",
            "Idle timeout"
        ],
        "right": [
            "Limit connections",
            "Pre-warm pool",
            "Wait limit",
            "Close unused"
        ]
    },
    {
        "q": "What is the output of this timeout calculation?",
        "type": "mcq",
        "c": "wait_time_ms = 3500\ntimeout_ms = 5000\nis_timeout = wait_time_ms >= timeout_ms\nprint(f'Timeout: {is_timeout}')",
        "o": [
            "Timeout: False",
            "Timeout: True",
            "3500",
            "Error"
        ]
    },
    {
        "q": "Idle connections should be closed to save resources.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the connection lifecycle:",
        "type": "rearrange",
        "words": [
            "Request connection",
            "Acquire from pool",
            "Execute query",
            "Return to pool",
            "Close if idle"
        ]
    }
]
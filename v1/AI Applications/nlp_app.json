[
    {
        "q": "NLP stands for Natural Language _____.",
        "type": "fill_blank",
        "answers": [
            "Processing"
        ],
        "other_options": [
            "Programming",
            "Parsing",
            "Protocol"
        ]
    },
    {
        "q": "Tokenization breaks text into smaller units called tokens.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "A token can be a word, character, or subword.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "text = 'Hello World'\nprint(text.split())",
        "o": [
            "['Hello', 'World']",
            "['Hello World']",
            "('Hello', 'World')",
            "Error"
        ]
    },
    {
        "q": "Stop words are common words often removed during preprocessing.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which of these is likely a stop word?",
        "type": "mcq",
        "o": [
            "the",
            "algorithm",
            "computer",
            "learning"
        ]
    },
    {
        "q": "Stemming reduces words to their root form.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Lemmatization reduces words to their base form (lemma) using a dictionary.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Stemming is usually faster but less accurate than Lemmatization.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the preprocessing step:",
        "type": "match",
        "left": [
            "Lowercasing",
            "Stop word removal",
            "Stemming",
            "Tokenization"
        ],
        "right": [
            "A -> a",
            "Remove 'the'",
            "Running -> run",
            "Split text"
        ]
    },
    {
        "q": "Bag of Words (BoW) represents text as a frequency count of words.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "BoW preserves the order of words.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "TF-IDF stands for Term Frequency-Inverse Document _____.",
        "type": "fill_blank",
        "answers": [
            "Frequency"
        ],
        "other_options": [
            "Format",
            "File",
            "Function"
        ]
    },
    {
        "q": "TF-IDF increases the weight of rare words across documents.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "text = 'AI is fun'\nprint(len(text))",
        "o": [
            "9",
            "3",
            "8",
            "10"
        ]
    },
    {
        "q": "N-grams are contiguous sequences of N items from a given text.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "A unigram consists of _____ word(s).",
        "type": "fill_blank",
        "answers": [
            "1"
        ],
        "other_options": [
            "2",
            "3",
            "0"
        ]
    },
    {
        "q": "A bigram consists of 2 words.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "One-Hot Encoding creates a vector of zeros with a single one.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "One-Hot Encoding vectors are sparse and high-dimensional for large vocabularies.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Sentiment Analysis determines the emotional tone of text.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Sentiment Analysis is typically a classification task.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Common sentiment classes are Positive, Negative, and Neutral.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange sentiment pipeline:",
        "type": "rearrange",
        "words": [
            "Input Text",
            "Tokenization",
            "Feature Extraction",
            "Classification"
        ]
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "x = 'Good'\nprint(x.lower())",
        "o": [
            "good",
            "Good",
            "GOOD",
            "Error"
        ]
    },
    {
        "q": "Naive Bayes is a common algorithm for text classification.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Naive Bayes assumes independence between features (words).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Spam detection is a binary classification problem.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Regular Expressions (Regex) are useful for pattern matching in text.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the regex symbol:",
        "type": "match",
        "left": [
            ".",
            "*",
            "+",
            "?"
        ],
        "right": [
            "Any char",
            "0 or more",
            "1 or more",
            "0 or 1"
        ]
    },
    {
        "q": "RNN stands for Recurrent Neural _____.",
        "type": "fill_blank",
        "answers": [
            "Network"
        ],
        "other_options": [
            "Node",
            "Number",
            "Neighbor"
        ]
    },
    {
        "q": "RNNs are designed to handle sequential data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "RNNs maintain a hidden state that captures information from previous steps.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Vanishing gradient is a common problem in standard RNNs.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "LSTM stands for Long Short-Term _____.",
        "type": "fill_blank",
        "answers": [
            "Memory"
        ],
        "other_options": [
            "Model",
            "Machine",
            "Matrix"
        ]
    },
    {
        "q": "LSTM prevents vanishing gradients using gates.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "GRU (Gated Recurrent Unit) is a simpler variation of LSTM.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "l = ['a', 'b']\nprint(''.join(l))",
        "o": [
            "ab",
            "a b",
            "a,b",
            "Error"
        ]
    },
    {
        "q": "Word Embeddings map words to dense vectors.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Word2Vec is a popular word embedding technique.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In Word2Vec, similar words have vectors close to each other.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "CBOW and Skip-gram are two architectures of Word2Vec.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "GloVe (Global Vectors) is another word embedding method.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "NLTK is a popular Python library for NLP.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Spacy is an industrial-strength NLP library.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "TextBlob is a simple library for sentiment analysis and processing.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the library:",
        "type": "match",
        "left": [
            "NLTK",
            "Spacy",
            "Scikit-learn",
            "Gensim"
        ],
        "right": [
            "Education/Research",
            "Speed/Production",
            "ML Algorithms",
            "Topic Modeling"
        ]
    },
    {
        "q": "Language Modeling predicts the next word in a sequence.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Smoothing is used to handle zero probabilities in language models.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Laplace smoothing adds '1' to all counts.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Perplexity is a measure of how well a probability model predicts a sample.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Lower perplexity indicates a better model.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('123'.isnumeric())",
        "o": [
            "True",
            "False",
            "Error",
            "None"
        ]
    },
    {
        "q": "Part-of-Speech (POS) tagging assigns grammatical categories to words.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Common POS tags include Noun, Verb, Adjective.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Named Entity Recognition (NER) identifies entities like names, dates, locations.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Sequence labeling assigns a label to each token in a sequence.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "IOB tags (Inside, Outside, Beginning) are used in NER.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "HMM stands for Hidden Markov _____.",
        "type": "fill_blank",
        "answers": [
            "Model"
        ],
        "other_options": [
            "Machine",
            "Matrix",
            "Method"
        ]
    },
    {
        "q": "HMMs can be used for POS tagging.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Viterbi algorithm is used to find the most likely sequence of states in HMM.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Conditional Random Fields (CRF) are discriminative models often used for sequence labeling.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange NER process:",
        "type": "rearrange",
        "words": [
            "Sentence",
            "Tokenize",
            "Predict Tags",
            "Extract Entities"
        ]
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "text = ' Hello '\nprint(text.strip())",
        "o": [
            "Hello",
            " Hello ",
            "Hello ",
            " Hello"
        ]
    },
    {
        "q": "Machine Translation translates text from one language to another.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Statistical Machine Translation (SMT) uses probability models.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Neural Machine Translation (NMT) uses neural networks.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Seq2Seq models are commonly used for NMT.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Encoder-Decoder architecture is typical in Seq2Seq.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the task:",
        "type": "match",
        "left": [
            "Classification",
            "Translation",
            "Summarization",
            "QA"
        ],
        "right": [
            "Assign label",
            "Change language",
            "Shorten text",
            "Answer question"
        ]
    },
    {
        "q": "Text Summarization creates a concise summary of a longer text.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Extractive summarization selects sentences from the original text.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Abstractive summarization generates new sentences not in the original text.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Question Answering (QA) systems answer questions posed in natural language.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Corpus is a large collection of text.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Syntax refers to the grammatical structure of sentences.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Semantics refers to the meaning of text.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Pragmatics refers to meaning in context.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "s = 'apple,banana'\nprint(s.split(','))",
        "o": [
            "['apple', 'banana']",
            "['apple,banana']",
            "('apple', 'banana')",
            "Error"
        ]
    },
    {
        "q": "Dependency Parsing analyzes the grammatical structure relationships.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Constituency Parsing breaks a text into sub-phrases.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "A parse tree represents the syntactic structure.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Word Sense Disambiguation (WSD) determines which meaning of a word is used.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Polysemy means a word has multiple meanings.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Synonymy means different words have the same meaning.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Antonymy means words have opposite meanings.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Hyponymy is a 'type-of' relationship (e.g., Apple is a Fruit).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange hierarchy:",
        "type": "rearrange",
        "words": [
            "Character",
            "Word",
            "Sentence",
            "Paragraph",
            "Document"
        ]
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print(bool(''))",
        "o": [
            "False",
            "True",
            "None",
            "Error"
        ]
    },
    {
        "q": "Cosine similarity is often used to measure similarity between word vectors.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Euclidean distance can also be used but is sensitive to magnitude.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Jaccard similarity measures overlap between sets of tokens.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Levenshtein distance measures edit distance between two strings.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "BLEU score is a metric commonly used for Machine Translation.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "ELMo (Embeddings from Language Models) uses deep contextualized word representations.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "ELMo is based on _____ Neural Networks.",
        "type": "fill_blank",
        "answers": [
            "Recurrent"
        ],
        "other_options": [
            "Convolutional",
            "Graph",
            "Spiking"
        ]
    },
    {
        "q": "ELMo uses bidirectional LSTMs.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "ULMFiT (Universal Language Model Fine-tuning) introduced transfer learning to NLP.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "ULMFiT typically uses an AWD-LSTM architecture.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('Cat'.isalpha())",
        "o": [
            "True",
            "False",
            "Error",
            "None"
        ]
    },
    {
        "q": "Transformers process input sequences in parallel, unlike RNNs.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "The Transformer architecture relies entirely on _____ mechanisms.",
        "type": "fill_blank",
        "answers": [
            "Attention"
        ],
        "other_options": [
            "Convolution",
            "Recurrence",
            "Pooling"
        ]
    },
    {
        "q": "Self-Attention relates different positions of a single sequence to compute a representation.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Multi-Head Attention allows the model to jointly attend to information from different subspaces.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the Transformer component:",
        "type": "match",
        "left": [
            "Query",
            "Key",
            "Value",
            "Score"
        ],
        "right": [
            "Searching vector",
            "Indexing vector",
            "Content vector",
            "Match relevance"
        ]
    },
    {
        "q": "Positional Encoding is injected to give the Transformer information about the order of tokens.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "BERT stands for Bidirectional Encoder Representations from _____.",
        "type": "fill_blank",
        "answers": [
            "Transformers"
        ],
        "other_options": [
            "Tokens",
            "Text",
            "Tensors"
        ]
    },
    {
        "q": "BERT is an encoder-only architecture.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "BERT is pre-trained on Masked Language Modeling (MLM).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "BERT is also pre-trained on Next Sentence Prediction (NSP).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange BERT training steps:",
        "type": "rearrange",
        "words": [
            "Input Text",
            "Mask Tokens",
            "Transformer Encoder",
            "Predict Masked"
        ]
    },
    {
        "q": "GPT stands for Generative Pre-trained _____.",
        "type": "fill_blank",
        "answers": [
            "Transformer"
        ],
        "other_options": [
            "Text",
            "Token",
            "Tool"
        ]
    },
    {
        "q": "GPT is a decoder-only architecture.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "GPT is trained on Causal Language Modeling (predicting the next token).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "GPT is autoregressive, meaning it generates one token at a time.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the model type:",
        "type": "match",
        "left": [
            "BERT",
            "GPT",
            "T5",
            "Bart"
        ],
        "right": [
            "Encoder-only",
            "Decoder-only",
            "Encoder-Decoder",
            "Denoising Autoencoder"
        ]
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print(' '.isspace())",
        "o": [
            "True",
            "False",
            "Error",
            "None"
        ]
    },
    {
        "q": "Transfer Learning involves taking a pre-trained model and fine-tuning it on a downstream task.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Fine-tuning updates the weights of the pre-trained model.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Character-level models process text one character at a time.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Word-level models process text one word at a time.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Subword tokenization (e.g., BPE, WordPiece) is a middle ground between word and character levels.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Byte Pair Encoding (BPE) iteratively merges the most frequent pair of bytes/characters.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('abc'.upper())",
        "o": [
            "ABC",
            "abc",
            "Abc",
            "Error"
        ]
    },
    {
        "q": "Topic Modeling discovers abstract topics in a collection of documents.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "LDA stands for Latent Dirichlet _____.",
        "type": "fill_blank",
        "answers": [
            "Allocation"
        ],
        "other_options": [
            "Analysis",
            "Algorithm",
            "Application"
        ]
    },
    {
        "q": "LDA is a probabilistic model.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In LDA, each document is a mixture of topics.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In LDA, each topic is a mixture of words.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "LSA (Latent Semantic Analysis) uses SVD (Singular Value Decomposition).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "NMF (Non-negative Matrix Factorization) is another technique for topic modeling.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange LDA concepts:",
        "type": "rearrange",
        "words": [
            "Documents",
            "Topics",
            "Words",
            "Distribution"
        ]
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "l = [1, 2, 3]\nprint(len(l))",
        "o": [
            "3",
            "2",
            "1",
            "0"
        ]
    },
    {
        "q": "Text Classification assigns predefined categories to text.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Multi-class classification means there are more than 2 classes.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Multi-label classification means one instance can belong to multiple classes.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Confusion Matrix is used to evaluate classification performance.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Precision is TP / (TP + FP).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Recall is TP / (TP + FN).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "F1 score is the harmonic mean of Precision and Recall.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the metric:",
        "type": "match",
        "left": [
            "Accuracy",
            "Precision",
            "Recall",
            "F1"
        ],
        "right": [
            "Overall correctness",
            "Exactness",
            "Completeness",
            "Balance"
        ]
    },
    {
        "q": "Information Retrieval (IR) finds relevant documents for a query.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Inverted Index maps words to their locations in documents.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Boolean Retrieval uses operators like AND, OR, NOT.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Ranking puts the most relevant documents at the top.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "BM25 is a probabilistic retrieval function based on TF-IDF.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('a' * 3)",
        "o": [
            "aaa",
            "a",
            "3a",
            "Error"
        ]
    },
    {
        "q": "Parsing analyzes the grammatical structure of a sentence.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Top-down parsing starts from the root symbol (Sentence).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Bottom-up parsing starts from the words and builds up.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "CYK algorithm is a bottom-up parsing algorithm.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Shift-Reduce parsing is a common bottom-up method.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Embeddings capture semantic meaning.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Analogies like 'King - Man + Woman = Queen' work with good embeddings.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "FastText extends Word2Vec by using subword information.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "FastText works better for rare words and out-of-vocabulary words.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('123'.isdigit())",
        "o": [
            "True",
            "False",
            "Error",
            "None"
        ]
    },
    {
        "q": "Dialogue Systems interact with humans using natural language.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Task-oriented bots help users achieve specific goals (e.g., booking).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Chit-chat bots are designed for open-ended conversation.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Intent classification identifies what the user wants to do.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Slot filling extracts specific parameters (e.g., time, location).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange dialogue flow:",
        "type": "rearrange",
        "words": [
            "User Input",
            "NLU",
            "Dialogue Manager",
            "NLG",
            "Response"
        ]
    },
    {
        "q": "NLU stands for Natural Language _____.",
        "type": "fill_blank",
        "answers": [
            "Understanding"
        ],
        "other_options": [
            "Utilizing",
            "Underlining",
            "Upscaling"
        ]
    },
    {
        "q": "NLG stands for Natural Language Generation.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rule-based chatbots rely on predefined patterns.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Retrieval-based chatbots select a response from a database.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Generative chatbots create new responses from scratch.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('a' > 'b')",
        "o": [
            "False",
            "True",
            "Error",
            "None"
        ]
    },
    {
        "q": "Sentiment Lexicons are lists of words with sentiment scores.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "VADER is a rule-based sentiment analysis tool.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Aspect-based Sentiment Analysis associates sentiment with specific aspects of an entity.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Coreference Resolution determines which words refer to the same entity.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In 'John said he is busy', 'he' refers to 'John'. This is coreference.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Anaphora resolution resolves references to preceding antecedents.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Cataphora resolution resolves references to following postcedents.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the term:",
        "type": "match",
        "left": [
            "Anaphora",
            "Cataphora",
            "Coreference",
            "Ellipsis"
        ],
        "right": [
            "Back reference",
            "Forward reference",
            "Same entity",
            "Omitted words"
        ]
    },
    {
        "q": "Text Normalization converts text to a standard format.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Spell checking is a form of text normalization.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Noise removal involves cleaning up text (e.g., removing HTML tags).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Text augmentation increases the size of training data artificially.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Back-translation is a common text augmentation technique.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('a' in ['a', 'b'])",
        "o": [
            "True",
            "False",
            "Error",
            "None"
        ]
    },
    {
        "q": "Word2Vec treats each word as an atomic unit, ignoring internal structure.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "FastText improves over Word2Vec by learning embeddings for char n-grams.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "ELMo provides deep contextualized word encodings.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Static embeddings (like GloVe) give the same vector for 'bank' in all contexts.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Contextual embeddings (like BERT) give different vectors for 'bank' depending on context.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "The core innovation of Transformers is the Self-_____ mechanism.",
        "type": "fill_blank",
        "answers": [
            "Attention"
        ],
        "other_options": [
            "Convolution",
            "Activation",
            "Normalization"
        ]
    },
    {
        "q": "Scaled Dot-Product Attention divides the dot product by the square root of the dimension.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Attention mechanism allows the model to focus on relevant parts of the input.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Multi-Head Attention runs multiple attention mechanisms in parallel.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Layer Normalization is commonly used in Transformers.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange Transformer block:",
        "type": "rearrange",
        "words": [
            "Self Attention",
            "Add & Norm",
            "Feed Forward",
            "Add & Norm"
        ]
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('1' + '2')",
        "o": [
            "12",
            "3",
            "12",
            "Error"
        ]
    },
    {
        "q": "Seq2Seq models map an input sequence to an output sequence.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In NMT, the Encoder summarizes the input sentence into a context vector.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "The Decoder generates the translation word by word.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Teacher Forcing feeds the ground truth token to the decoder during training.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Beam Search improves decoding by exploring multiple paths.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Beam width determines how many candidate sequences are kept.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Greedy decoding picks the token with the highest probability at each step.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the decoding strategy:",
        "type": "match",
        "left": [
            "Greedy",
            "Beam Search",
            "Random Sampling",
            "Top-k Sampling"
        ],
        "right": [
            "Best token only",
            "Keep N best",
            "Weighted random",
            "Top k pool"
        ]
    },
    {
        "q": "BLEU score measures n-gram overlap with reference translations.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "ROUGE score is commonly used for text summarization.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "ROUGE-L measures the Longest Common _____.",
        "type": "fill_blank",
        "answers": [
            "Subsequence"
        ],
        "other_options": [
            "Sentence",
            "String",
            "Set"
        ]
    },
    {
        "q": "METEOR metric considers synonyms and stemming.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Perplexity is exp(Cross Entropy Loss).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "l = [1, 2, 3]\nprint(l[-1])",
        "o": [
            "3",
            "2",
            "1",
            "Error"
        ]
    },
    {
        "q": "Sentence Embeddings represent entire sentences as vectors.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Doc2Vec extends Word2Vec to documents.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Universal Sentence Encoder (USE) is a popular model for sentence embeddings.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "SBERT (Sentence-BERT) uses siamese networks to derive sentence embeddings.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "SBERT is fine-tuned on NLI (Natural Language Inference) data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Semantic Search uses sentence embeddings to find meaning-matches.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Question Answering can be Extractive or Generative.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "SQuAD (Stanford Question Answering Dataset) is a famous QA dataset.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In Extractive QA, the model predicts the start and end indices of the answer.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange QA pipeline:",
        "type": "rearrange",
        "words": [
            "Question",
            "Retriever",
            "Reader",
            "Answer"
        ]
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print(list(range(3)))",
        "o": [
            "[0, 1, 2]",
            "[1, 2, 3]",
            "[0, 1, 2, 3]",
            "Error"
        ]
    },
    {
        "q": "Zero-shot learning means the model performs a task without seeing examples.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Few-shot learning provides a few examples in the prompt.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Prompt Engineering involves designing inputs to guide the model.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Chain of Thought prompting encourages the model to explain its reasoning.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In-context learning allows LLMs to learn from the prompt without weight updates.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Hallucination is when an LLM generates plausible but _____ information.",
        "type": "fill_blank",
        "answers": [
            "false"
        ],
        "other_options": [
            "true",
            "short",
            "long"
        ]
    },
    {
        "q": "Temperature determines the randomness of the model's output.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Higher temperature leads to more diversity and creativity.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Top-p (Nucleus) sampling selects from the smallest set of tokens whose cumulative probability exceeds p.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the sampling param:",
        "type": "match",
        "left": [
            "Temperature",
            "Top-k",
            "Top-p",
            "Greedy"
        ],
        "right": [
            "Randomness scale",
            "Fixed # candidates",
            "Probability mass",
            "Max prob only"
        ]
    },
    {
        "q": "Cross-Lingual models can handle multiple languages.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "mBERT (Multilingual BERT) is trained on Wikipedia in 104 languages.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "XLM-R (XLM-RoBERTa) is a powerful cross-lingual model.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Zero-shot cross-lingual transfer means training on English and testing on French.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Code-Switching involves mixing languages in the same sentence.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "d = {'a': 1}\nprint('b' in d)",
        "o": [
            "False",
            "True",
            "Error",
            "None"
        ]
    },
    {
        "q": "Knowledge Graphs store structured information as entities and relationships.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Triples in KGs are in the form (Subject, Predicate, _____).",
        "type": "fill_blank",
        "answers": [
            "Object"
        ],
        "other_options": [
            "Entity",
            "Relation",
            "Property"
        ]
    },
    {
        "q": "Knowledge Graph Embeddings map entities and relations to vector space.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "TransE is a model for learning KG embeddings.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Coreference resolution often helps in Knowledge Graph Construction.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Text-to-Speech (TTS) converts text into spoken voice.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Speech-to-Text (ASR) converts spoken voice into text.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "WaveNet is a generative model for raw audio.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Mel-spectrogram is a common audio feature representation.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "CTC (Connectionist Temporal Classification) loss aligns audio frames to text.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange ASR pipeline:",
        "type": "rearrange",
        "words": [
            "Audio",
            "Feature Extraction",
            "Acoustic Model",
            "Language Model"
        ]
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('Hello'.replace('l', ''))",
        "o": [
            "Heo",
            "Helo",
            "Hello",
            "Error"
        ]
    },
    {
        "q": "Data Augmentation in NLP is harder than in Computer Vision.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Synonym Replacement is a simple NLP augmentation technique.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Random Insertion/Deletion can change the meaning of a sentence.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Back-translation preserves semantics better than random noise.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Mixup helps regularize models by interpolating inputs.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Active Learning is used to reduce labeling costs.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Uncertainty Sampling queries instances the model is least sure about.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Diversity Sampling queries instances different from labeled ones.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Curriculum Learning introduces examples from easy to hard.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Distillation compresses a large 'Teacher' model into a small '_____' model.",
        "type": "fill_blank",
        "answers": [
            "Student"
        ],
        "other_options": [
            "Learner",
            "Child",
            "Master"
        ]
    },
    {
        "q": "DistilBERT is a distilled version of BERT.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "TinyBERT uses distillation at multiple layers.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Quantization reduces the precision of weights (e.g., float32 to int8).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Pruning removes less important connections/weights.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the compression tech:",
        "type": "match",
        "left": [
            "Distillation",
            "Quantization",
            "Pruning",
            "Factorization"
        ],
        "right": [
            "Teacher-Student",
            "Lower precision",
            "Remove weights",
            "Matrix decomp"
        ]
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print(list(zip([1, 2], [3, 4])))",
        "o": [
            "[(1, 3), (2, 4)]",
            "[(1, 2), (3, 4)]",
            "[(1, 4), (2, 3)]",
            "Error"
        ]
    },
    {
        "q": "Bias in NLP can lead to stereotypical or harmful outputs.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Word embeddings can capture gender bias (e.g., Doctor-Man, Nurse-Woman).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Debiasing techniques aim to remove these biases.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Toxicity detection identifies hateful or offensive language.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Explainability (XAI) in NLP helps understand model decisions.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Attention visualization shows which words the model focused on.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "LIME and SHAP are general XAI methods applicable to NLP.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Adversarial Examples are slightly modified inputs that fool the model.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "TextAttack is a library for adversarial attacks in NLP.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('a' in 'apple')",
        "o": [
            "True",
            "False",
            "Error",
            "None"
        ]
    },
    {
        "q": "RoBERTa stands for Robustly Optimized BERT Pretraining Approach.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "RoBERTa removes the Next Sentence Prediction (NSP) task used in BERT.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "RoBERTa uses dynamic masking instead of static masking.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "ALBERT (A Lite BERT) reduces parameters by sharing weights across layers.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "ALBERT factorizes the embedding parameterization matrix.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "XLNet uses Permutation Language Modeling to capture bidirectional context.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "T5 (Text-to-Text Transfer Transformer) frames all NLP tasks as text-to-text problems.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "T5 uses an Encoder-Decoder architecture.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange T5 task format:",
        "type": "rearrange",
        "words": [
            "Task Prefix",
            "Input Text",
            "Model",
            "Target Text"
        ]
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "s = 'hello'\nprint(s.count('l'))",
        "o": [
            "2",
            "1",
            "3",
            "Error"
        ]
    },
    {
        "q": "SpanBERT is optimized for span-based tasks like Question Answering.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "ELECTRA uses a Generator and a Discriminator (Replaced Token Detection).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "ELECTRA is more sample-efficient than BERT.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "DeBERTa introduces Disentangled Attention mechanism.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Longformer uses sparse attention to handle long sequences.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "BigBird is another sparse attention model for long documents.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the efficiency model:",
        "type": "match",
        "left": [
            "ALBERT",
            "DistilBERT",
            "Longformer",
            "ELECTRA"
        ],
        "right": [
            "Parameter sharing",
            "Distillation",
            "Sparse attention",
            "Token detection"
        ]
    },
    {
        "q": "RLHF stands for Reinforcement Learning from Human _____.",
        "type": "fill_blank",
        "answers": [
            "Feedback"
        ],
        "other_options": [
            "Features",
            "Factors",
            "Functions"
        ]
    },
    {
        "q": "RLHF aligns LLMs with human values and preferences.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "The Reward Model in RLHF predicts a scalar score for a given text.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "PPO (Proximal Policy Optimization) is a common RL algorithm used in RLHF.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "InstructGPT is a GPT-3 model fine-tuned using RLHF.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('123'.zfill(5))",
        "o": [
            "00123",
            "12300",
            "0012300",
            "Error"
        ]
    },
    {
        "q": "PEFT stands for Parameter-Efficient Fine-Tuning.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Adapter modules insert small learnable layers into the foundation model.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "LoRA (Low-Rank Adaptation) freezes pre-trained weights and adds trainable rank decomposition matrices.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "LoRA significantly reduces the number of trainable parameters.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Prefix Tuning prepends learnable vectors to the input keys and values.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Prompt Tuning is a simplified version of Prefix Tuning.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Format for LoRA rank decomposition: W = W0 + _____.",
        "type": "fill_blank",
        "answers": [
            "BA"
        ],
        "other_options": [
            "AB",
            "B",
            "A"
        ]
    },
    {
        "q": "Soft Prompts are learnable vectors optimized during training.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Hard Prompts are fixed discrete text tokens.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "l = ['a', 'b', 'c']\nprint(''.join(l))",
        "o": [
            "abc",
            "a b c",
            "a,b,c",
            "Error"
        ]
    },
    {
        "q": "Constituency Parsing typically generates a tree structure.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Dependency Parsing creates a graph of word relationships.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Transition-based dependency parsers use a stack and a buffer.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Graph-based dependency parsers search for the Maximum Spanning Tree.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Arc-Standard and Arc-Eager are transition systems.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange parsing actions:",
        "type": "rearrange",
        "words": [
            "Shift",
            "Left-Arc",
            "Right-Arc",
            "Reduce"
        ]
    },
    {
        "q": "Visual Question Answering (VQA) requires understanding both image and text.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Image Captioning generates a textual description of an image.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Show and Tell is a classic encoder-decoder model for image captioning.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "VisualBERT incorporates visual features into BERT.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Lava and Flamingo are examples of Multimodal LLMs.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('HELLO'.isupper())",
        "o": [
            "True",
            "False",
            "Error",
            "None"
        ]
    },
    {
        "q": "Semantic Role Labeling (SRL) identifies 'who did what to whom'.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "SRL labels arguments like Agent, Patient, Instrument.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Abstract Meaning Representation (AMR) is a graph-based semantic representation.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Universal Dependencies (UD) aims for consistent annotation across languages.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Discourse Analysis studies the structure of text beyond the sentence level.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Coherence refers to the logical flow of ideas.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Cohesion refers to the linguistic ties between sentences.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the semantic role:",
        "type": "match",
        "left": [
            "Agent",
            "Patient",
            "Goal",
            "Instrument"
        ],
        "right": [
            "Doer of action",
            "Receiver of action",
            "Destination",
            "Tool used"
        ]
    },
    {
        "q": "Retrieval-Augmented Generation (RAG) combines an LLM with an external knowledge base.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "RAG reduces hallucinations by grounding the answer in retrieved context.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In RAG, the retriever finds relevant documents for the query.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "The generator produces the final answer using the query and context.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Dense Passage Retrieval (DPR) is often used in RAG.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "s = 'apple'\nprint(s.find('p'))",
        "o": [
            "1",
            "2",
            "0",
            "-1"
        ]
    },
    {
        "q": "Adversarial Training makes models robust to small perturbations.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Trigger words can cause a model to output a specific target.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Watermarking LLM text helps identify AI-generated content.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Red teaming involves trying to break or exploit the model.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Jailbreaking refers to bypassing an LLM's safety filters.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Prompt Injection is a security vulnerability in LLMs.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange RAG steps:",
        "type": "rearrange",
        "words": [
            "Query",
            "Retrieve Context",
            "Combine",
            "Generate"
        ]
    },
    {
        "q": "Active Learning cycle: Train -> Query -> Label -> _____.",
        "type": "fill_blank",
        "answers": [
            "Retrain"
        ],
        "other_options": [
            "Retire",
            "Reject",
            "Remove"
        ]
    },
    {
        "q": "Pool-based sampling selects samples from a large unlabeled pool.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Stream-based sampling decides on-the-fly whether to label a sample.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('1,2,3'.split(','))",
        "o": [
            "['1', '2', '3']",
            "['1,2,3']",
            "(1, 2, 3)",
            "Error"
        ]
    },
    {
        "q": "Contrastive Learning pulls similar samples together and pushes dissimilar ones apart.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "SimCSE is a contrastive learning method for sentence embeddings.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Hard negatives are samples that are different but look similar.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In-batch negatives use other samples in the minibatch as negatives.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the Transformer attention:",
        "type": "match",
        "left": [
            "Global",
            "Local",
            "Sparse",
            "Linear"
        ],
        "right": [
            "All tokens",
            "Window based",
            "Random/Select",
            "Low rank approx"
        ]
    },
    {
        "q": "Linear Attention reduces complexity from O(N^2) to O(N).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Flash Attention optimizes memory access for speedup.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Scaling Laws describe how model performance improves with size and data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Chinchilla Scaling Laws suggest current models are undertrained.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Chinchilla suggests scaling _____ proportional to model size.",
        "type": "fill_blank",
        "answers": [
            "tokens"
        ],
        "other_options": [
            "layers",
            "heads",
            "depth"
        ]
    },
    {
        "q": "Emergent abilities appear only in large models.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('Hello'.startswith('H'))",
        "o": [
            "True",
            "False",
            "Error",
            "None"
        ]
    },
    {
        "q": "Constituency parsing focuses on 'part-of' relationships.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "CKY Algorithm requires Grammar to be in Chomsky Normal Form (CNF).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Discriminative Ranking reranks best K parses.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Shift-Reduce is faster but greedy compared to CKY.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Recursive Neural Networks can parse tree structures.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Tree-LSTMs generalize LSTMs to tree-structured network topologies.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('  ab  '.strip())",
        "o": [
            "ab",
            "  ab  ",
            "ab  ",
            "  ab"
        ]
    },
    {
        "q": "ReAct (Reasoning and Acting) combines Chain-of-Thought with action execution.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "ReAct allows LLMs to interact with external tools (e.g., search engine).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Chain-of-Thought (CoT) prompting usually improves performance on math and logic tasks.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Zero-shot CoT uses the prompt 'Let's think step by _____'.",
        "type": "fill_blank",
        "answers": [
            "step"
        ],
        "other_options": [
            "example",
            "time",
            "logic"
        ]
    },
    {
        "q": "Self-Consistency in CoT involves sampling multiple reasoning paths and taking the majority vote.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('1' + '1')",
        "o": [
            "11",
            "2",
            "Error",
            "None"
        ]
    },
    {
        "q": "Reformer uses Locality Sensitive Hashing (LSH) for efficient attention.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Reformer also uses reversible residual layers to save memory.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Linformer projects the key and value matrices to lower dimensions.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Performer approximates the attention mechanism using kernels (Random Feature Maps).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the efficient Transformer:",
        "type": "match",
        "left": [
            "Reformer",
            "Linformer",
            "Performer",
            "Longformer"
        ],
        "right": [
            "LSH Attention",
            "Low-rank projection",
            "Kernel approximation",
            "Sliding window"
        ]
    },
    {
        "q": "Hallucination in LLMs can vary from subtle inaccuracies to complete fabrication.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Faithfulness metrics measure how well the summary allows inferring the source content.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Fact-checking systems often use a claim verification pipeline.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Self-Correction involves the model reviewing and fixing its own output.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange verification steps:",
        "type": "rearrange",
        "words": [
            "Detect Claim",
            "Retrieve Evidence",
            "Verify Claim",
            "Verdict"
        ]
    },
    {
        "q": "Nucleus Sampling (Top-p) truncates the tail of the probability distribution.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Typical Sampling aims to select tokens with information content close to the expected information.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Mirostat is a decoding algorithm that dynamically adjusts k and p to maintain perplexity.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Contrastive Search penalizes repetitive tokens during decoding.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('a'.center(3))",
        "o": [
            " a ",
            "a  ",
            "  a",
            "Error"
        ]
    },
    {
        "q": "Constituency trees can be converted to dependency trees.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Head rules define which child in a constituent is the head.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Projective dependency trees have no crossing edges.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Non-projective dependency trees are common in languages with free word order.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Graph-based parsers handle non-projectivity better than standard transition-based ones.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Cross-Encoder architectures process query and document together.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Bi-Encoder architectures process query and document independently.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Cross-Encoders are generally more accurate but slower than Bi-Encoders.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Bi-Encoders enable efficient vector search (ANN).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "ColBERT uses late interaction to combine efficiency of Bi-Encoders with accuracy of Cross-Encoders.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('123'.ljust(5, '0'))",
        "o": [
            "12300",
            "00123",
            "01230",
            "Error"
        ]
    },
    {
        "q": "Coreference Resolution involves finding all expressions that refer to the same entity.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Mention Detection is the first step in coreference resolution.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Winograd Schema Challenge tests coreference resolution requiring world knowledge.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Entity Linking connects entity mentions to a Knowledge Base (e.g., Wikipedia).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange Entity Linking:",
        "type": "rearrange",
        "words": [
            "Mention Detection",
            "Candidate Generation",
            "Disambiguation",
            "Linking"
        ]
    },
    {
        "q": "Subword regularization (e.g., BPE-Dropout) improves robustness.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "SentencePiece is a tokenizer that treats the input as a raw data stream.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "SentencePiece does not require pre-tokenization (splitting by space).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Unigram Language Model is a subword tokenization algorithm supported by SentencePiece.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the tokenizer:",
        "type": "match",
        "left": [
            "BPE",
            "WordPiece",
            "Unigram",
            "SentencePiece"
        ],
        "right": [
            "Merge frequent pairs",
            "Maximize likelihood",
            "Probabilistic drop",
            "Language agnostic"
        ]
    },
    {
        "q": "Zero-shot Text Classification can be done using NLI models.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "By treating the label as a hypothesis, NLI predicts entailment.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "SetFit (Sentence Transformer Fine-tuning) is an efficient framework for few-shot classification.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "SetFit uses contrastive learning followed by a classification head.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('abc'.rfind('b'))",
        "o": [
            "1",
            "0",
            "2",
            "-1"
        ]
    },
    {
        "q": "Text Style Transfer aims to change the style (e.g., formal to informal) while preserving content.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Disentangled representation learning separates style and content features.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Back-translation can be used for style transfer with style-specific constraints.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Continuous Prompting optimizes a sequence of continuous vectors for style control.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Data pruning identifies and removes harmful or redundant data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Coreset selection finds a small subset that approximates the full dataset's performance.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Influence Functions estimate the effect of a training point on predictions.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Dataset Cartography maps data based on confidence and variability.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the data map region:",
        "type": "match",
        "left": [
            "Easy-to-learn",
            "Ambiguous",
            "Hard-to-learn"
        ],
        "right": [
            "High conf, Low var",
            "Med conf, High var",
            "Low conf, Low var"
        ]
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('a,b'.partition(','))",
        "o": [
            "('a', ',', 'b')",
            "['a', 'b']",
            "('a', 'b')",
            "Error"
        ]
    },
    {
        "q": "Automated Essay Scoring (AES) evaluates written text quality.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Quadratic Weighted Kappa is a standard metric for AES.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Grammatical Error Correction (GEC) is a sequence-to-sequence task.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Fact Extraction and Verification (FEVER) is a benchmark for fact-checking.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Dialogue State Tracking (DST) estimates the user's goal at each turn.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Policy Learning determines the system's next action in a dialogue.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "TOD-BERT is a pre-trained model for Task-Oriented Dialogue.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Slot Filling is often modeled as a sequence tagging problem.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange TOD pipeline:",
        "type": "rearrange",
        "words": [
            "NLU",
            "State Tracking",
            "Policy",
            "NLG"
        ]
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('Test'.lower())",
        "o": [
            "test",
            "Test",
            "TEST",
            "Error"
        ]
    },
    {
        "q": "Vector Databases differ from traditional DBs by optimizing similarity search.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "HNSW graphs allow logarithmic time complexity search.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Product Quantization (PQ) splits vectors into sub-vectors for compression.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Locality Sensitive Hashing (LSH) hashes similar items to the same bucket.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "IVF (Inverted File) index restricts search to a subset of clusters.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Federated Learning allows training NLP models on decentralized data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Differential Privacy adds noise to gradients to protect privacy.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Secure Aggregation ensures the server sees only the update sum, not individual updates.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Private Aggregation of Teacher Ensembles (PATE) uses disjoint teacher models.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('1' * 2)",
        "o": [
            "11",
            "2",
            "1",
            "Error"
        ]
    },
    {
        "q": "Lexical Entailment refers to hyponymy relations.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Selectional Preference describes likely arguments for a predicate (e.g., 'eat' prefers food).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Distributional Hypothesis states words in similar contexts have similar meanings.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Latent Semantic Analysis (LSA) captures distributional semantics.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Mixture of Experts (MoE) models activate only a subset of parameters per token.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In MoE, a _____ network determines which experts process each token.",
        "type": "fill_blank",
        "answers": [
            "gating",
            "router"
        ],
        "other_options": [
            "neural",
            "dense",
            "switching"
        ]
    },
    {
        "q": "Load balancing loss in MoE prevents the gating network from always choosing the same experts.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Switch Transformer is an example of a sparse MoE model.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('123'.isnumeric())",
        "o": [
            "True",
            "False",
            "None",
            "Error"
        ]
    },
    {
        "q": "Constitutional AI trains models using a set of principles rather than human labels alone.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "RLAIF stands for Reinforcement Learning from AI Feedback.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Red Teaming involves probing the model to find vulnerabilities and safety issues.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Jailbreaking attempts to bypass an LLM's safety filters.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the safety concept:",
        "type": "match",
        "left": [
            "Red Teaming",
            "Jailbreaking",
            "Prompt Injection",
            "Watermarking"
        ],
        "right": [
            "Adversarial testing",
            "Bypassing controls",
            "Overriding instructions",
            "Provenance tracking"
        ]
    },
    {
        "q": "Detecting AI-generated text is becoming increasingly difficult as models improve.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Watermarking text involves embedding a secret pattern in the token distribution.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Code LLMs like Codex are often trained on data from GitHub.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Fill-in-the-middle (FIM) training allows models to infill code based on surrounding context.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "StarCoder is an open-access code LLM.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print(' '.join(['a', 'b']))",
        "o": [
            "a b",
            "ab",
            "a,b",
            "Error"
        ]
    },
    {
        "q": "Rotary Positional Embeddings (RoPE) rotate the query and key vectors.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "RoPE allows for better generalization to sequence lengths longer than seen during training.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "ALiBi (Attention with Linear Biases) adds a static bias to attention scores based on distance.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "PagedAttention is a memory management technique used in vLLM to optimize KV cache.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the long-context technique:",
        "type": "match",
        "left": [
            "RoPE",
            "ALiBi",
            "Sliding Window",
            "Sparse Attention"
        ],
        "right": [
            "Rotation",
            "Distance penalty",
            "Local context",
            "Selective context"
        ]
    },
    {
        "q": "Continuous Batching (or Cellular Batching) improves throughput by not waiting for all sequences to finish.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Quantization reduces model size by using lower-precision data types (e.g., INT8, INT4).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "GPTQ is a post-training quantization method.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "AWQ (Activation-aware Weight Quantization) protects salient weights during quantization.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('a\\nb'.splitlines())",
        "o": [
            "['a', 'b']",
            "['a\\nb']",
            "('a', 'b')",
            "Error"
        ]
    },
    {
        "q": "Flamingo is a visual language model that fuses vision and language through gated cross-attention.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "BLIP (Bootstrapping Language-Image Pre-training) uses a captioner and a filter to clean noisy web data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Visual Instruction Tuning (e.g., LLaVA) fine-tunes models on image-text instruction data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "GPT-4V is a multimodal model capable of processing image and text inputs.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Tree of Thoughts (ToT) generalizes CoT by exploring multiple reasoning paths as a tree search.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Graph of Thoughts (GoT) models reasoning as an arbitrary graph.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Program-aided Language Models (PAL) offload calculation steps to a Python interpreter.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange reasoning evolution:",
        "type": "rearrange",
        "words": [
            "Standard Prompting",
            "CoT",
            "Self-Consistency",
            "Tree of Thoughts"
        ]
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('abc'.startswith('ab'))",
        "o": [
            "True",
            "False",
            "None",
            "Error"
        ]
    },
    {
        "q": "Semantic Caching stores responses based on the semantic similarity of prompts.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Speculative Decoding uses a small draft model to speed up a large target model.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In Speculative Decoding, the large model verifies the draft tokens in parallel.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "FlashAttention optimizes attention computation by making it IO-aware.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "FlashAttention reduces memory access cost (HBM reads/writes).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Direct Preference Optimization (DPO) optimizes the policy directly without a separate reward model.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "DPO uses a simple classification loss on preference pairs.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "KTO (Kahneman-Tversky Optimization) learns from binary feedback (good/bad) instead of pairs.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the alignment method:",
        "type": "match",
        "left": [
            "RLHF",
            "DPO",
            "KTO",
            "Constitutional AI"
        ],
        "right": [
            "Uses Reward Model",
            "Implicit Reward",
            "Unpaired feedback",
            "AI feedback"
        ]
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('abc'.replace('a', 'z'))",
        "o": [
            "zbc",
            "azc",
            "abc",
            "Error"
        ]
    },
    {
        "q": "Multi-Query Attention (MQA) shares keys and values across all attention heads.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Grouped-Query Attention (GQA) is a compromise between MHA and MQA.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "MQA significantly reduces the size of the KV cache.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Retrieval-Augmented Generation (RAG) reduces hallucinations by grounding answers in retrieved docs.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Hybrid Search in RAG combines keyword search (BM25) and _____ search.",
        "type": "fill_blank",
        "answers": [
            "vector",
            "semantic"
        ],
        "other_options": [
            "binary",
            "linear",
            "tree"
        ]
    },
    {
        "q": "Reranking improves RAG retrieval by re-scoring top candidates with a more powerful model.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Recursive summarization is a technique for summarizing very long documents.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange RAG pipeline:",
        "type": "rearrange",
        "words": [
            "Query",
            "Retrieval",
            "Reranking",
            "Generation"
        ]
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print(' 1 '.strip())",
        "o": [
            "1",
            " 1 ",
            "1 ",
            "Error"
        ]
    },
    {
        "q": "Scaling Laws suggest language model performance scales as a power law with compute, data, and parameters.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Chinchilla scaling laws emphasize that tokens should scale linearly with parameters for optimal compute.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Emergent abilities are capabilities that appear only in larger models.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In-context learning refers to learning from prompts without weight updates.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Grokking is the phenomenon where generalization happens long after overfitting on training data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Synthetic data generation involves using LLMs to create training data for other models.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Model distillation can be used to transfer knowledge from a large model to a smaller one.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the model size concept:",
        "type": "match",
        "left": [
            "Distillation",
            "Pruning",
            "Quantization",
            "Scaling"
        ],
        "right": [
            "Teacher-Student",
            "Remove weights",
            "Lower precision",
            "Increase size"
        ]
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('123'.isdigit())",
        "o": [
            "True",
            "False",
            "None",
            "Error"
        ]
    },
    {
        "q": "Adversarial Examples are inputs designed to confuse a model.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Trigger words in NLP can cause a model to output specific, potentially harmful content.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Data poisoning involves injecting malicious data into the training set.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Model inversion attacks attempt to reconstruct training data from model outputs.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Membership inference attacks determine if a specific data point was in the training set.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Self-Supervised Learning predicts parts of the data from other parts.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Masked Language Modeling (MLM) is a form of self-supervised learning.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Causal Language Modeling (CLM) predicts the _____ token.",
        "type": "fill_blank",
        "answers": [
            "next",
            "following"
        ],
        "other_options": [
            "previous",
            "random",
            "masked"
        ]
    },
    {
        "q": "Auto-Regressive models generate tokens one by one.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('a,b'.split(','))",
        "o": [
            "['a', 'b']",
            "('a', 'b')",
            "['a,b']",
            "Error"
        ]
    },
    {
        "q": "Compound AI Systems involve chaining multiple models and tools.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "LangChain is a popular framework for building compound AI systems.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Agents are systems that use an LLM as a reasoning engine to determine actions.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Memory in agents allows them to persist state across interactions.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Curriculum Learning presents training examples in a meaningful order (easy to hard).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Prompt Tuning is a parameter-efficient fine-tuning method.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Prefix Tuning adds trainable vectors to the beginning of the input.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Adapter layers are small modules inserted between transformer layers for fine-tuning.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the PEFT method:",
        "type": "match",
        "left": [
            "LoRA",
            "Prefix Tuning",
            "Adapters",
            "Prompt Tuning"
        ],
        "right": [
            "Low-rank matrices",
            "Input vectors",
            "Bottleneck layers",
            "Soft prompts"
        ]
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print(len('abc'))",
        "o": [
            "3",
            "2",
            "4",
            "Error"
        ]
    },
    {
        "q": "Universal sentence embeddings aim to work across multiple tasks.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "LaBSE (Language-agnostic BERT Sentence Embedding) supports 100+ languages.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "LASER (Language-Agnostic SEntence Representations) uses a BiLSTM encoder.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Polyglot models are designed to handle multiple languages.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Code switching occurs when a speaker alternates between two or more languages.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Zero-resource cross-lingual transfer learning allows models to work on languages with no training data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "BitFit fine-tunes only the bias terms of the model.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "IA3 (Infused Adapter by Inhibiting and Amplifying Inner Activations) scales activations with learned vectors.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "QLoRA combines Quantization and LoRA for efficient fine-tuning on consumer GPUs.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Gradient Checkpointing saves memory by recomputing activations during backward pass.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Mixed Precision training uses both 16-bit and 32-bit floating point numbers.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "ZeRO (Zero Redundancy Optimizer) partitions model states across GPUs.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Pipeline Parallelism splits the model layers across different devices.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Tensor Parallelism splits individual tensors (e.g., matrix multiplications) across devices.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange training parallelism:",
        "type": "rearrange",
        "words": [
            "Data",
            "Pipeline",
            "Tensor",
            "Model"
        ]
    },
    {
        "q": "Megatron-LM is a library for training large language models using model parallelism.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "DeepSpeed is a deep learning optimization library.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "FSDP (Fully Sharded Data Parallel) shards model parameters, gradients, and optimizer states.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the output?",
        "type": "mcq",
        "c": "print('abc'.upper())",
        "o": [
            "ABC",
            "abc",
            "Abc",
            "Error"
        ]
    },
    {
        "q": "Bloom is an open-science, multilingual language model.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Llama 2 is a family of open-access models released by Meta.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Falcon is a causal decoder-only model trained on RefinedWeb data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Mistral 7B uses Grouped-Query Attention and Sliding Window Attention.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Gemma is a family of lightweight, open models built from Gemini research.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the open model:",
        "type": "match",
        "left": [
            "Llama 2",
            "Mistral",
            "Falcon",
            "Bloom"
        ],
        "right": [
            "Meta",
            "Mistral AI",
            "TII",
            "BigScience"
        ]
    }
]
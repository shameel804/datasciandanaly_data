{
    "id": "learn_dl_intro",
    "topicId": "dl_intro",
    "topicTitle": "Neural Networks Basics",
    "description": "Introduction to neural network architecture and fundamentals",
    "baseKP": 85,
    "slides": [
        {
            "id": "dl_intro_1",
            "type": "content",
            "title": "Deep Learning Overview",
            "content": "# Neural Networks Basics ğŸ§ \n\n## What is Deep Learning?\n\nNeural networks with multiple layers that learn hierarchical representations.\n\n## Why Deep?\n\n- Learn complex patterns\n- Automatic feature extraction\n- State-of-the-art in many domains\n\n## Key Milestones\n\n| Year | Breakthrough |\n|------|-------------|\n| 2012 | AlexNet (ImageNet) |\n| 2014 | GANs |\n| 2017 | Transformers |\n| 2022 | ChatGPT |\n\n> ğŸ’¡ Deep learning powers modern AI!"
        },
        {
            "id": "dl_intro_2",
            "type": "content",
            "title": "Neuron and Layers",
            "content": "# Neurons and Layers ğŸ”—\n\n## Artificial Neuron\n\noutput = activation(Î£ weights Ã— inputs + bias)\n\n## Layer Types\n\n| Layer | Purpose |\n|-------|--------|\n| Input | Receive data |\n| Hidden | Learn features |\n| Output | Make predictions |\n\n## Activation Functions\n\n- ReLU: max(0, x)\n- Sigmoid: 1/(1+e^-x)\n- Tanh: (e^x - e^-x)/(e^x + e^-x)\n\n> ğŸ¯ Layers build understanding progressively!"
        },
        {
            "id": "dl_intro_quiz_1",
            "type": "quiz",
            "title": "Basics Quiz",
            "content": "Test your knowledge!",
            "quizQuestion": "What does ReLU activation return for negative inputs?",
            "quizOptions": [
                "The negative value",
                "Zero",
                "One",
                "The absolute value"
            ],
            "correctOptionIndex": 1
        },
        {
            "id": "dl_intro_3",
            "type": "content",
            "title": "Training Neural Networks",
            "content": "# Training Process ğŸ“ˆ\n\n## Key Concepts\n\n1. **Forward Pass** - Compute predictions\n2. **Loss Calculation** - Measure error\n3. **Backpropagation** - Compute gradients\n4. **Weight Update** - Improve model\n\n## Loss Functions\n\n| Task | Loss |\n|------|------|\n| Classification | Cross-entropy |\n| Regression | MSE |\n\n## Optimizers\n\n- SGD (Stochastic Gradient Descent)\n- Adam (popular default)\n- RMSprop\n\n> ğŸ’¡ Training iteratively reduces error!"
        },
        {
            "id": "dl_intro_4",
            "type": "content",
            "title": "Common Challenges",
            "content": "# Challenges in Deep Learning âš ï¸\n\n## Overfitting\n\nSolutions:\n- Dropout\n- Regularization\n- Early stopping\n- Data augmentation\n\n## Vanishing Gradients\n\nSolutions:\n- ReLU activation\n- Batch normalization\n- Skip connections\n\n## Training Tips\n\n| Tip | Benefit |\n|-----|--------|\n| Normalize inputs | Faster convergence |\n| Use batch norm | Stable training |\n| Start simple | Debug easier |"
        },
        {
            "id": "dl_intro_quiz_2",
            "type": "quiz",
            "title": "Training Quiz",
            "content": "Test your knowledge!",
            "quizQuestion": "Which technique randomly disables neurons during training?",
            "quizOptions": [
                "Batch normalization",
                "Dropout",
                "Weight decay",
                "Early stopping"
            ],
            "correctOptionIndex": 1
        },
        {
            "id": "dl_intro_5",
            "type": "content",
            "title": "Summary",
            "content": "# Congratulations! ğŸ‰\n\nYou've learned Neural Network Basics!\n\n## Key Takeaways\n\n- âœ… Neurons compute weighted sums + activation\n- âœ… Layers learn hierarchical features\n- âœ… Training uses backpropagation\n- âœ… Handle overfitting with dropout/regularization\n\n> ğŸš€ Deep learning is the foundation of modern AI!\n\nKeep learning! ğŸ§ "
        }
    ]
}
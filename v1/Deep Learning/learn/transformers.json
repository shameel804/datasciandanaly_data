{
    "id": "learn_transformers",
    "topicId": "transformers",
    "topicTitle": "Transformer Models",
    "description": "Master transformer architecture and modern language models",
    "baseKP": 90,
    "slides": [
        {
            "id": "transformers_1",
            "type": "content",
            "title": "Transformer Revolution",
            "content": "# Transformer Models ğŸ¤–\n\n## What Changed?\n\n\"Attention Is All You Need\" (2017) revolutionized NLP.\n\n## Key Innovation\n\n- Self-attention mechanism\n- Parallel processing\n- No recurrence needed\n\n## Impact\n\n| Before | After |\n|--------|-------|\n| RNN/LSTM | Transformers |\n| Sequential | Parallel |\n| Limited context | Long context |\n\n> ğŸ’¡ Transformers power modern AI!"
        },
        {
            "id": "transformers_2",
            "type": "content",
            "title": "Self-Attention",
            "content": "# Self-Attention Mechanism ğŸ”\n\n## Core Concept\n\nEach word attends to all other words.\n\n## Components\n\n- Query (Q)\n- Key (K) \n- Value (V)\n\n## Attention Formula\n\nAttention(Q,K,V) = softmax(QK^T/âˆšd)V\n\n## Multi-Head Attention\n\n- Multiple attention heads\n- Different perspectives\n- Richer representation\n\n> ğŸ¯ Attention captures relationships!"
        },
        {
            "id": "transformers_quiz_1",
            "type": "quiz",
            "title": "Attention Quiz",
            "content": "Test your knowledge!",
            "quizQuestion": "What are the three components of self-attention?",
            "quizOptions": [
                "Input, Hidden, Output",
                "Query, Key, Value",
                "Forward, Backward, Loss",
                "Encoder, Decoder, Attention"
            ],
            "correctOptionIndex": 1
        },
        {
            "id": "transformers_3",
            "type": "content",
            "title": "BERT and GPT",
            "content": "# BERT and GPT ğŸ†\n\n## BERT (Bidirectional)\n\n- Encoder-only\n- Bidirectional context\n- Great for understanding\n\n## GPT (Generative)\n\n- Decoder-only\n- Left-to-right\n- Great for generation\n\n| Model | Strength |\n|-------|----------|\n| BERT | Classification, NER |\n| GPT | Text generation |\n| T5 | Both (encoder-decoder) |\n\n> ğŸ’¡ Choose based on your task!"
        },
        {
            "id": "transformers_4",
            "type": "content",
            "title": "Modern LLMs",
            "content": "# Large Language Models ğŸš€\n\n## Evolution\n\n| Year | Model | Size |\n|------|-------|------|\n| 2018 | BERT | 340M |\n| 2020 | GPT-3 | 175B |\n| 2023 | GPT-4 | ~1T |\n\n## Capabilities\n\n- Reasoning\n- Code generation\n- Multimodal\n- Tool use\n\n## Challenges\n\n- Hallucination\n- Bias\n- Cost\n- Privacy"
        },
        {
            "id": "transformers_quiz_2",
            "type": "quiz",
            "title": "LLM Quiz",
            "content": "Test your knowledge!",
            "quizQuestion": "Which model is encoder-only and bidirectional?",
            "quizOptions": [
                "GPT",
                "BERT",
                "T5",
                "DALL-E"
            ],
            "correctOptionIndex": 1
        },
        {
            "id": "transformers_5",
            "type": "content",
            "title": "Summary",
            "content": "# Congratulations! ğŸ‰\n\nYou've learned Transformer Models!\n\n## Key Takeaways\n\n- âœ… Self-attention replaces recurrence\n- âœ… BERT for understanding\n- âœ… GPT for generation\n- âœ… LLMs increasingly capable\n\n> ğŸš€ Transformers are the foundation of modern AI!\n\nKeep building! ğŸ¤–"
        }
    ]
}
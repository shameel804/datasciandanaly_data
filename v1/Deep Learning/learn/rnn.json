{
    "id": "learn_rnn",
    "topicId": "rnn",
    "topicTitle": "Recurrent Neural Networks",
    "description": "Master RNNs and LSTMs for sequential data",
    "baseKP": 85,
    "slides": [
        {
            "id": "rnn_1",
            "type": "content",
            "title": "Introduction to RNNs",
            "content": "# Recurrent Neural Networks ğŸ”„\n\n## What are RNNs?\n\nNeural networks with memory for sequential data.\n\n## Key Feature\n\n- Process sequences\n- Share weights across time\n- Maintain hidden state\n\n## Applications\n\n| Task | Example |\n|------|--------|\n| NLP | Language modeling |\n| Speech | Voice recognition |\n| Time series | Stock prediction |\n\n> ğŸ’¡ RNNs remember the past!"
        },
        {
            "id": "rnn_2",
            "type": "content",
            "title": "LSTM and GRU",
            "content": "# LSTM and GRU ğŸ§ \n\n## Problem with Basic RNN\n\nVanishing gradients - can't learn long dependencies.\n\n## LSTM (Long Short-Term Memory)\n\n- Forget gate\n- Input gate\n- Output gate\n- Cell state\n\n## GRU (Gated Recurrent Unit)\n\n- Simpler than LSTM\n- Update and reset gates\n- Often similar performance\n\n> ğŸ¯ Use LSTM/GRU for long sequences!"
        },
        {
            "id": "rnn_quiz_1",
            "type": "quiz",
            "title": "RNN Quiz",
            "content": "Test your knowledge!",
            "quizQuestion": "What problem do LSTMs solve that basic RNNs have?",
            "quizOptions": [
                "Overfitting",
                "Vanishing gradients",
                "Slow training",
                "Memory usage"
            ],
            "correctOptionIndex": 1
        },
        {
            "id": "rnn_3",
            "type": "content",
            "title": "Sequence Tasks",
            "content": "# Sequence Tasks ğŸ“\n\n## Types\n\n| Type | Example |\n|------|--------|\n| Many-to-one | Sentiment analysis |\n| One-to-many | Image captioning |\n| Many-to-many | Translation |\n\n## Bidirectional RNN\n\n- Process forward and backward\n- Better context understanding\n\n## Attention (Preview)\n\n- Focus on relevant parts\n- Led to Transformers\n\n> ğŸ’¡ Choose architecture for your task!"
        },
        {
            "id": "rnn_quiz_2",
            "type": "quiz",
            "title": "Tasks Quiz",
            "content": "Test your knowledge!",
            "quizQuestion": "Sentiment analysis is which type of sequence task?",
            "quizOptions": [
                "One-to-one",
                "Many-to-one",
                "One-to-many",
                "Many-to-many"
            ],
            "correctOptionIndex": 1
        },
        {
            "id": "rnn_4",
            "type": "content",
            "title": "Summary",
            "content": "# Congratulations! ğŸ‰\n\nYou've learned RNNs!\n\n## Key Takeaways\n\n- âœ… RNNs process sequences\n- âœ… LSTM/GRU solve vanishing gradients\n- âœ… Bidirectional for better context\n- âœ… Transformers now often preferred\n\n> ğŸš€ RNNs handle sequential data!\n\nKeep learning! ğŸ”„"
        }
    ]
}
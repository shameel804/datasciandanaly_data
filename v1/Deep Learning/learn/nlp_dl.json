{
    "id": "learn_nlp_dl",
    "topicId": "nlp_dl",
    "topicTitle": "NLP with Deep Learning",
    "description": "Master deep learning approaches for natural language processing",
    "baseKP": 85,
    "slides": [
        {
            "id": "nlp_dl_1",
            "type": "content",
            "title": "NLP Overview",
            "content": "# NLP with Deep Learning ğŸ“\n\n## What is NLP?\n\nNatural Language Processing - teaching machines to understand human language.\n\n## Deep Learning Revolution\n\n| Era | Approach |\n|-----|----------|\n| Before 2013 | Hand-crafted features |\n| 2013-2017 | Word embeddings |\n| 2018+ | Transformers |\n\n## Applications\n\n- Machine translation\n- Sentiment analysis\n- Question answering\n- Chatbots\n\n> ğŸ’¡ DL enabled human-like language understanding!"
        },
        {
            "id": "nlp_dl_2",
            "type": "content",
            "title": "Word Embeddings",
            "content": "# Word Embeddings ğŸ”¤\n\n## Concept\n\nRepresent words as dense vectors capturing meaning.\n\n## Popular Methods\n\n| Method | Key Idea |\n|--------|----------|\n| Word2Vec | Predict word from context |\n| GloVe | Co-occurrence statistics |\n| FastText | Subword information |\n\n## Properties\n\n- Similar words â†’ similar vectors\n- Capture relationships\n- king - man + woman â‰ˆ queen\n\n> ğŸ¯ Embeddings are the foundation of NLP!"
        },
        {
            "id": "nlp_dl_quiz_1",
            "type": "quiz",
            "title": "NLP Quiz",
            "content": "Test your knowledge!",
            "quizQuestion": "What do word embeddings represent?",
            "quizOptions": [
                "Grammar rules",
                "Words as dense vectors",
                "Sentence structure",
                "Punctuation"
            ],
            "correctOptionIndex": 1
        },
        {
            "id": "nlp_dl_3",
            "type": "content",
            "title": "Sequence Models for NLP",
            "content": "# Sequence Models ğŸ”„\n\n## RNN for NLP\n\n- Process word by word\n- Maintain context\n- Bi-directional for better understanding\n\n## Attention Mechanism\n\n- Focus on relevant words\n- Solved long-range dependencies\n- Key innovation before Transformers\n\n## Tasks\n\n| Task | Model Type |\n|------|------------|\n| Classification | Many-to-one |\n| NER | Many-to-many |\n| Translation | Seq2seq |"
        },
        {
            "id": "nlp_dl_quiz_2",
            "type": "quiz",
            "title": "Models Quiz",
            "content": "Test your knowledge!",
            "quizQuestion": "What mechanism allows models to focus on relevant parts of input?",
            "quizOptions": [
                "Dropout",
                "Attention",
                "Pooling",
                "Embedding"
            ],
            "correctOptionIndex": 1
        },
        {
            "id": "nlp_dl_4",
            "type": "content",
            "title": "Summary",
            "content": "# Congratulations! ğŸ‰\n\nYou've learned NLP with Deep Learning!\n\n## Key Takeaways\n\n- âœ… Word embeddings capture meaning\n- âœ… RNNs process sequences  \n- âœ… Attention focuses on relevant parts\n- âœ… Transformers dominate modern NLP\n\n> ğŸš€ NLP enables human-machine communication!\n\nKeep learning! ğŸ“"
        }
    ]
}
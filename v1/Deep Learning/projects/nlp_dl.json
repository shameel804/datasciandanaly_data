[
    {
        "title": "Word Embeddings üî§",
        "ques": "What are **word embeddings**?",
        "answer": {
            "type": "text",
            "content": "### Word Embeddings:\n\n**Definition:** Dense vector representations of words.\n\n### Properties:\n- Similar words ‚Üí Similar vectors\n- Capture semantics\n- Fixed dimension (e.g., 300)\n\n### Methods: Word2Vec, GloVe, FastText"
        },
        "explanation": "**Embeddings** encode meaning as vectors."
    },
    {
        "title": "Seq2Seq Models üîÑ",
        "ques": "What is a **Sequence-to-Sequence** model?",
        "answer": {
            "type": "text",
            "content": "### Seq2Seq Architecture:\n\n**Structure:**\n- Encoder: Process input sequence\n- Decoder: Generate output sequence\n\n### Applications:\n| Task | Input ‚Üí Output |\n|------|---------------|\n| Translation | English ‚Üí French |\n| Summarization | Long ‚Üí Short |"
        },
        "explanation": "**Seq2Seq** maps sequences to sequences."
    },
    {
        "title": "Attention Mechanism üëÅÔ∏è",
        "ques": "What is **attention** in NLP?",
        "answer": {
            "type": "text",
            "content": "### Attention Mechanism:\n\n**Definition:** Focus on relevant input parts.\n\n### How It Works:\n1. Calculate relevance scores\n2. Create weighted combination\n3. Use for prediction\n\n### Benefit: Handles long sequences better"
        },
        "explanation": "**Attention** looks at the most important parts."
    },
    {
        "title": "BERT Model ü§ñ",
        "ques": "What makes **BERT** special?",
        "answer": {
            "type": "text",
            "content": "### BERT Features:\n\n| Feature | Benefit |\n|---------|--------|\n| **Bidirectional** | Full context |\n| **Pre-trained** | Transfer learning |\n| **Transformer** | Parallel processing |\n| **Fine-tunable** | Adapt to tasks |"
        },
        "explanation": "**BERT** understands context from both directions."
    },
    {
        "title": "GPT Models üí¨",
        "ques": "What is **GPT**?",
        "answer": {
            "type": "text",
            "content": "### GPT (Generative Pre-trained Transformer):\n\n**Approach:** Unidirectional, generative\n\n### Versions:\n| Version | Parameters |\n|---------|------------|\n| GPT-2 | 1.5B |\n| GPT-3 | 175B |\n| GPT-4 | Multimodal |\n\n### Use: Text generation, chat"
        },
        "explanation": "**GPT** excels at generating human-like text."
    },
    {
        "title": "Tokenization üî¢",
        "ques": "What is **tokenization** for transformers?",
        "answer": {
            "type": "text",
            "content": "### Transformer Tokenization:\n\n| Method | Approach |\n|--------|----------|\n| **WordPiece** | Subword units (BERT) |\n| **BPE** | Byte Pair Encoding (GPT) |\n\n### Purpose:\n- Handle any text\n- Manage vocabulary size\n- Deal with rare words"
        },
        "explanation": "**Subword tokenization** balances coverage and efficiency."
    }
]
{
    "id": "learn_ds_intro",
    "topicId": "ds_intro",
    "topicTitle": "Introduction to Data Science",
    "description": "Explore what Data Science is, the Data Science process, real-world applications, the tools ecosystem, and team roles",
    "baseKP": 75,
    "slides": [
        {
            "id": "ds_intro_1",
            "type": "content",
            "title": "Welcome to Data Science",
            "content": "# Introduction to Data Science ğŸ”¬\n\nWelcome to the fascinating world of Data Science! In this lesson, you'll discover the **foundations** of this transformative field.\n\n## What you'll learn:\n- **What is Data Science** - Definition and core concepts\n- **Data Science Process** - The systematic approach to solving problems\n- **Applications** - Real-world use cases across industries\n- **Tools Ecosystem** - Essential technologies and platforms\n- **Team Roles** - The people who make data magic happen\n\n> ğŸ’¡ **Fun Fact:** The term \"Data Science\" was coined in 2008 by DJ Patil and Jeff Hammerbacher, who worked at LinkedIn and Facebook respectively!\n\n## Why Data Science Matters?\n- ğŸ“ˆ **Data explosion** - We create 2.5 quintillion bytes of data daily\n- ğŸ’° **Business value** - Companies using data analytics are 5x more likely to make faster decisions\n- ğŸ¯ **Competitive edge** - Data-driven organizations outperform peers by 20%\n- ğŸš€ **Career growth** - #1 job in America for multiple years\n\nLet's dive in!"
        },
        {
            "id": "ds_intro_2",
            "type": "content",
            "title": "What is Data Science?",
            "content": "# What is Data Science? ğŸ§ª\n\nData Science is an **interdisciplinary field** that uses scientific methods, algorithms, and systems to extract knowledge and insights from structured and unstructured data.\n\n## The Data Science Venn Diagram\n\n```\n        Mathematics &\n         Statistics\n            /\\\n           /  \\\n          /    \\\n         / DATA \\\n        / SCIENCE \\\n       /__________\\\n      /            \\\n  Computer      Domain\n   Science      Expertise\n```\n\n## Three Pillars of Data Science\n\n| Pillar | Description | Examples |\n|--------|-------------|----------|\n| **Math/Statistics** | Foundation for analysis | Probability, Linear Algebra |\n| **Computer Science** | Tools and implementation | Python, Machine Learning |\n| **Domain Expertise** | Business context | Industry knowledge |\n\n## Key Characteristics\n\n- ğŸ” **Exploratory** - Discovering patterns and trends\n- ğŸ“Š **Quantitative** - Based on measurable data\n- ğŸ¤– **Automated** - Uses algorithms and models\n- ğŸ’¼ **Actionable** - Drives business decisions\n\n> ğŸ¯ **Pro Tip:** The best data scientists combine technical skills with strong business acumen and communication abilities."
        },
        {
            "id": "ds_intro_3",
            "type": "content",
            "title": "Types of Analytics",
            "content": "# Types of Analytics ğŸ“Š\n\nData Science encompasses four main types of analytics, each building on the previous.\n\n## The Analytics Maturity Model\n\n### 1. Descriptive Analytics ğŸ“‹\n**Question:** \"What happened?\"\n```python\ntotal_sales = df['sales'].sum()\nmonthly_avg = df.groupby('month')['sales'].mean()\n```\n\n<!-- FULL_CODE_START\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create sample sales data\nmonths = ['Jan', 'Jan', 'Feb', 'Feb', 'Mar', 'Mar', 'Apr', 'Apr', 'May', 'May']\nsales = [100, 150, 200, 180, 220, 240, 190, 210, 250, 270]\nproducts = ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B']\n\ndf = pd.DataFrame({\n    'month': months,\n    'sales': sales,\n    'product': products\n})\n\nprint(\"=== Descriptive Analytics Demo ===\\n\")\nprint(\"Sample Sales Data:\")\nprint(df)\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Descriptive analytics calculations\ntotal_sales = df['sales'].sum()\nmonthly_avg = df.groupby('month')['sales'].mean()\nproduct_sales = df.groupby('product')['sales'].sum()\n\nprint(f\"1. Total Sales: ${total_sales:,.2f}\")\nprint(\"\\n2. Monthly Average Sales:\")\nfor month, avg in monthly_avg.items():\n    print(f\"   {month}: ${avg:.2f}\")\n\nprint(\"\\n3. Sales by Product:\")\nfor product, total in product_sales.items():\n    print(f\"   Product {product}: ${total:,.2f}\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Create visualization\nplt.figure(figsize=(10, 5))\n\n# Bar chart for monthly averages\nplt.subplot(1, 2, 1)\nmonthly_avg.plot(kind='bar', color='skyblue')\nplt.title('Average Sales by Month')\nplt.ylabel('Sales ($)')\nplt.xticks(rotation=45)\nplt.grid(axis='y', alpha=0.3)\n\n# Pie chart for product distribution\nplt.subplot(1, 2, 2)\nproduct_sales.plot(kind='pie', autopct='%1.1f%%', colors=['lightcoral', 'lightgreen'])\nplt.title('Sales Distribution by Product')\nplt.ylabel('')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Descriptive analytics helps us understand WHAT happened in our sales data!\")\nFULL_CODE_END -->\n\n- Summarizes historical data\n- Dashboards and reports\n- Foundation of all analytics\n\n### 2. Diagnostic Analytics ğŸ”\n**Question:** \"Why did it happen?\"\n- Root cause analysis\n- Drill-down analysis\n- Correlation discovery\n\n### 3. Predictive Analytics ğŸ”®\n**Question:** \"What will happen?\"\n```python\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nfuture_sales = model.predict(X_future)\n```\n\n<!-- FULL_CODE_START\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\n# Create sample time series data for sales prediction\nnp.random.seed(42)\nn_periods = 24\nmonths = pd.date_range('2023-01-01', periods=n_periods, freq='M')\ntrend = np.linspace(100, 300, n_periods)\nseasonality = 50 * np.sin(2 * np.pi * np.arange(n_periods) / 12)\nnoise = np.random.normal(0, 20, n_periods)\nsales = trend + seasonality + noise\n\ndf = pd.DataFrame({\n    'date': months,\n    'sales': sales,\n    'month_num': np.arange(n_periods)\n})\n\nprint(\"=== Predictive Analytics Demo ===\\n\")\nprint(\"Sample Sales Data (24 months):\")\nprint(df.head())\nprint(f\"...\\n{df.tail()}\")\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Prepare data for prediction\nX = df[['month_num']]\ny = df['sales']\n\n# Split into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n\n# Train linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions\npredictions = model.predict(X_test)\n\n# Predict future sales\nfuture_months = pd.DataFrame({'month_num': [24, 25, 26, 27, 28]})\nfuture_sales = model.predict(future_months)\n\nprint(\"Model Results:\")\nprint(f\"Intercept: {model.intercept_:.2f}\")\nprint(f\"Coefficient: {model.coef_[0]:.2f}\")\nprint(f\"\\nTraining R-squared: {model.score(X_train, y_train):.3f}\")\nprint(f\"Test R-squared: {model.score(X_test, y_test):.3f}\")\nmae = mean_absolute_error(y_test, predictions)\nprint(f\"Mean Absolute Error: ${mae:.2f}\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\nprint(\"Future Sales Predictions:\")\nfuture_dates = pd.date_range('2025-01-01', periods=5, freq='M')\nfor date, pred in zip(future_dates, future_sales):\n    print(f\"  {date.strftime('%b %Y')}: ${pred:.2f}\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Visualization\nplt.figure(figsize=(12, 6))\n\n# Plot historical data\nplt.plot(df['date'], df['sales'], 'b-', label='Historical Sales', linewidth=2)\n\n# Plot predictions\nplt.plot(df['date'][-len(predictions):], predictions, 'r--', label='Predictions', linewidth=2)\n\n# Plot future predictions\nplt.plot(future_dates, future_sales, 'g--', label='Future Forecast', linewidth=2, marker='o')\n\nplt.title('Sales Prediction with Linear Regression')\nplt.xlabel('Date')\nplt.ylabel('Sales ($)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\nprint(\"Predictive analytics helps us forecast WHAT WILL happen with our sales!\")\nFULL_CODE_END -->\n\n- Machine learning models\n- Forecasting\n- Risk assessment\n\n### 4. Prescriptive Analytics âš¡\n**Question:** \"What should we do?\"\n- Optimization algorithms\n- Recommendation systems\n- Automated decision-making\n\n> ğŸ“ˆ **Value Ladder:** As you move up from Descriptive to Prescriptive, both **complexity** and **business value** increase!"
        },
        {
            "id": "ds_intro_quiz_1",
            "type": "quiz",
            "title": "Quick Check",
            "content": "Test your understanding of Data Science fundamentals!",
            "quizQuestion": "Which type of analytics answers the question 'What will happen?'",
            "quizOptions": [
                "Descriptive Analytics",
                "Diagnostic Analytics",
                "Predictive Analytics",
                "Prescriptive Analytics"
            ],
            "correctOptionIndex": 2
        },
        {
            "id": "ds_intro_4",
            "type": "content",
            "title": "The Data Science Process",
            "content": "# Data Science Process ğŸ”„\n\nData Science follows a systematic, iterative process to solve problems.\n\n## CRISP-DM Framework\n\nThe **Cross-Industry Standard Process for Data Mining** is the most widely-used methodology:\n\n### 1. Business Understanding ğŸ’¼\n- Define objectives\n- Assess resources\n- Determine success criteria\n\n### 2. Data Understanding ğŸ”\n- Collect initial data\n- Describe data\n- Explore data quality\n\n### 3. Data Preparation ğŸ§¹\n- Clean and transform data\n- Handle missing values\n- Feature engineering\n\n### 4. Modeling ğŸ¤–\n- Select modeling techniques\n- Build and train models\n- Parameter tuning\n\n### 5. Evaluation âœ…\n- Evaluate model performance\n- Review business criteria\n- Determine next steps\n\n### 6. Deployment ğŸš€\n- Plan deployment\n- Monitor and maintain\n- Produce final report\n\n> ğŸ’¡ **Key Point:** This is an **iterative process** - you often go back to previous steps as you learn more!"
        },
        {
            "id": "ds_intro_5",
            "type": "content",
            "title": "Data Science Process Details",
            "content": "# Deep Dive: Process Steps ğŸ”¬\n\n## Business Understanding\n\n```python\nproblem = \"Reduce customer churn by 20% in Q4\"\nmetrics = [\"churn_rate\", \"customer_ltv\", \"satisfaction_score\"]\nstakeholders = [\"Marketing\", \"Product\", \"Customer Success\"]\n```\n\n<!-- FULL_CODE_START\n# Business Understanding Phase Demo\n\ndef define_business_problem():\n    \"\"\"Define the business problem clearly\"\"\"\n    problem = {\n        \"problem_statement\": \"Reduce customer churn by 20% in Q4\",\n        \"current_churn_rate\": 15.2,  # percentage\n        \"target_churn_rate\": 12.2,   # percentage\n        \"timeframe\": \"Q4 2024\",\n        \"business_impact\": \"Expected to save $500,000 in customer acquisition costs\",\n        \"success_metric\": \"Churn rate reduction\"\n    }\n    return problem\n\ndef define_metrics():\n    \"\"\"Define key metrics to track\"\"\"\n    metrics = {\n        \"primary\": \"churn_rate\",\n        \"secondary\": [\"customer_ltv\", \"satisfaction_score\", \"engagement_score\"],\n        \"leading_indicators\": [\"login_frequency\", \"feature_usage\", \"support_tickets\"],\n        \"lagging_indicators\": [\"revenue_per_user\", \"retention_rate\"]\n    }\n    return metrics\n\ndef identify_stakeholders():\n    \"\"\"Identify key stakeholders and their interests\"\"\"\n    stakeholders = {\n        \"Marketing\": {\n            \"interests\": [\"customer_acquisition_cost\", \"campaign_roi\"],\n            \"contact\": \"cmorrison@company.com\",\n            \"priority\": \"High\"\n        },\n        \"Product\": {\n            \"interests\": [\"feature_adoption\", \"user_engagement\"],\n            \"contact\": \"jchen@company.com\",\n            \"priority\": \"High\"\n        },\n        \"Customer Success\": {\n            \"interests\": [\"satisfaction_score\", \"ticket_resolution_time\"],\n            \"contact\": \"srodriguez@company.com\",\n            \"priority\": \"Medium\"\n        },\n        \"Finance\": {\n            \"interests\": [\"customer_lifetime_value\", \"revenue_impact\"],\n            \"contact\": \"rpatel@company.com\",\n            \"priority\": \"Medium\"\n        }\n    }\n    return stakeholders\n\nprint(\"=== Business Understanding Phase ===\\n\")\n\n# Define the problem\nproblem = define_business_problem()\nprint(\"1. Problem Definition:\")\nfor key, value in problem.items():\n    print(f\"   â€¢ {key.replace('_', ' ').title()}: {value}\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Define metrics\nmetrics = define_metrics()\nprint(\"2. Key Metrics:\")\nprint(f\"   Primary Metric: {metrics['primary']}\")\nprint(f\"   Secondary Metrics: {', '.join(metrics['secondary'])}\")\nprint(f\"   Leading Indicators: {', '.join(metrics['leading_indicators'])}\")\nprint(f\"   Lagging Indicators: {', '.join(metrics['lagging_indicators'])}\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Identify stakeholders\nstakeholders = identify_stakeholders()\nprint(\"3. Stakeholder Analysis:\")\nfor dept, info in stakeholders.items():\n    print(f\"\\n   {dept} Department:\")\n    print(f\"     â€¢ Interests: {', '.join(info['interests'])}\")\n    print(f\"     â€¢ Contact: {info['contact']}\")\n    print(f\"     â€¢ Priority: {info['priority']}\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Create project brief\nprint(\"4. Project Brief Summary:\")\nprint(f\"\"\"\n   Project: Churn Reduction Initiative\n   Objective: {problem['problem_statement']}\n   Timeline: {problem['timeframe']}\n   Key Metric: {metrics['primary']}\n   Target: Reduce from {problem['current_churn_rate']}% to {problem['target_churn_rate']}%\n   Expected Impact: {problem['business_impact']}\n   Key Stakeholders: {', '.join(stakeholders.keys())}\n\"\"\")\n\nprint(\"\\nBusiness understanding phase complete! Ready for data collection.\")\nFULL_CODE_END -->\n\n## Data Understanding\n\n| Activity | Purpose | Output |\n|----------|---------|--------|\n| Data Collection | Gather all relevant data | Dataset inventory |\n| Data Profiling | Understand data characteristics | Summary statistics |\n| Data Quality Check | Identify issues | Quality report |\n\n## Data Preparation (Often 60-80% of time!)\n\n```python\ndf = df.dropna()\ndf = pd.get_dummies(df, columns=['category'])\ndf['amount'] = (df['amount'] - mean) / std\n```\n\n<!-- FULL_CODE_START\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\nprint(\"=== Data Preparation Demo ===\\n\")\n\n# Create a realistic sample dataset with common data issues\ndata = {\n    'customer_id': [101, 102, 103, 104, 105, 106, 107, 108],\n    'age': [25, 32, 45, np.nan, 28, 33, 50, 29],\n    'income': [50000, 62000, 80000, 55000, np.nan, 72000, 95000, 48000],\n    'category': ['A', 'B', 'A', 'C', 'B', 'A', np.nan, 'C'],\n    'purchase_amount': [150.50, 89.99, 200.00, 45.75, 120.25, 300.00, 75.50, 180.00],\n    'churned': [0, 1, 0, 1, 0, 1, 0, 0]\n}\n\ndf = pd.DataFrame(data)\nprint(\"1. Original Dataset (with issues):\")\nprint(df)\nprint(f\"\\nShape: {df.shape}\")\nprint(f\"\\nMissing Values Count:\")\nprint(df.isnull().sum())\nprint(f\"\\nData Types:\\n{df.dtypes}\")\n\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# Step 1: Handle missing values\nprint(\"2. Handling Missing Values:\")\n\n# For numerical columns, impute with median\nnumeric_cols = ['age', 'income']\nimputer = SimpleImputer(strategy='median')\ndf[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n\n# For categorical columns, impute with most frequent\ncategorical_cols = ['category']\nimputer_cat = SimpleImputer(strategy='most_frequent')\ndf[categorical_cols] = imputer_cat.fit_transform(df[categorical_cols])\n\nprint(\"After imputation:\")\nprint(df[numeric_cols + categorical_cols])\nprint(f\"\\nMissing values after imputation:\\n{df.isnull().sum()}\")\n\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# Step 2: Encode categorical variables\nprint(\"3. Encoding Categorical Variables:\")\n\n# Option A: One-hot encoding\ndf_encoded = pd.get_dummies(df, columns=['category'], prefix='cat')\nprint(\"One-hot encoded columns:\")\nencoded_cols = [col for col in df_encoded.columns if 'cat_' in col]\nprint(encoded_cols)\n\n# Option B: Label encoding\nlabel_encoder = LabelEncoder()\ndf['category_encoded'] = label_encoder.fit_transform(df['category'])\nprint(f\"\\nLabel encoded categories:\")\nprint(f\"Mapping: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# Step 3: Normalize numerical features\nprint(\"4. Feature Normalization:\")\n\nscaler = StandardScaler()\nnumeric_features = ['age', 'income', 'purchase_amount']\ndf_normalized = df.copy()\ndf_normalized[numeric_features] = scaler.fit_transform(df[numeric_features])\n\nprint(\"Original vs Normalized (first 3 rows):\")\ncomparison = pd.DataFrame({\n    'age_orig': df['age'].head(3).values,\n    'age_norm': df_normalized['age'].head(3).values,\n    'income_orig': df['income'].head(3).values,\n    'income_norm': df_normalized['income'].head(3).values,\n    'purchase_orig': df['purchase_amount'].head(3).values,\n    'purchase_norm': df_normalized['purchase_amount'].head(3).values\n})\nprint(comparison)\n\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# Step 4: Create new features\nprint(\"5. Feature Engineering:\")\n\n# Create interaction features\ndf_normalized['income_age_ratio'] = df_normalized['income'] / (df_normalized['age'] + 1)\n\n# Create bins for age\ndf_normalized['age_group'] = pd.cut(df['age'], bins=[20, 30, 40, 50, 60], \n                                    labels=['20-30', '30-40', '40-50', '50-60'])\n\n# Create binary flag for high-value purchases\ndf_normalized['high_value_purchase'] = (df['purchase_amount'] > df['purchase_amount'].median()).astype(int)\n\nprint(\"New features created:\")\nprint(f\"  - income_age_ratio: Ratio of income to age\")\nprint(f\"  - age_group: Categorical age groups\")\nprint(f\"  - high_value_purchase: Flag for purchases above median\")\n\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# Final dataset\nprint(\"6. Final Prepared Dataset:\")\nprint(f\"Original shape: {df.shape}\")\nprint(f\"Final shape: {df_normalized.shape}\")\nprint(f\"\\nColumns in final dataset:\")\nprint(list(df_normalized.columns))\nprint(f\"\\nFirst few rows:\")\nprint(df_normalized.head())\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Data preparation complete! Dataset is ready for modeling.\")\nFULL_CODE_END -->\n\n## Key Principle: Garbage In, Garbage Out\n\n> âš ï¸ **Warning:** No algorithm can fix bad data! Quality data preparation is crucial for success.\n\n## Success Factors\n- âœ… Clear problem definition\n- âœ… Quality data\n- âœ… Appropriate model selection\n- âœ… Continuous iteration\n- âœ… Stakeholder alignment"
        },
        {
            "id": "ds_intro_quiz_2",
            "type": "quiz",
            "title": "Application Check",
            "content": "Test your knowledge of Data Science applications!",
            "quizQuestion": "Which of the following is NOT a typical Data Science application in the finance industry?",
            "quizOptions": [
                "Fraud detection",
                "Credit scoring",
                "Disease prediction",
                "Algorithmic trading"
            ],
            "correctOptionIndex": 2
        },
        {
            "id": "ds_intro_7",
            "type": "content",
            "title": "The Data Science Tools Ecosystem",
            "content": "# Tools Ecosystem ğŸ› ï¸\n\nData Scientists use a diverse set of tools across the workflow.\n\n## Programming Languages\n\n| Language | Strengths | Use Cases |\n|----------|-----------|------------|\n| **Python** ğŸ | Versatile, ML libraries | General purpose, ML |\n| **R** ğŸ“Š | Statistics, visualization | Research, academia |\n| **SQL** ğŸ—„ï¸ | Data querying | Database operations |\n| **Scala** âš¡ | Big data processing | Spark applications |\n\n## Key Python Libraries\n\n```python\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nimport tensorflow as tf\n```\n\n<!-- FULL_CODE_START\n# Python Data Science Ecosystem Demo\n\nprint(\"=== Python Data Science Ecosystem Demo ===\\n\")\n\n# 1. Pandas for data manipulation\nimport pandas as pd\nimport numpy as np\n\nprint(\"1. PANDAS - Data Manipulation:\")\nprint(\"   Creating and manipulating DataFrames...\")\n\n# Create sample data\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n    'Age': [25, 32, 45, 28, 33],\n    'Salary': [50000, 62000, 80000, 55000, 72000],\n    'Department': ['HR', 'Engineering', 'Engineering', 'HR', 'Marketing']\n}\n\ndf = pd.DataFrame(data)\nprint(f\"\\nSample DataFrame:\\n{df}\")\nprint(f\"\\nDataFrame Info:\")\ndf.info()\nprint(f\"\\nBasic Statistics:\\n{df.describe()}\")\nprint(f\"\\nGroup by Department (Average Salary):\\n{df.groupby('Department')['Salary'].mean()}\")\n\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# 2. NumPy for numerical computing\nprint(\"2. NUMPY - Numerical Computing:\")\nprint(\"   Creating arrays and performing calculations...\")\n\n# Create arrays\narr1 = np.array([1, 2, 3, 4, 5])\narr2 = np.array([10, 20, 30, 40, 50])\n\nprint(f\"\\nArray 1: {arr1}\")\nprint(f\"Array 2: {arr2}\")\nprint(f\"Sum: {arr1 + arr2}\")\nprint(f\"Mean of Array 1: {np.mean(arr1)}\")\nprint(f\"Standard Deviation of Array 2: {np.std(arr2)}\")\n\n# Matrix operations\nmatrix = np.array([[1, 2], [3, 4]])\nprint(f\"\\nMatrix:\\n{matrix}\")\nprint(f\"Matrix Transpose:\\n{matrix.T}\")\nprint(f\"Matrix Determinant: {np.linalg.det(matrix)}\")\n\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# 3. Matplotlib for visualization\nimport matplotlib.pyplot as plt\n\nprint(\"3. MATPLOTLIB - Data Visualization:\")\nprint(\"   Creating visualizations...\")\n\n# Create sample data for visualization\nx = np.linspace(0, 10, 100)\ny1 = np.sin(x)\ny2 = np.cos(x)\n\n# Create figure with subplots\nfig, axes = plt.subplots(2, 2, figsize=(10, 8))\n\n# Plot 1: Line plot\naxes[0, 0].plot(x, y1, 'b-', label='sin(x)')\naxes[0, 0].plot(x, y2, 'r--', label='cos(x)')\naxes[0, 0].set_title('Sine and Cosine Waves')\naxes[0, 0].set_xlabel('x')\naxes[0, 0].set_ylabel('y')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Plot 2: Scatter plot\nnp.random.seed(42)\nx_scatter = np.random.randn(50)\ny_scatter = 2 * x_scatter + np.random.randn(50) * 0.5\naxes[0, 1].scatter(x_scatter, y_scatter, alpha=0.6, color='green')\naxes[0, 1].set_title('Scatter Plot with Noise')\naxes[0, 1].set_xlabel('x')\naxes[0, 1].set_ylabel('y')\naxes[0, 1].grid(True, alpha=0.3)\n\n# Plot 3: Bar chart from our DataFrame\ndept_salary = df.groupby('Department')['Salary'].mean()\naxes[1, 0].bar(dept_salary.index, dept_salary.values, color=['skyblue', 'lightcoral', 'lightgreen'])\naxes[1, 0].set_title('Average Salary by Department')\naxes[1, 0].set_xlabel('Department')\naxes[1, 0].set_ylabel('Average Salary ($)')\naxes[1, 0].tick_params(axis='x', rotation=45)\n\n# Plot 4: Histogram\nages = df['Age'].values\naxes[1, 1].hist(ages, bins=5, color='purple', alpha=0.7, edgecolor='black')\naxes[1, 1].set_title('Age Distribution')\naxes[1, 1].set_xlabel('Age')\naxes[1, 1].set_ylabel('Frequency')\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nprint(\"   Visualizations created! (Check your plot window)\")\nplt.show()\n\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# 4. Scikit-learn for machine learning\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nprint(\"4. SCIKIT-LEARN - Machine Learning:\")\nprint(\"   Loading dataset and training a simple model...\")\n\n# Load sample dataset\ndiabetes = datasets.load_diabetes()\nX = diabetes.data[:, :2]  # Use only first two features for simplicity\ny = diabetes.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Evaluate\nmse = mean_squared_error(y_test, y_pred)\nprint(f\"\\nModel trained successfully!\")\nprint(f\"Coefficients: {model.coef_}\")\nprint(f\"Intercept: {model.intercept_:.2f}\")\nprint(f\"Mean Squared Error: {mse:.2f}\")\nprint(f\"R-squared Score: {model.score(X_test, y_test):.3f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"\\nPython ecosystem demonstration complete!\")\nprint(\"These libraries form the foundation of data science work in Python.\")\nFULL_CODE_END -->\n\n## Data Storage & Processing\n\n- **Databases:** PostgreSQL, MongoDB, Redis\n- **Big Data:** Hadoop, Spark, Kafka\n- **Cloud:** AWS, Google Cloud, Azure\n\n## Visualization Tools\n\n- **Code-based:** Matplotlib, Seaborn, Plotly\n- **BI Tools:** Tableau, Power BI, Looker\n- **Notebooks:** Jupyter, Google Colab\n\n> ğŸ’¡ **Tip:** Start with Python + Pandas + Scikit-learn. These cover 80% of what you'll need!"
        },
        {
            "id": "ds_intro_8",
            "type": "content",
            "title": "Development Environments",
            "content": "# Development Environments ğŸ’»\n\nChoosing the right environment boosts productivity significantly.\n\n## Jupyter Notebooks ğŸ““\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = {'x': [1, 2, 3, 4, 5], 'y': [2, 4, 6, 8, 10]}\ndf = pd.DataFrame(data)\ndf.head()\n```\n\n<!-- FULL_CODE_START\n# Jupyter Notebook Demo - Complete Working Notebook\n\n# Cell 1: Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\nprint(\"Libraries imported successfully!\")\n\n# Cell 2: Load dataset\niris = load_iris()\ndf = pd.DataFrame(data=iris.data, columns=iris.feature_names)\ndf['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n\nprint(\"Dataset loaded successfully!\")\nprint(f\"Shape: {df.shape}\")\nprint(f\"\\nFirst 5 rows:\")\nprint(df.head())\n\n# Cell 3: Data summary\nprint(\"Dataset Information:\")\nprint(\"=\" * 40)\ndf.info()\n\nprint(\"\\n\\nBasic Statistics:\")\nprint(\"=\" * 40)\nprint(df.describe())\n\nprint(\"\\n\\nSpecies Distribution:\")\nprint(\"=\" * 40)\nprint(df['species'].value_counts())\n\n# Cell 4: Set up visualization style\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\n# Cell 5: Create multiple visualizations\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# 1. Scatter plot: Sepal length vs width colored by species\nfor species in df['species'].unique():\n    species_data = df[df['species'] == species]\n    axes[0, 0].scatter(species_data['sepal length (cm)'], \n                      species_data['sepal width (cm)'], \n                      label=species, alpha=0.7)\naxes[0, 0].set_xlabel('Sepal Length (cm)')\naxes[0, 0].set_ylabel('Sepal Width (cm)')\naxes[0, 0].set_title('Sepal Dimensions by Species')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# 2. Histogram of petal length\naxes[0, 1].hist([df[df['species'] == species]['petal length (cm)'] \n                for species in df['species'].unique()],\n               label=df['species'].unique(), \n               bins=15, alpha=0.7, stacked=True)\naxes[0, 1].set_xlabel('Petal Length (cm)')\naxes[0, 1].set_ylabel('Frequency')\naxes[0, 0].set_title('Petal Length Distribution')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# 3. Box plot of sepal length by species\ndf.boxplot(column='sepal length (cm)', by='species', ax=axes[1, 0])\naxes[1, 0].set_xlabel('Species')\naxes[1, 0].set_ylabel('Sepal Length (cm)')\naxes[1, 0].set_title('Sepal Length by Species')\naxes[1, 0].grid(True, alpha=0.3)\n\n# 4. Correlation heatmap\ncorrelation = df.select_dtypes(include=[np.number]).corr()\nim = axes[1, 1].imshow(correlation, cmap='coolwarm', aspect='auto')\naxes[1, 1].set_title('Feature Correlation Heatmap')\naxes[1, 1].set_xticks(range(len(correlation.columns)))\naxes[1, 1].set_xticklabels(correlation.columns, rotation=45, ha='right')\naxes[1, 1].set_yticks(range(len(correlation.columns)))\naxes[1, 1].set_yticklabels(correlation.columns)\n\n# Add colorbar\nplt.colorbar(im, ax=axes[1, 1])\n\nplt.tight_layout()\nprint(\"Visualizations created successfully!\")\nplt.show()\n\n# Cell 6: Calculate average measurements by species\nprint(\"Average Measurements by Species:\")\nprint(\"=\" * 50)\n\navg_by_species = df.groupby('species').mean()\nprint(avg_by_species.round(2))\n\nprint(\"\\n\" + \"=\"*50)\n\n# Calculate ratios\ndf['sepal_ratio'] = df['sepal length (cm)'] / df['sepal width (cm)']\ndf['petal_ratio'] = df['petal length (cm)'] / df['petal width (cm)']\n\nratio_avg = df.groupby('species')[['sepal_ratio', 'petal_ratio']].mean()\nprint(ratio_avg.round(2))\n\n# Cell 7: Interactive filtering\nprint(\"Filtering Example:\")\nprint(\"=\" * 50)\n\nlarge_sepal = df[df['sepal length (cm)'] > df['sepal length (cm)'].mean()]\nprint(f\"Number of flowers with above-average sepal length: {len(large_sepal)}/{len(df)}\")\nprint(f\"\\nSpecies distribution in large sepal group:\")\nprint(large_sepal['species'].value_counts())\n\n# Cell 8: Save processed data\nsummary = df.groupby('species').agg({\n    'sepal length (cm)': ['mean', 'std', 'min', 'max'],\n    'petal length (cm)': ['mean', 'std', 'min', 'max']\n}).round(2)\n\nprint(\"Summary Statistics:\")\nprint(\"=\" * 50)\nprint(summary)\n\nsummary.to_csv('iris_analysis_summary.csv')\nprint(\"\\nSummary saved to 'iris_analysis_summary.csv'\")\nFULL_CODE_END -->\n\n**Pros:**\n- Interactive coding\n- Mix code, text, and visuals\n- Easy to share\n\n**Cons:**\n- Not ideal for production code\n- Version control challenges\n\n## IDEs (Integrated Development Environment)\n\n| Tool | Best For |\n|------|----------|\n| **VS Code** | General purpose, lightweight |\n| **PyCharm** | Python development |\n| **RStudio** | R programming |\n| **Spyder** | Scientific Python |\n\n## Cloud Platforms â˜ï¸\n\n- **Google Colab** - Free GPU, collaborative\n- **Amazon SageMaker** - End-to-end ML platform\n- **Databricks** - Unified analytics platform\n- **Kaggle Notebooks** - Competition platform\n\n## Version Control with Git\n\n```bash\ngit init\ngit add .\ngit commit -m \"Initial data analysis\"\ngit push origin main\n```\n\n<!-- FULL_CODE_START\n# Git Version Control Demo\n\necho \"=== Git Version Control Demo ===\"\necho \"\"\necho \"1. Initialize a new git repository\"\necho \"mkdir my_data_science_project\"\necho \"cd my_data_science_project\"\necho \"git init\"\necho \"\"\n\necho \"2. Create project structure\"\necho \"mkdir -p data/raw data/processed\"\necho \"mkdir -p notebooks scripts models\"\necho \"mkdir -p docs tests\"\necho \"touch README.md requirements.txt .gitignore\"\necho \"\"\n\necho \"3. Configure .gitignore for data science\"\necho \"# Add to .gitignore:\"\necho \"cat > .gitignore << 'EOF'\"\necho \"# Data files\"\necho \"data/raw/\"\necho \"data/processed/\"\necho \"*.csv\"\necho \"*.pkl\"\necho \"*.h5\"\necho \"\"\necho \"# Python\"\necho \"__pycache__/\"\necho \"*.py[cod]\"\necho \"*$py.class\"\necho \"\"\necho \"# Jupyter\"\necho \".ipynb_checkpoints\"\necho \"\"\necho \"# Environment\"\necho \".env\"\necho \".venv\"\necho \"env/\"\necho \"venv/\"\necho \"ENV/\"\necho \"\"\necho \"# IDE\"\necho \".vscode/\"\necho \".idea/\"\necho \"*.swp\"\necho \"*.swo\"\necho \"EOF\"\necho \"\"\n\necho \"4. Stage and commit initial setup\"\necho \"git add .\"\necho \"git commit -m \"Initial project setup with basic structure\"\"\necho \"\"\n\necho \"5. Create a new branch for feature development\"\necho \"git checkout -b feature/add-model-training\"\necho \"\"\n\necho \"6. Add a model training script\"\necho \"cat > scripts/model_training.py << 'EOF'\"\necho \"from sklearn.model_selection import train_test_split\"\necho \"from sklearn.ensemble import RandomForestClassifier\"\necho \"from sklearn.metrics import accuracy_score\"\necho \"import joblib\"\necho \"\"\necho \"def train_model(X, y):\"\necho \"    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\"\necho \"    model = RandomForestClassifier(n_estimators=100, random_state=42)\"\necho \"    model.fit(X_train, y_train)\"\necho \"    y_pred = model.predict(X_test)\"\necho \"    accuracy = accuracy_score(y_test, y_pred)\"\necho \"    print(f\\\"Model accuracy: {accuracy:.3f}\\\")\"\necho \"    return model\"\necho \"\"\necho \"def save_model(model, filepath):\"\necho \"    joblib.dump(model, filepath)\"\necho \"    print(f\\\"Model saved to {filepath}\\\")\"\necho \"EOF\"\necho \"\"\n\necho \"7. Commit the feature\"\necho \"git add scripts/model_training.py\"\necho \"git commit -m \"Add model training script with Random Forest\"\"\necho \"\"\n\necho \"8. Merge feature back to main\"\necho \"git checkout main\"\necho \"git merge feature/add-model-training\"\necho \"\"\n\necho \"9. Connect to remote repository\"\necho \"git remote add origin https://github.com/yourusername/my_data_science_project.git\"\necho \"git branch -M main\"\necho \"git push -u origin main\"\necho \"\"\n\necho \"=== Demo Complete ===\"\necho \"This workflow helps maintain version control in data science projects.\"\nFULL_CODE_END -->\n\n> ğŸ¯ **Best Practice:** Use Jupyter for exploration, move to IDEs for production code!"
        },
        {
            "id": "ds_intro_9",
            "type": "content",
            "title": "Data Science Team Roles",
            "content": "# Team Roles ğŸ‘¥\n\nData Science teams consist of specialized roles that work together.\n\n## Core Roles\n\n### Data Scientist ğŸ§¬\n- Builds predictive models\n- Statistical analysis\n- Machine learning implementation\n- **Skills:** Python, ML, Statistics\n\n### Data Analyst ğŸ“Š\n- Creates reports and dashboards\n- Business insights\n- SQL and visualization\n- **Skills:** SQL, Excel, Tableau\n\n### Data Engineer ğŸ”§\n- Builds data pipelines\n- Database management\n- Infrastructure\n- **Skills:** Python, SQL, Spark, Cloud\n\n### Machine Learning Engineer ğŸ¤–\n- Deploys ML models to production\n- Scales and optimizes models\n- MLOps practices\n- **Skills:** Python, Docker, Kubernetes\n\n## The Data Team Workflow\n\n```\nData Engineer â†’ Data Analyst â†’ Data Scientist â†’ ML Engineer\n     â†“              â†“               â†“              â†“\n  Build         Analyze          Model          Deploy\n  Pipeline      & Report         & Train        & Scale\n```\n\n> ğŸ’¼ **Career Tip:** Start as an analyst, then specialize based on your interests!"
        },
        {
            "id": "ds_intro_10",
            "type": "content",
            "title": "Emerging Roles in Data Science",
            "content": "# Emerging Roles ğŸŒŸ\n\nThe data field continues to evolve with new specialized roles.\n\n## Modern Data Roles\n\n### Analytics Engineer ğŸ“\n- Bridge between data engineers and analysts\n- dbt and data transformation\n- Data modeling and testing\n\n### MLOps Engineer âš™ï¸\n- CI/CD for machine learning\n- Model monitoring and versioning\n- Infrastructure automation\n\n### AI/ML Research Scientist ğŸ”¬\n- Novel algorithm development\n- Academic research\n- Cutting-edge innovation\n\n### Data Product Manager ğŸ“‹\n- Data strategy\n- Stakeholder management\n- Product roadmap for data\n\n## Skills Matrix\n\n| Role | Technical | Business | Communication |\n|------|-----------|----------|---------------|\n| Data Scientist | â­â­â­â­ | â­â­â­ | â­â­â­ |\n| Data Analyst | â­â­â­ | â­â­â­â­ | â­â­â­â­ |\n| Data Engineer | â­â­â­â­â­ | â­â­ | â­â­ |\n| ML Engineer | â­â­â­â­â­ | â­â­ | â­â­â­ |\n\n> ğŸš€ **Future Outlook:** As AI advances, roles requiring domain expertise and ethical judgment will become more valuable!"
        },
        {
            "id": "ds_intro_quiz_3",
            "type": "quiz",
            "title": "Roles Quiz",
            "content": "Test your knowledge of Data Science roles!",
            "quizQuestion": "Which role is primarily responsible for building and maintaining data pipelines?",
            "quizOptions": [
                "Data Scientist",
                "Data Analyst",
                "Data Engineer",
                "Business Analyst"
            ],
            "correctOptionIndex": 2
        },
        {
            "id": "ds_intro_11",
            "type": "content",
            "title": "Getting Started in Data Science",
            "content": "# Getting Started ğŸš€\n\nReady to begin your Data Science journey? Here's your roadmap.\n\n## Learning Path\n\n### Phase 1: Foundations (1-3 months)\n- âœ… Python programming basics\n- âœ… Statistics fundamentals\n- âœ… SQL for data querying\n- âœ… Pandas for data manipulation\n\n### Phase 2: Core Skills (3-6 months)\n- âœ… Data visualization\n- âœ… Exploratory data analysis\n- âœ… Machine learning basics\n- âœ… Feature engineering\n\n### Phase 3: Advanced Topics (6-12 months)\n- âœ… Deep learning\n- âœ… Big data technologies\n- âœ… Cloud platforms\n- âœ… Specialized domains\n\n## Build Your Portfolio\n\n```python\nprojects = [\n    \"Exploratory Data Analysis on Kaggle dataset\",\n    \"Predictive model for house prices\",\n    \"Dashboard for business metrics\",\n    \"NLP sentiment analysis project\",\n    \"Recommendation system\"\n]\n```\n\n<!-- FULL_CODE_START\n# Data Science Portfolio Projects Demo\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nprint(\"=== Data Science Portfolio Projects Demo ===\\n\")\nprint(\"This script demonstrates 3 portfolio project ideas:\")\nprint(\"1. Exploratory Data Analysis (EDA)\")\nprint(\"2. Predictive Modeling\")\nprint(\"3. Data Visualization Dashboard\")\nprint(\"\")\n\n# Project 1: Exploratory Data Analysis\nprint(\"=\" * 60)\nprint(\"PROJECT 1: EXPLORATORY DATA ANALYSIS\")\nprint(\"=\" * 60)\n\nhousing = fetch_california_housing()\ndf_housing = pd.DataFrame(housing.data, columns=housing.feature_names)\ndf_housing['Price'] = housing.target * 100000\n\nprint(\"\\nDataset Overview:\")\nprint(f\"Shape: {df_housing.shape}\")\nprint(f\"Columns: {list(df_housing.columns)}\")\nprint(\"\\n\\nBasic Statistics:\")\nprint(df_housing.describe().round(2))\n\ncorrelation = df_housing.corr()['Price'].sort_values(ascending=False)\nprint(\"\\n\\nCorrelation Matrix (top correlations with Price):\")\nprint(correlation.head(6))\n\n# Visualization\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\naxes[0, 0].scatter(df_housing['MedInc'], df_housing['Price'], alpha=0.3)\naxes[0, 0].set_xlabel('Median Income')\naxes[0, 0].set_ylabel('House Price')\naxes[0, 0].set_title('Income vs Price')\naxes[0, 0].grid(True, alpha=0.3)\n\naxes[0, 1].hist(df_housing['Price'], bins=50, edgecolor='black', alpha=0.7)\naxes[0, 1].set_xlabel('Price ($)')\naxes[0, 1].set_ylabel('Frequency')\naxes[0, 1].set_title('Price Distribution')\naxes[0, 1].grid(True, alpha=0.3)\n\n# Box plot\ndf_housing['AgeGroup'] = pd.cut(df_housing['HouseAge'], bins=5)\ndf_housing.boxplot(column='Price', by='AgeGroup', ax=axes[1, 0])\naxes[1, 0].set_xlabel('House Age Group')\naxes[1, 0].set_ylabel('Price ($)')\naxes[1, 0].set_title('Price by Age Group')\naxes[1, 0].tick_params(axis='x', rotation=45)\naxes[1, 0].grid(True, alpha=0.3)\n\n# Correlation heatmap\ncorr_matrix = df_housing.corr()\nim = axes[1, 1].imshow(corr_matrix, cmap='coolwarm', aspect='auto')\naxes[1, 1].set_title('Correlation Heatmap')\nplt.colorbar(im, ax=axes[1, 1])\n\nplt.tight_layout()\nplt.savefig('eda_visualizations.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"\\nEDA Complete! Visualizations saved as 'eda_visualizations.png'\")\n\n# Project 2: Predictive Modeling\nprint(\"\\n\" + \"=\" * 60)\nprint(\"PROJECT 2: PREDICTIVE MODELING - House Price Prediction\")\nprint(\"=\" * 60)\n\nX = df_housing.drop(['Price', 'AgeGroup'], axis=1)\ny = df_housing['Price']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"\\nTraining set size: {X_train.shape[0]} samples\")\nprint(f\"Test set size: {X_test.shape[0]} samples\")\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"\\nModel Performance:\")\nprint(f\"  Mean Squared Error (MSE): ${mse:,.2f}\")\nprint(f\"  Root Mean Squared Error (RMSE): ${rmse:,.2f}\")\nprint(f\"  R-squared Score: {r2:.3f}\")\n\n# Predictions vs actual plot\nplt.figure(figsize=(8, 6))\nplt.scatter(y_test, y_pred, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel('Actual Price ($)')\nplt.ylabel('Predicted Price ($)')\nplt.title('Actual vs Predicted House Prices')\nplt.grid(True, alpha=0.3)\nplt.text(0.05, 0.95, f'RÂ² = {r2:.3f}', transform=plt.gca().transAxes, \n         fontsize=12, verticalalignment='top',\n         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\nplt.tight_layout()\nplt.savefig('predictions_vs_actual.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"\\nPrediction visualization saved as 'predictions_vs_actual.png'\")\n\n# Project 3: Dashboard\nprint(\"\\n\" + \"=\" * 60)\nprint(\"PROJECT 3: INTERACTIVE DASHBOARD\")\nprint(\"=\" * 60)\n\nsummary_stats = {\n    'Total Houses': len(df_housing),\n    'Average Price': f\"${df_housing['Price'].mean():,.0f}\",\n    'Median Price': f\"${df_housing['Price'].median():,.0f}\",\n    'Price Range': f\"${df_housing['Price'].min():,.0f} - ${df_housing['Price'].max():,.0f}\",\n    'Model RMSE': f\"${rmse:,.0f}\",\n    'Model R-squared': f\"{r2:.3f}\"\n}\n\nprint(\"Key Dashboard Metrics:\")\nfor key, value in summary_stats.items():\n    print(f\"  {key:20s}: {value}\")\n\nfeature_importance = pd.DataFrame({\n    'feature': X.columns,\n    'coefficient': abs(model.coef_),\n    'impact': model.coef_\n}).sort_values('coefficient', ascending=False)\n\nprint(\"\\n\\nTop 5 Most Important Features:\")\nfor i, row in feature_importance.head().iterrows():\n    direction = \"increases\" if row['impact'] > 0 else \"decreases\"\n    print(f\"  {row['feature']:15s}: ${abs(row['coefficient']):,.2f} per unit ({direction} price)\")\n\n# Dashboard visualizations\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\naxes[0, 0].hist(df_housing['Price'], bins=50, color='skyblue', edgecolor='black', alpha=0.7)\naxes[0, 0].axvline(df_housing['Price'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ${df_housing[\"Price\"].mean():,.0f}')\naxes[0, 0].axvline(df_housing['Price'].median(), color='green', linestyle='--', linewidth=2, label=f'Median: ${df_housing[\"Price\"].median():,.0f}')\naxes[0, 0].set_xlabel('Price ($)')\naxes[0, 0].set_ylabel('Frequency')\naxes[0, 0].set_title('House Price Distribution')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\nfeatures = feature_importance['feature'].head(8)\nimportance = feature_importance['coefficient'].head(8)\ncolors = ['green' if x > 0 else 'red' for x in feature_importance['impact'].head(8)]\naxes[0, 1].barh(features, importance, color=colors, alpha=0.7)\naxes[0, 1].set_xlabel('Absolute Coefficient Value')\naxes[0, 1].set_title('Feature Importance in Price Prediction')\naxes[0, 1].grid(True, alpha=0.3, axis='x')\n\naxes[1, 0].scatter(y_test, y_pred, alpha=0.5, s=20)\naxes[1, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\naxes[1, 0].set_xlabel('Actual Price ($)')\naxes[1, 0].set_ylabel('Predicted Price ($)')\naxes[1, 0].set_title('Model Predictions vs Actual')\naxes[1, 0].grid(True, alpha=0.3)\naxes[1, 0].text(0.05, 0.95, f'RÂ² = {r2:.3f}\\nRMSE = ${rmse:,.0f}', \n                transform=axes[1, 0].transAxes, fontsize=10,\n                verticalalignment='top',\n                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\nresiduals = y_test - y_pred\naxes[1, 1].scatter(y_pred, residuals, alpha=0.5, s=20)\naxes[1, 1].axhline(y=0, color='r', linestyle='--', linewidth=2)\naxes[1, 1].set_xlabel('Predicted Price ($)')\naxes[1, 1].set_ylabel('Residuals ($)')\naxes[1, 1].set_title('Residuals Analysis')\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('dashboard_summary.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"PORTFOLIO PROJECTS SUMMARY\")\nprint(\"=\" * 60)\nprint(f\"\"\"\nThree portfolio projects demonstrated:\n\n1. EXPLORATORY DATA ANALYSIS\n   - Loaded and explored California housing dataset\n   - Created summary statistics and visualizations\n   - Identified key correlations and patterns\n\n2. PREDICTIVE MODELING\n   - Built linear regression model for price prediction\n   - Achieved RÂ² score of {r2:.3f}\n   - RMSE of ${rmse:,.0f} on test data\n\n3. INTERACTIVE DASHBOARD\n   - Created comprehensive visualizations\n   - Displayed key metrics and feature importance\n   - Showed model performance and residual analysis\n\"\"\")\n\ndf_housing.to_csv('california_housing_analysis.csv', index=False)\nfeature_importance.to_csv('feature_importance.csv', index=False)\n\nprint(\"\\nData files saved for portfolio:\")\nprint(\"  - california_housing_analysis.csv\")\nprint(\"  - feature_importance.csv\")\nprint(\"  - eda_visualizations.png\")\nprint(\"  - predictions_vs_actual.png\")\nprint(\"  - dashboard_summary.png\")\nprint(\"\")\nprint(\"Portfolio project demonstration complete!\")\nFULL_CODE_END -->\n\n## Resources\n- ğŸ“š **Courses:** Coursera, DataCamp, Udacity\n- ğŸ’» **Practice:** Kaggle, LeetCode\n- ğŸ“– **Books:** \"Python for Data Analysis\", \"Hands-On ML\"\n- ğŸ‘¥ **Community:** Reddit, LinkedIn, Meetups"
        },
        {
            "id": "ds_intro_12",
            "type": "content",
            "title": "Summary",
            "content": "# Congratulations! ğŸ‰\n\nYou've completed the Introduction to Data Science! Here's what you learned:\n\n## Key Takeaways\n\n### What is Data Science\n- âœ… Interdisciplinary field combining math, CS, and domain expertise\n- âœ… Four types of analytics: Descriptive, Diagnostic, Predictive, Prescriptive\n\n### Data Science Process\n- âœ… CRISP-DM framework with 6 phases\n- âœ… Iterative and business-focused approach\n- âœ… Data preparation is 60-80% of the work\n\n### Applications\n- âœ… Healthcare, Finance, Retail, Transportation, Entertainment\n- âœ… Solving real-world problems at scale\n\n### Tools Ecosystem\n- âœ… Python as the primary language\n- âœ… Libraries: Pandas, NumPy, Scikit-learn\n- âœ… Cloud platforms and notebooks\n\n### Team Roles\n- âœ… Data Scientist, Analyst, Engineer, ML Engineer\n- âœ… Emerging roles: Analytics Engineer, MLOps\n\n> ğŸ¯ **Remember:** Data Science is about solving problems, not just using fancy algorithms!\n\n## Next Steps\n- ğŸ“Š Learn **Basic Statistics** to build your foundation\n- ğŸ Master **Python** for data manipulation\n- ğŸ“ˆ Practice with real **datasets** on Kaggle\n\nKeep learning and happy data exploring! ğŸš€"
        }
    ]
}
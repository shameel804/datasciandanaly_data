{
    "id": "learn_data_lifecycle",
    "topicId": "data_lifecycle",
    "topicTitle": "Data Science Lifecycle",
    "description": "Master the complete Data Science lifecycle from data collection through deployment and monitoring",
    "baseKP": 75,
    "slides": [
        {
            "id": "data_lifecycle_1",
            "type": "content",
            "title": "Welcome to the Data Science Lifecycle",
            "content": "# Data Science Lifecycle ğŸ”„\n\nEvery successful Data Science project follows a structured lifecycle.\n\n## What you'll learn:\n- **Data Collection** - Gathering data from various sources\n- **Data Preparation** - Cleaning and transforming data\n- **Exploratory Analysis** - Understanding your data deeply\n- **Model Building** - Creating predictive models\n- **Deployment** - Putting models into production\n- **Monitoring** - Ensuring ongoing performance\n\n> ğŸ’¡ **Key Insight:** The lifecycle is not linear - you'll iterate between stages!\n\n## Why This Matters\n\n| Without Lifecycle | With Lifecycle |\n|-------------------|----------------|\n| Ad-hoc analysis | Systematic approach |\n| Inconsistent results | Reproducible outcomes |\n| Scope creep | Clear milestones |\n\nLet's explore each stage!"
        },
        {
            "id": "data_lifecycle_2",
            "type": "content",
            "title": "Stage 1: Data Collection",
            "content": "# Data Collection ğŸ“¥\n\nThe first stage is gathering the data you need.\n\n## Data Sources\n\n### Internal Sources\n- **Databases** - SQL, NoSQL systems\n- **Data Warehouses** - Historical business data\n- **Logs** - Application and server logs\n\n### External Sources\n- **APIs** - Web services\n- **Web Scraping** - Public website data\n- **Third-party Data** - Purchased datasets\n\n## Collection Example\n\n```python\nimport pandas as pd\nimport requests\n\n# From database\ndf_db = pd.read_sql('SELECT * FROM customers', engine)\n\n# From API\nresponse = requests.get('https://api.example.com/data')\ndf_api = pd.DataFrame(response.json())\n\n# From CSV\ndf_file = pd.read_csv('sales_data.csv')\n```\n\n## Key Considerations\n- âœ… Data availability and access rights\n- âœ… Privacy and compliance (GDPR, HIPAA)\n- âœ… Data freshness requirements"
        },
        {
            "id": "data_lifecycle_3",
            "type": "content",
            "title": "Stage 2: Data Preparation",
            "content": "# Data Preparation ğŸ§¹\n\nTransforms raw data into a clean, usable format.\n\n> ğŸ“Š Data Scientists spend **60-80% of their time** on data preparation!\n\n## Key Activities\n\n### 1. Data Cleaning\n```python\n# Handle missing values\ndf['age'].fillna(df['age'].median(), inplace=True)\n\n# Remove duplicates\ndf = df.drop_duplicates()\n\n# Fix inconsistent values\ndf['country'] = df['country'].str.strip().str.title()\n```\n\n### 2. Data Transformation\n```python\n# Type conversion\ndf['date'] = pd.to_datetime(df['date'])\n\n# Create new features\ndf['year'] = df['date'].dt.year\n\n# Scaling\nfrom sklearn.preprocessing import StandardScaler\ndf['amount_scaled'] = scaler.fit_transform(df[['amount']])\n```\n\n## Common Issues\n- âŒ Missing values\n- âŒ Outliers\n- âŒ Inconsistent formats\n- âŒ Duplicate records"
        },
        {
            "id": "data_lifecycle_quiz_1",
            "type": "quiz",
            "title": "Preparation Check",
            "content": "Test your understanding!",
            "quizQuestion": "What percentage of time do Data Scientists typically spend on data preparation?",
            "quizOptions": [
                "20-30%",
                "40-50%",
                "60-80%",
                "90-100%"
            ],
            "correctOptionIndex": 2
        },
        {
            "id": "data_lifecycle_4",
            "type": "content",
            "title": "Stage 3: Exploratory Data Analysis",
            "content": "# Exploratory Data Analysis ğŸ”\n\nEDA is about understanding your data before modeling.\n\n## Goals of EDA\n- ğŸ“Š Understand data distributions\n- ğŸ”— Find relationships between variables\n- ğŸš¨ Detect anomalies and outliers\n- ğŸ’¡ Generate hypotheses\n\n## Statistical Summary\n\n```python\n# Basic statistics\nprint(df.describe())\n\n# Correlation matrix\ncorr_matrix = df.corr()\n```\n\n## Visualization Techniques\n\n```python\nimport seaborn as sns\n\n# Distribution\nsns.histplot(df['sales'], kde=True)\n\n# Relationship\nsns.scatterplot(x='advertising', y='sales', data=df)\n\n# Comparison\nsns.boxplot(x='region', y='sales', data=df)\n\n# Correlation heatmap\nsns.heatmap(df.corr(), annot=True)\n```\n\n## EDA Checklist\n- [ ] Univariate analysis\n- [ ] Bivariate analysis\n- [ ] Check for multicollinearity\n- [ ] Identify outliers"
        },
        {
            "id": "data_lifecycle_5",
            "type": "content",
            "title": "Stage 4: Model Building",
            "content": "# Model Building ğŸ¤–\n\nCreate algorithms to solve your problem.\n\n## Problem Type Matrix\n\n| Problem | Target | Algorithms |\n|---------|--------|------------|\n| Classification | Categorical | Logistic Regression, Random Forest |\n| Regression | Numerical | Linear Regression, XGBoost |\n| Clustering | None | K-Means, DBSCAN |\n\n## Modeling Workflow\n\n```python\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\n# 1. Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42)\n\n# 2. Train model\nmodel = RandomForestClassifier(n_estimators=100)\nmodel.fit(X_train, y_train)\n\n# 3. Predict\ny_pred = model.predict(X_test)\n\n# 4. Evaluate\nfrom sklearn.metrics import accuracy_score\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n```\n\n## Key Activities\n- Model selection\n- Hyperparameter tuning\n- Cross-validation\n- Performance evaluation"
        },
        {
            "id": "data_lifecycle_6",
            "type": "content",
            "title": "Model Validation",
            "content": "# Model Validation âœ…\n\nProper validation ensures your model generalizes well.\n\n## Cross-Validation\n\n```python\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(model, X, y, cv=5)\nprint(f\"Mean CV Score: {scores.mean():.2f} (+/- {scores.std()*2:.2f})\")\n```\n\n## Classification Metrics\n\n| Metric | Use Case |\n|--------|----------|\n| **Accuracy** | Balanced classes |\n| **Precision** | Minimize false positives |\n| **Recall** | Minimize false negatives |\n| **F1-Score** | Balance precision/recall |\n| **AUC-ROC** | Overall performance |\n\n## Regression Metrics\n\n```python\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nprint(f\"RMSE: {mean_squared_error(y_test, y_pred, squared=False):.2f}\")\nprint(f\"RÂ²: {r2_score(y_test, y_pred):.2f}\")\n```\n\n## Common Pitfalls\n- âš ï¸ Data leakage\n- âš ï¸ Overfitting\n- âš ï¸ Underfitting\n- âš ï¸ Selection bias"
        },
        {
            "id": "data_lifecycle_quiz_2",
            "type": "quiz",
            "title": "Model Quiz",
            "content": "Test your modeling knowledge!",
            "quizQuestion": "Which metric should you prioritize when you need to minimize false negatives?",
            "quizOptions": [
                "Accuracy",
                "Precision",
                "Recall",
                "F1-Score"
            ],
            "correctOptionIndex": 2
        },
        {
            "id": "data_lifecycle_7",
            "type": "content",
            "title": "Stage 5: Deployment",
            "content": "# Model Deployment ğŸš€\n\nDeployment puts your model into production.\n\n## Deployment Options\n\n### 1. API Deployment\n```python\nfrom flask import Flask, request, jsonify\nimport joblib\n\napp = Flask(__name__)\nmodel = joblib.load('model.pkl')\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    data = request.json\n    prediction = model.predict([data['features']])\n    return jsonify({'prediction': prediction.tolist()})\n```\n\n### 2. Batch Prediction\n- Scheduled jobs (daily, weekly)\n- Full dataset predictions\n\n### 3. Embedded Deployment\n- Model in application\n- Edge devices (mobile, IoT)\n\n## Model Serialization\n\n```python\nimport joblib\n\n# Save model\njoblib.dump(model, 'model.joblib')\n\n# Load model\nloaded_model = joblib.load('model.joblib')\n```\n\n## Deployment Checklist\n- [ ] Model serialized and versioned\n- [ ] API endpoints tested\n- [ ] Error handling implemented\n- [ ] Documentation complete"
        },
        {
            "id": "data_lifecycle_8",
            "type": "content",
            "title": "Stage 6: Monitoring",
            "content": "# Monitoring & Maintenance ğŸ“Š\n\nModels degrade over time - continuous monitoring is essential.\n\n## Why Models Degrade\n- ğŸ“‰ **Data drift** - Input data changes over time\n- ğŸ“‰ **Concept drift** - Feature-target relationship changes\n- ğŸ“‰ **Feature drift** - Individual features behave differently\n\n## Monitoring Metrics\n\n```python\ndef monitor_predictions(predictions):\n    stats = {\n        'mean': predictions.mean(),\n        'std': predictions.std(),\n        'null_pct': predictions.isnull().sum() / len(predictions)\n    }\n    return stats\n```\n\n## Monitoring Dashboard\n\n| Metric | Current | Baseline | Status |\n|--------|---------|----------|--------|\n| Accuracy | 0.85 | 0.88 | âš ï¸ |\n| Latency | 45ms | 50ms | âœ… |\n| Error rate | 0.5% | 0.3% | âš ï¸ |\n\n## Retraining Strategy\n- **Scheduled** - Weekly, monthly\n- **Triggered** - When drift exceeds threshold\n\n> ğŸ’¡ Set up automated alerts for performance degradation!"
        },
        {
            "id": "data_lifecycle_9",
            "type": "content",
            "title": "Lifecycle Integration",
            "content": "# Bringing It All Together ğŸ”—\n\n## The Complete Lifecycle\n\n```\nCollection â†’ Preparation â†’ EDA â†’ Modeling\n     â†‘                              â†“\n     â””â”€â”€â”€â”€ Monitoring â†â”€â”€ Deployment\n```\n\n## MLOps: Operationalizing Data Science\n\n| Component | Purpose | Tools |\n|-----------|---------|-------|\n| Version Control | Track code/data | Git, DVC |\n| Experiment Tracking | Log experiments | MLflow |\n| CI/CD | Automate deployment | GitHub Actions |\n| Containerization | Reproducibility | Docker |\n| Monitoring | Track production | Prometheus |\n\n## Success Metrics by Stage\n\n| Stage | Success Indicator |\n|-------|-------------------|\n| Collection | Complete data gathered |\n| Preparation | Clean dataset ready |\n| EDA | Clear insights gained |\n| Modeling | Performance criteria met |\n| Deployment | Model serving in production |\n| Monitoring | Sustained performance |\n\n> ğŸ¯ The lifecycle is circular - monitoring feeds back into new collection!"
        },
        {
            "id": "data_lifecycle_quiz_3",
            "type": "quiz",
            "title": "Lifecycle Quiz",
            "content": "Final check on the lifecycle!",
            "quizQuestion": "What causes 'concept drift' in a deployed model?",
            "quizOptions": [
                "The model file gets corrupted",
                "The server runs out of memory",
                "The relationship between features and target changes",
                "The API endpoint changes"
            ],
            "correctOptionIndex": 2
        },
        {
            "id": "data_lifecycle_10",
            "type": "content",
            "title": "Summary",
            "content": "# Congratulations! ğŸ‰\n\nYou've mastered the Data Science Lifecycle!\n\n## The Six Stages\n\n### 1. Data Collection ğŸ“¥\n- Gather data from internal/external sources\n\n### 2. Data Preparation ğŸ§¹\n- Clean and transform (60-80% of time)\n\n### 3. Exploratory Data Analysis ğŸ”\n- Understand distributions and relationships\n\n### 4. Model Building ğŸ¤–\n- Select, train, and validate models\n\n### 5. Deployment ğŸš€\n- Put models into production\n\n### 6. Monitoring ğŸ“Š\n- Track and maintain performance\n\n## Key Takeaways\n- âœ… The lifecycle is **iterative**, not linear\n- âœ… **Data quality** determines model quality\n- âœ… **Monitoring** is as important as building\n- âœ… **Documentation** throughout is essential\n\n> ğŸš€ **Next Steps:** Dive deeper into each stage!\n\nKeep building and deploying! ğŸ¯"
        }
    ]
}
[
    {
        "q": "Which stage of the Data Science Lifecycle comes first?",
        "type": "mcq",
        "o": [
            "Data Collection",
            "Model Building",
            "Deployment",
            "Monitoring"
        ]
    },
    {
        "q": "The main goal of the Data Preparation stage is to:",
        "type": "mcq",
        "o": [
            "Clean and transform raw data into a usable format",
            "Build predictive models",
            "Collect data from APIs",
            "Visualize final results"
        ]
    },
    {
        "q": "Exploratory Data Analysis (EDA) primarily happens during which phase?",
        "type": "mcq",
        "o": [
            "Exploratory Analysis",
            "Data Collection",
            "Model Building",
            "Deployment"
        ]
    },
    {
        "q": "In which stage do we train machine learning models?",
        "type": "mcq",
        "o": [
            "Model Building",
            "Data Preparation",
            "Monitoring",
            "Data Collection"
        ]
    },
    {
        "q": "Deploying a trained model into production typically occurs in the ______ stage.",
        "type": "fill_blank",
        "answers": ["Deployment"],
        "other_options": ["Monitoring", "Model Building", "Preparation"]
    },
    {
        "q": "After a model is deployed, we continuously track its performance in the ______ phase.",
        "type": "fill_blank",
        "answers": ["Monitoring"],
        "other_options": ["Deployment", "Collection", "Analysis"]
    },
    {
        "q": "Web scraping and using public APIs are common techniques in which stage?",
        "type": "mcq",
        "o": [
            "Data Collection",
            "Data Preparation",
            "Exploratory Analysis",
            "Deployment"
        ]
    },
    {
        "q": "Handling missing values and removing duplicates is part of:",
        "type": "mcq",
        "o": [
            "Data Preparation",
            "Data Collection",
            "Model Building",
            "Monitoring"
        ]
    },
    {
        "q": "Creating correlation heatmaps and distribution plots is a key activity in:",
        "type": "mcq",
        "o": [
            "Exploratory Analysis",
            "Data Preparation",
            "Deployment",
            "Monitoring"
        ]
    },
    {
        "q": "Model deployment can be done using tools like Flask, FastAPI, or cloud services such as AWS SageMaker.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Monitoring is only needed during the first week after deployment.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the Data Science Lifecycle stage with its primary purpose:",
        "type": "match",
        "left": ["Data Collection", "Data Preparation", "Exploratory Analysis", "Model Building"],
        "right": ["Gather raw data from various sources", "Clean, transform, and organize data", "Understand patterns and relationships", "Train and evaluate predictive models"]
    },
    {
        "q": "Rearrange these stages to form the correct sequential order of a typical Data Science Lifecycle:",
        "type": "rearrange",
        "words": ["Data Collection", "Data Preparation", "Exploratory Analysis", "Model Building", "Deployment", "Monitoring"]
    },
    {
        "q": "Concept drift and data drift are primarily detected and addressed in which stage?",
        "type": "mcq",
        "o": [
            "Monitoring",
            "Deployment",
            "Data Collection",
            "Exploratory Analysis"
        ]
    },
    {
        "q": "Feature engineering is mainly performed during the ______ stage.",
        "type": "fill_blank",
        "answers": ["Data Preparation"],
        "other_options": ["Collection", "Deployment", "Monitoring"]
    },
    {
        "q": "Which of the following is NOT a common source used during the Data Collection stage?",
        "type": "mcq",
        "o": [
            "Trained machine learning model",
            "Relational databases",
            "Public APIs",
            "CSV files from sensors"
        ]
    },
    {
        "q": "In the Data Preparation stage, converting categorical variables into numerical format is known as:",
        "type": "mcq",
        "o": [
            "Encoding",
            "Scaling",
            "Imputation",
            "Normalization"
        ]
    },
    {
        "q": "Which plot is most commonly used in Exploratory Analysis to detect outliers in a single variable?",
        "type": "mcq",
        "o": [
            "Box plot",
            "Pie chart",
            "Line chart",
            "Bar chart"
        ]
    },
    {
        "q": "During Model Building, splitting data into training and test sets helps prevent:",
        "type": "mcq",
        "o": [
            "Overfitting",
            "Data leakage",
            "Under-sampling",
            "Feature scaling"
        ]
    },
    {
        "q": "A model served as a REST API endpoint using Docker is an example of the ______ stage.",
        "type": "fill_blank",
        "answers": ["Deployment"],
        "other_options": ["Monitoring", "Preparation", "Collection"]
    },
    {
        "q": "Model drift that occurs when the statistical properties of the target variable change over time is called:",
        "type": "mcq",
        "o": [
            "Concept drift",
            "Data drift",
            "Prediction drift",
            "Feature drift"
        ]
    },
    {
        "q": "Imputing missing values with the median is a technique typically used in the ______ stage.",
        "type": "fill_blank",
        "answers": ["Data Preparation"],
        "other_options": ["Exploratory Analysis", "Deployment", "Monitoring"]
    },
    {
        "q": "Calculating summary statistics like mean, median, and standard deviation is part of:",
        "type": "mcq",
        "o": [
            "Exploratory Analysis",
            "Model Building",
            "Data Collection",
            "Deployment"
        ]
    },
    {
        "q": "Batch prediction and real-time prediction are two common paradigms in which stage?",
        "type": "mcq",
        "o": [
            "Deployment",
            "Monitoring",
            "Model Building",
            "Data Preparation"
        ]
    },
    {
        "q": "Retraining a model when performance drops below a threshold is a task performed in the ______ phase.",
        "type": "fill_blank",
        "answers": ["Monitoring"],
        "other_options": ["Deployment", "Collection", "Analysis"]
    },
    {
        "q": "It is acceptable to perform feature selection after deploying the model in production.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Data versioning tools like DVC or MLflow Data are most useful during the Data Preparation stage.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match each tool with the Data Science Lifecycle stage where it is primarily used:",
        "type": "match",
        "left": ["BeautifulSoup", "pandas", "seaborn", "scikit-learn", "MLflow", "Prometheus"],
        "right": ["Data Collection", "Data Preparation", "Exploratory Analysis", "Model Building", "Deployment", "Monitoring"]
    },
    {
        "q": "Rearrange these activities into the correct lifecycle stage order (earliest to latest):",
        "type": "rearrange",
        "words": ["Handle missing values", "Create pair plots", "Train-test split", "Containerize with Docker", "Set up performance alerts"]
    },
    {
        "q": "Which of the following is a key reason to perform train-validation-test split during Model Building?",
        "type": "mcq",
        "o": [
            "To tune hyperparameters without touching the final test set",
            "To reduce training time",
            "To increase dataset size",
            "To remove duplicate records"
        ]
    },
    {
        "q": "Sampling bias is most likely introduced during which stage of the Data Science Lifecycle?",
        "type": "mcq",
        "o": [
            "Data Collection",
            "Data Preparation",
            "Exploratory Analysis",
            "Model Building"
        ]
    },
    {
        "q": "The process of combining multiple datasets using keys (e.g., user_id) is called:",
        "type": "mcq",
        "o": [
            "Joining/Merging",
            "Concatenation",
            "Pivoting",
            "Resampling"
        ]
    },
    {
        "q": "A scatter plot matrix (pair plot) is primarily created during:",
        "type": "mcq",
        "o": [
            "Exploratory Analysis",
            "Data Preparation",
            "Deployment",
            "Monitoring"
        ]
    },
    {
        "q": "Cross-validation is a technique primarily used in which stage to estimate model performance?",
        "type": "mcq",
        "o": [
            "Model Building",
            "Deployment",
            "Monitoring",
            "Data Collection"
        ]
    },
    {
        "q": "Shadow deployment and canary release are advanced strategies used in the ______ stage.",
        "type": "fill_blank",
        "answers": ["Deployment"],
        "other_options": ["Monitoring", "Preparation", "Collection"]
    },
    {
        "q": "Tracking input data distribution changes over time is a core activity in:",
        "type": "mcq",
        "o": [
            "Monitoring",
            "Data Preparation",
            "Exploratory Analysis",
            "Model Building"
        ]
    },
    {
        "q": "______ is the practice of creating new features from existing ones, usually done in Data Preparation.",
        "type": "fill_blank",
        "answers": ["Feature engineering"],
        "other_options": ["Feature scaling", "Feature selection", "Feature extraction"]
    },
    {
        "q": "The primary goal of A/B testing a deployed model belongs to which stage?",
        "type": "mcq",
        "o": [
            "Monitoring",
            "Deployment",
            "Model Building",
            "Data Collection"
        ]
    },
    {
        "q": "Using tools like Great Expectations or TFDV to validate incoming data happens mainly in:",
        "type": "mcq",
        "o": [
            "Monitoring",
            "Data Preparation",
            "Exploratory Analysis",
            "Deployment"
        ]
    },
    {
        "q": "Model interpretability techniques like SHAP or LIME are most commonly applied right after:",
        "type": "mcq",
        "o": [
            "Model Building",
            "Deployment",
            "Monitoring",
            "Data Collection"
        ]
    },
    {
        "q": "It is safe to skip the Monitoring stage if the model performs well on the test set.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Exploratory Analysis can sometimes reveal that no modeling is actually needed to solve the business problem.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the lifecycle stage with a typical deliverable:",
        "type": "match",
        "left": ["Data Collection", "Data Preparation", "Exploratory Analysis", "Model Building", "Deployment", "Monitoring"],
        "right": ["Cleaned dataset (e.g., final_df.csv)", "EDA notebook with insights", "Trained model pickle file", "REST API endpoint /predict", "Dashboard showing accuracy over time", "Raw JSON logs from sensors"]
    },
    {
        "q": "Rearrange these tasks in the typical order they occur across the lifecycle:",
        "type": "rearrange",
        "words": ["Define API schema", "Detect multicollinearity", "Apply one-hot encoding", "Write data ingestion script", "Set up model registry", "Log prediction latency"]
    },
    {
        "q": "Which stage is most concerned with reproducibility of the entire pipeline?",
        "type": "mcq",
        "o": [
            "All stages",
            "Only Model Building",
            "Only Deployment",
            "Only Monitoring"
        ]
    },
    {
        "q": "A sudden drop in prediction latency after deployment usually signals a problem that should be caught in which stage?",
        "type": "mcq",
        "o": [
            "Monitoring",
            "Deployment",
            "Model Building",
            "Data Collection"
        ]
    },
    {
        "q": "In real-world projects, the Data Science Lifecycle is strictly linear and never requires looping back to earlier stages.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Creating a data dictionary and documenting column meanings is a best practice in the ______ stage.",
        "type": "fill_blank",
        "answers": ["Data Preparation"],
        "other_options": ["Exploratory Analysis", "Monitoring", "Deployment"]
    },
    {
        "q": "Which of the following is an example of real-time model monitoring metric?",
        "type": "mcq",
        "o": [
            "Prediction response time",
            "Training loss curve",
            "Cross-validation score",
            "Feature importance ranking"
        ]
    },
    {
        "q": "Using target encoding on a categorical feature before train-test split can cause:",
        "type": "mcq",
        "o": [
            "Data leakage",
            "Underfitting",
            "Faster training",
            "Reduced memory usage"
        ]
    },
    {
        "q": "The stage where business stakeholders usually review initial visualizations and decide whether to continue the project is typically:",
        "type": "mcq",
        "o": [
            "Exploratory Analysis",
            "Data Collection",
            "Model Building",
            "Monitoring"
        ]
    },
    {
        "q": "______ models are often deployed as serverless functions for low-traffic use cases.",
        "type": "fill_blank",
        "answers": ["Lightweight"],
        "other_options": ["Complex", "Untrained", "Overfitted"]
    },
    {
        "q": "Reproducing the exact same EDA results six months later depends heavily on:",
        "type": "mcq",
        "o": [
            "Data and code versioning",
            "Model accuracy",
            "Cloud provider choice",
            "Team size"
        ]
    },
    {
        "q": "Match the challenge with the lifecycle stage where it is most commonly addressed:",
        "type": "match",
        "left": ["Class imbalance", "API rate limiting", "High-dimensional data", "Model staleness"],
        "right": ["Data Preparation", "Data Collection", "Model Building", "Monitoring"]
    },
    {
        "q": "Rearrange these actions in the order they would appear in a responsible ML pipeline:",
        "type": "rearrange",
        "words": ["Check for bias in predictions", "Scale features using training statistics only", "Log raw input features alongside predictions", "Perform train-val-test split", "Write unit tests for preprocessing function"]
    },
    {
        "q": "Which stage benefits most from automated data quality checks running every time new data arrives?",
        "type": "mcq",
        "o": [
            "Monitoring",
            "Deployment",
            "Model Building",
            "Exploratory Analysis"
        ]
    },
    {
        "q": "Saving the exact random_state value used for train-test split is important for:",
        "type": "mcq",
        "o": [
            "Reproducibility",
            "Faster computation",
            "Reducing bias",
            "Model compression"
        ]
    },
    {
        "q": "A model that worked perfectly in staging but fails in production most likely suffered from:",
        "type": "mcq",
        "o": [
            "Environment or data mismatch",
            "Insufficient hyperparameters",
            "Too many features",
            "Wrong programming language"
        ]
    },
    {
        "q": "Exploratory Analysis should always be performed on the full dataset, including the test set.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Pipeline orchestration tools like Airflow or Kubeflow are primarily used to automate transitions between ______ stages.",
        "type": "fill_blank",
        "answers": ["multiple"],
        "other_options": ["single", "only two", "none"]
    },
    {
        "q": "When a model starts predicting the majority class for almost every input after deployment, which stage should trigger an alert first?",
        "type": "mcq",
        "o": [
            "Monitoring",
            "Model Building",
            "Data Preparation",
            "Exploratory Analysis"
        ]
    },
    {
        "q": "In a production pipeline, the step that transforms incoming JSON payload exactly the same way as during training belongs to the ______ stage.",
        "type": "fill_blank",
        "answers": ["Deployment"],
        "other_options": ["Monitoring", "Collection", "Preparation"]
    },
    {
        "q": "Which of these is considered a 'silent failure' that only the Monitoring stage can reliably detect?",
        "type": "mcq",
        "o": [
            "Gradual shift in input feature distribution",
            "Syntax error in training script",
            "Missing column during one-time EDA",
            "Hard drive crash during model saving"
        ]
    },
    {
        "q": "A data scientist spends two weeks collecting data from 47 different internal microservices. This intensive effort belongs primarily to:",
        "type": "mcq",
        "o": [
            "Data Collection",
            "Data Preparation",
            "Deployment",
            "Monitoring"
        ]
    },
    {
        "q": "The CRISP-DM framework treats the Data Science Lifecycle as ______ rather than strictly sequential.",
        "type": "fill_blank",
        "answers": ["iterative"],
        "other_options": ["linear", "random", "parallel"]
    },
    {
        "q": "Using a model in production without saving the exact preprocessing pipeline that created the training features is a common cause of:",
        "type": "mcq",
        "o": [
            "Training-serving skew",
            "Overfitting",
            "Class imbalance",
            "Missing values"
        ]
    },
    {
        "q": "Stakeholders rejecting a highly accurate model because it uses sensitive demographic features usually happens after reviewing results from:",
        "type": "mcq",
        "o": [
            "Model Building",
            "Deployment",
            "Data Collection",
            "Monitoring"
        ]
    },
    {
        "q": "The practice of 'golden dataset' maintenance for continuous evaluation is most critical in the ______ stage.",
        "type": "fill_blank",
        "answers": ["Monitoring"],
        "other_options": ["Deployment", "Collection", "Preparation"]
    },
    {
        "q": "Model cards and data sheets are documentation artifacts typically created toward the end of which stage?",
        "type": "mcq",
        "o": [
            "Model Building",
            "Exploratory Analysis",
            "Data Collection",
            "Data Preparation"
        ]
    },
    {
        "q": "It is considered acceptable to tune hyperparameters directly on the final test set if time is very limited.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Data lineage tracking becomes essential when regulations require proving where every prediction’s input data originated.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the term with the stage where it is most relevant:",
        "type": "match",
        "left": ["Schema drift", "Hyperparameter tuning", "Cold start problem", "Ground truth delay"],
        "right": ["Monitoring", "Model Building", "Deployment", "Monitoring"]
    },
    {
        "q": "Rearrange these real-world events in the most common chronological order after initial deployment:",
        "type": "rearrange",
        "words": ["First accuracy drop detected", "Canary traffic increased to 100%", "Initial 5% canary rollout", "Retraining triggered automatically", "Post-retrain model promoted to production"]
    },
    {
        "q": "Which stage directly benefits from setting up automated regression tests for the prediction endpoint?",
        "type": "mcq",
        "o": [
            "Deployment",
            "Monitoring",
            "Model Building",
            "Data Collection"
        ]
    },
    {
        "q": "An unexpected spike in NULL values in a feature that was clean during training would first be flagged by:",
        "type": "mcq",
        "o": [
            "Production data validation in Monitoring",
            "One-time missing value imputation",
            "Exploratory box plots",
            "Train-test split logic"
        ]
    },
    {
        "q": "The main purpose of ______ is to turn the trained model into a service that other systems can call.",
        "type": "fill_blank",
        "answers": ["Deployment"],
        "other_options": ["Monitoring", "Exploratory Analysis", "Model Building"]
    },
    {
        "q": "______ drift occurs when the relationship between input features and the target variable changes over time.",
        "type": "fill_blank",
        "answers": ["Concept"],
        "other_options": ["Data", "Prediction", "Feature"]
    },
    {
        "q": "In production, ______ is the difference between metrics measured offline on test data and the same metrics observed on live traffic.",
        "type": "fill_blank",
        "answers": ["training-serving skew", "Training-serving skew"],
        "other_options": ["concept drift", "data drift", "overfitting"]
    },
    {
        "q": "Rearrange these Data Preparation steps in the safest order to avoid data leakage:",
        "type": "rearrange",
        "words": ["Perform train-test split", "Fit StandardScaler on training data only", "Apply the fitted scaler to both train and test", "Impute missing values using training statistics", "One-hot encode categorical columns"]
    },
    {
        "q": "Match the lifecycle stage to the most common file/artifact produced:",
        "type": "match",
        "left": ["Data Collection", "Data Preparation", "Exploratory Analysis", "Model Building", "Deployment", "Monitoring"],
        "right": ["raw_data_2025.parquet", "cleaned_features_v3.csv", "eda_insights.ipynb", "best_model.pkl", "docker_image.tar", "weekly_drift_report.pdf"]
    },
    {
        "q": "______ is the stage where you first calculate baseline metrics using a simple rule-based or majority-class model.",
        "type": "fill_blank",
        "answers": ["Model Building"],
        "other_options": ["Exploratory Analysis", "Deployment", "Monitoring"]
    },
    {
        "q": "A model registry (e.g., MLflow, Vertex AI Model Registry) is primarily used during the transition from ______ to ______.",
        "type": "fill_blank",
        "answers": ["Model Building", "Deployment"],
        "other_options": ["Preparation", "Monitoring", "Collection", "Analysis"]
    },
    {
        "q": "True or False: It is safe to explore and visualize the test set during Exploratory Analysis as long as you don’t train on it.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "True or False: Shadow mode deployment means the new model receives live traffic but its predictions are not shown to users.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match each monitoring concept with its definition:",
        "type": "match",
        "left": ["Data drift", "Concept drift", "Prediction drift", "Ground-truth drift"],
        "right": ["Input feature distributions change", "Target distribution or relationship changes", "Model output distribution changes unexpectedly", "Delay or errors in receiving true labels"]
    },
    {
        "q": "Rearrange these tools/techniques into the stage where they are most commonly applied for the first time:",
        "type": "rearrange",
        "words": ["pandas profiling", "SMOTE oversampling", "GridSearchCV", "FastAPI endpoint", "Evidently AI dashboard"]
    },
    {
        "q": "The practice of storing raw input data together with model predictions for later analysis is known as ______ logging.",
        "type": "fill_blank",
        "answers": ["payload", "Payload"],
        "other_options": ["error", "metric", "trace"]
    },
    {
        "q": "______ testing compares the performance of a new model version against the current champion before full rollout.",
        "type": "fill_blank",
        "answers": ["A/B", "AB", "A/B testing"],
        "other_options": ["unit", "integration", "shadow"]
    },
    {
        "q": "True or False: Feature store is most useful during the Deployment and Monitoring stages to ensure training-serving consistency.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the deployment strategy with its risk level (lowest → highest):",
        "type": "match",
        "left": ["Blue-green deployment", "Canary release", "Direct traffic switch", "Shadow deployment"],
        "right": ["Very low risk (zero downtime)", "Low to medium risk", "High risk", "Zero user impact"]
    },
    {
        "q": "The very last stage of the lifecycle that never truly ends in production systems is ______.",
        "type": "fill_blank",
        "answers": ["Monitoring"],
        "other_options": ["Deployment", "Model Building", "Data Collection"]
    },
    {
        "q": "True or False: Once a model reaches 99% accuracy on held-out data, the Monitoring stage can be skipped to save costs.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these events in a typical model retraining loop triggered by drift detection:",
        "type": "rearrange",
        "words": ["Drift metric exceeds threshold", "Automated retraining job starts", "New model version registered", "Performance comparison against champion", "New version gradually rolled out"]
    },
    {
        "q": "The term 'feedback loop' in production ML most commonly refers to the delay between making a prediction and receiving the ______ label.",
        "type": "fill_blank",
        "answers": ["ground truth", "true"],
        "other_options": ["predicted", "confidence", "feature"]
    },
    {
        "q": "In regulated industries (e.g., finance, healthcare), the ______ stage must produce auditable evidence that no forbidden features were used.",
        "type": "fill_blank",
        "answers": ["Model Building"],
        "other_options": ["Deployment", "Monitoring", "Collection"]
    },
    {
        "q": "______ deployment means running the new model in parallel with the old one and comparing predictions before trusting the new version.",
        "type": "fill_blank",
        "answers": ["Shadow"],
        "other_options": ["Canary", "Blue-green", "Rolling"]
    },
    {
        "q": "True or False: Exploratory Analysis is allowed to modify the original raw data files.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "True or False: Batch inference jobs are typically scheduled during the Monitoring stage to refresh dashboards.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these preprocessing steps in the order they should appear inside a scikit-learn Pipeline object:",
        "type": "rearrange",
        "words": ["SimpleImputer", "StandardScaler", "OneHotEncoder", "SelectKBest", "RandomForestClassifier"]
    },
    {
        "q": "Match the lifecycle pain point with the tool that best mitigates it:",
        "type": "match",
        "left": ["Inconsistent preprocessing between training and serving", "Hard to reproduce experiments", "No visibility into production data quality", "Slow model iteration due to manual retraining"],
        "right": ["Feature Store", "MLflow Tracking", "Evidently / Alibi Detect", "Kubeflow Pipelines / Airflow"]
    },
    {
        "q": "The practice of gradually routing more traffic to a new model version while watching error rates is called ______ release.",
        "type": "fill_blank",
        "answers": ["Canary"],
        "other_options": ["Shadow", "Blue-green", "Recreate"]
    },
    {
        "q": "True or False: Data Collection can sometimes be fully automated using CDC (Change Data Capture) tools from production databases.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: Model Monitoring should only track accuracy and never track business KPIs such as revenue impact.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match each stage to its most typical success metric:",
        "type": "match",
        "left": ["Data Collection", "Data Preparation", "Exploratory Analysis", "Model Building", "Deployment", "Monitoring"],
        "right": ["99.9% uptime of prediction endpoint", "% of rows with missing values < 1%", "Identified 3 strong predictors of churn", "Coverage: 10 million records ingested", "AUC = 0.89 on hold-out set", "Prediction latency P95 < 80ms"]
    },
    {
        "q": "Rearrange these actions when a severe concept drift alert fires at 2 AM:",
        "type": "rearrange",
        "words": ["PagerDuty alert wakes engineer", "Human reviews drift report", "Emergency rollback to previous model", "Root cause analysis begins", "Retraining with latest data scheduled"]
    },
    {
        "q": "The ______ stage is where you decide whether the problem actually requires machine learning or can be solved with simple heuristics.",
        "type": "fill_blank",
        "answers": ["Exploratory Analysis"],
        "other_options": ["Data Collection", "Deployment", "Monitoring"]
    },
    {
        "q": "Saving scaler parameters, encoder mappings, and imputation values alongside the model is a best practice to avoid ______ skew.",
        "type": "fill_blank",
        "answers": ["training-serving", "Training-serving"],
        "other_options": ["concept", "data", "prediction"]
    },
    {
        "q": "True or False: In some companies, the Monitoring stage includes automated compensation payouts when model predictions are proven wrong.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: Data Preparation is the only stage that is allowed to drop entire columns from the dataset.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the deployment tool to its primary strength:",
        "type": "match",
        "left": ["BentoML", "Seldon Core", "TensorFlow Serving", "TorchServe"],
        "right": ["Easy packaging of any Python model", "Native Kubernetes integration & explanations", "High-performance serving for TF models", "Optimized serving for PyTorch models"]
    },
    {
        "q": "The final step before declaring a Data Science project 'complete' in industry is usually handing over ownership to the ______ team during the Monitoring stage.",
        "type": "fill_blank",
        "answers": ["MLOps", "ML Engineering", "Platform"],
        "other_options": ["Data Science", "Analytics", "Business"]
    },
    {
        "q": "A sudden increase in the proportion of 'unknown' category after one-hot encoding in production is usually a sign of ______ in incoming data.",
        "type": "fill_blank",
        "answers": ["new categories", "previously unseen categories"],
        "other_options": ["overfitting", "underfitting", "scaling issue"]
    },
    {
        "q": "______ is the stage where you first ask: 'Do we even have enough signal in the data to solve this problem?'",
        "type": "fill_blank",
        "answers": ["Exploratory Analysis"],
        "other_options": ["Data Collection", "Model Building", "Monitoring"]
    },
    {
        "q": "In a mature MLOps setup, the trigger that automatically starts retraining is usually defined in the ______ stage.",
        "type": "fill_blank",
        "answers": ["Monitoring"],
        "other_options": ["Deployment", "Model Building", "Preparation"]
    },
    {
        "q": "True or False: It is acceptable to use the test set to decide which features to keep during feature selection.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "True or False: Data Collection includes defining the data retention policy and deletion schedule for compliance.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these items in the order they should appear in a production-ready model serving code:",
        "type": "rearrange",
        "words": ["Load preprocessing artifacts", "Validate input schema", "Apply preprocessing", "Run model inference", "Post-process output", "Return JSON response"]
    },
    {
        "q": "Match the term to the stage where it first becomes a major concern:",
        "type": "match",
        "left": ["PII masking", "Label leakage", "Latency SLA", "Explanation decay"],
        "right": ["Data Collection", "Data Preparation", "Deployment", "Monitoring"]
    },
    {
        "q": "The technique of serving two model versions simultaneously and routing users randomly to measure business impact is called ______ testing.",
        "type": "fill_blank",
        "answers": ["A/B", "A/B testing"],
        "other_options": ["shadow", "multi-armed bandit", "canary"]
    },
    {
        "q": "True or False: In batch prediction pipelines, the Deployment stage is replaced by scheduled jobs instead of a live API.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: You should always normalize or standardize features before applying PCA.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the advanced technique to its primary lifecycle stage:",
        "type": "match",
        "left": ["Multi-armed bandit deployment", "Counterfactual logging", "Adversarial validation", "Continuous retraining trigger"],
        "right": ["Deployment", "Monitoring", "Data Preparation", "Monitoring"]
    },
    {
        "q": "Rearrange these Monitoring alerts from most urgent to least urgent for a credit-risk model:",
        "type": "rearrange",
        "words": ["Accuracy dropped 18% overnight", "Average latency increased from 40ms to 180ms", "Input age distribution shifted slightly", "Number of daily requests doubled", "Model version tag missing in logs"]
    },
    {
        "q": "______ is the practice of splitting historical data chronologically instead of randomly when the use case involves time-series forecasting.",
        "type": "fill_blank",
        "answers": ["Time-based split", "temporal split"],
        "other_options": ["stratified split", "random split", "k-fold"]
    },
    {
        "q": "A model that was trained on data labeled by human annotators may suffer from ______ bias if the annotation guidelines changed mid-project.",
        "type": "fill_blank",
        "answers": ["label", "annotation"],
        "other_options": ["selection", "sampling", "confirmation"]
    },
    {
        "q": "True or False: The Deployment stage is responsible for defining rollback procedures in case the new model performs worse.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: Exploratory Analysis should always include stakeholder interviews to confirm that visualized patterns are meaningful.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the cost type to the lifecycle stage where it typically becomes the dominant expense:",
        "type": "match",
        "left": ["GPU training costs", "Data labeling budget", "Cloud serving costs", "Storage of historical predictions"],
        "right": ["Model Building", "Data Collection", "Deployment", "Monitoring"]
    },
    {
        "q": "The very first question a data scientist should ask before starting Model Building is: ______.",
        "type": "fill_blank",
        "answers": ["What is the evaluation metric?", "How will success be measured?"],
        "other_options": ["Which algorithm is best?", "How much data do we have?", "Can we use deep learning?"]
    },
    {
        "q": "During Data Collection, deciding whether to store data in a data lake or a data warehouse is part of ______ planning.",
        "type": "fill_blank",
        "answers": ["infrastructure", "architecture"],
        "other_options": ["model", "visualization", "monitoring"]
    },
    {
        "q": "The phenomenon where a model performs well in the lab but fails when users start gaming the system is called ______ exploitation.",
        "type": "fill_blank",
        "answers": ["adversarial", "gaming"],
        "other_options": ["concept drift", "overfitting", "underfitting"]
    },
    {
        "q": "______ is the stage where you calculate proxy metrics when true labels arrive with weeks of delay.",
        "type": "fill_blank",
        "answers": ["Monitoring"],
        "other_options": ["Model Building", "Deployment", "Exploratory Analysis"]
    },
    {
        "q": "True or False: In most companies, the same person who performed Exploratory Analysis also owns the final production monitoring dashboard.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "True or False: Outlier removal should always be done before the train-test split.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these columns in the order they should be processed in a typical feature store pipeline:",
        "type": "rearrange",
        "words": ["Raw source table", "Batch feature job", "Online low-latency store", "Training dataset join", "Real-time feature lookup at serving"]
    },
    {
        "q": "Match the fairness-related issue to the stage where it is most effectively caught:",
        "type": "match",
        "left": ["Disparate impact from protected attributes", "Historical bias in training labels", "Calibration difference across segments", "Prediction disparity after deployment"],
        "right": ["Model Building", "Data Collection", "Model Building", "Monitoring"]
    },
    {
        "q": "The process of converting a Jupyter notebook into a reproducible pipeline with versioned inputs and outputs is called ______ the analysis.",
        "type": "fill_blank",
        "answers": ["productionizing", "operationalizing"],
        "other_options": ["visualizing", "cleaning", "deploying"]
    },
    {
        "q": "True or False: Edge-case testing of the deployed model with crafted malicious inputs belongs to the Deployment stage.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: You can safely drop duplicate rows during Data Preparation without checking if they represent legitimate repeated events.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the experiment tracking need to its primary stage:",
        "type": "match",
        "left": ["Log dataset version hash", "Log model hyperparameters", "Log serving latency distribution", "Log feature importance changes"],
        "right": ["Data Preparation", "Model Building", "Deployment", "Monitoring"]
    },
    {
        "q": "Rearrange these decisions from the moment a business problem is raised until the model is retired:",
        "type": "rearrange",
        "words": ["Define success metric", "Approve data access", "Sign off on fairness audit", "Schedule model sunset date", "Launch A/B test", "Build first prototype"]
    },
    {
        "q": "______ validation simulates production data distribution using adversarial techniques before final model selection.",
        "type": "fill_blank",
        "answers": ["Adversarial"],
        "other_options": ["Cross", "Time-series", "Stratified"]
    },
    {
        "q": "When a recommendation system starts creating filter bubbles that harm user experience, the root cause is usually detected through long-term ______ monitoring.",
        "type": "fill_blank",
        "answers": ["behavioral", "user behavior"],
        "other_options": ["latency", "accuracy", "throughput"]
    },
    {
        "q": "True or False: Model explainability is optional in Deployment but mandatory in highly regulated industries.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: The Monitoring stage is responsible for triggering data deletion requests under GDPR right-to-be-forgotten rules.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the scaling challenge to the stage where it typically first appears at scale:",
        "type": "match",
        "left": ["10 TB raw data ingestion daily", "Feature computation takes 6 hours", "99.99% prediction uptime required", "Storing 5 years of prediction logs"],
        "right": ["Data Collection", "Data Preparation", "Deployment", "Monitoring"]
    },
    {
        "q": "The final handoff document that describes expected input schema, failure modes, and rollback steps is typically called a ______ Runbook.",
        "type": "fill_blank",
        "answers": ["Model", "Production"],
        "other_options": ["Training", "EDA", "Collection"]
    },
    {
        "q": "The 'cold start' problem in production ML most commonly affects newly registered users during the ______ stage.",
        "type": "fill_blank",
        "answers": ["Deployment"],
        "other_options": ["Monitoring", "Model Building", "Data Collection"]
    },
    {
        "q": "______ is the term for intentionally keeping a portion of labeled data hidden from model training to serve as an unbiased benchmark in Monitoring.",
        "type": "fill_blank",
        "answers": ["held-out ground truth", "gold label set", "shadow labels"],
        "other_options": ["test set", "validation set", "proxy labels"]
    },
    {
        "q": "True or False: In streaming pipelines, Data Collection and Data Preparation often happen in the same microsecond-scale service.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: Correlation does not imply causation is a lesson most painfully learned during the Exploratory Analysis stage.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: You should retrain your model immediately every time a single new labeled example arrives.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these fairness checks in the order they should ideally be performed:",
        "type": "rearrange",
        "words": ["Check training data for historical bias", "Measure demographic parity on validation set", "Run bias audit on shadow traffic", "Monitor protected group performance in production", "Apply mitigation if disparity exceeds threshold"]
    },
    {
        "q": "Match the 'unexpected stakeholder' to the stage where they usually first appear:",
        "type": "match",
        "left": ["Legal & compliance team", "Security team", "Finance / cost center owner", "Customer support team"],
        "right": ["Data Collection", "Deployment", "Monitoring", "Monitoring"]
    },
    {
        "q": "______ modeling is when you deliberately build a simpler, interpretable model just to set a human-understandable baseline during Model Building.",
        "type": "fill_blank",
        "answers": ["Baseline", "Interpretability", "White-box"],
        "other_options": ["Complex", "Deep", "Black-box"]
    },
    {
        "q": "True or False: A model can be fully deprecated and deleted from the model registry during the Monitoring stage.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: Data Preparation is allowed to create synthetic data if the original dataset is too small.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the crisis scenario to the stage that failed most critically:",
        "type": "match",
        "left": ["Model recommends dangerous dosage", "Prediction endpoint returns 500 errors for 3 hours", "Model discriminates against protected group", "Training job runs for 10 days with no checkpoint"],
        "right": ["Model Building", "Deployment", "Monitoring", "Model Building"]
    },
    {
        "q": "Rearrange these items into the typical sections of a Model Card document:",
        "type": "rearrange",
        "words": ["Intended use & limitations", "Training data description", "Evaluation metrics & slices", "Ethical considerations", "Owner & contact", "Version & date"]
    },
    {
        "q": "The practice of routing high-value customers to a more accurate (but expensive) model while serving others with a lighter model is called model ______.",
        "type": "fill_blank",
        "answers": ["cascading", "tiering", "routing"],
        "other_options": ["sharding", "balancing", "splitting"]
    },
    {
        "q": "______ is when the same raw data passes through two completely different preprocessing paths (one for training, one for serving), causing bugs.",
        "type": "fill_blank",
        "answers": ["Pipeline divergence", "Dual pipeline syndrome", "Preprocessing fork"],
        "other_options": ["Concept drift", "Schema drift", "Feature drift"]
    },
    {
        "q": "True or False: In some recommendation systems, user feedback received after a prediction can be used as a quasi-label for online learning in Monitoring.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: Exploratory Analysis is the only stage where it is acceptable to use interactive SQL directly on the production database.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the 'what-if' question to the stage where it is most commonly answered:",
        "type": "match",
        "left": ["What if we had 10× more data?", "What if we remove this sensitive feature?", "What if traffic doubles overnight?", "What if ground truth stops arriving?"],
        "right": ["Exploratory Analysis", "Model Building", "Deployment", "Monitoring"]
    },
    {
        "q": "The unofficial 'seventh stage' some teams add after Monitoring when a model is officially retired is called ______.",
        "type": "fill_blank",
        "answers": ["Decommissioning", "Retirement", "Sunsetting"],
        "other_options": ["Archiving", "Freezing", "Pausing"]
    },
    {
        "q": "When a fraud detection model suddenly stops catching any fraud for 12 hours because attackers learned its exact decision boundary, this is an example of ______ attack detected in Monitoring.",
        "type": "fill_blank",
        "answers": ["evasion", "adversarial evasion"],
        "other_options": ["poisoning", "extraction", "inversion"]
    },
    {
        "q": "______ is the stage in which you first realize that the original business question was poorly defined and needs to be reframed.",
        "type": "fill_blank",
        "answers": ["Exploratory Analysis"],
        "other_options": ["Data Collection", "Model Building", "Deployment"]
    },
    {
        "q": "In large organizations, the ______ team usually owns the final sign-off before traffic is routed to a new model version.",
        "type": "fill_blank",
        "answers": ["Change Advisory Board", "Release board", "CAB"],
        "other_options": ["Data Science", "Analytics", "Product"]
    },
    {
        "q": "True or False: You can legally collect public social media posts for training without user consent in most jurisdictions.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "True or False: During Data Preparation, it is acceptable to filter out rows where the target label is missing if you are building a supervised model.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: A model that achieves 100% accuracy on real production traffic almost certainly has a label leakage bug.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these actions when onboarding a legacy model that was never properly documented:",
        "type": "rearrange",
        "words": ["Reverse-engineer preprocessing steps", "Recreate training dataset from logs", "Add input validation and tests", "Wrap model in standard serving container", "Write Model Card from tribal knowledge", "Set up basic monitoring alerts"]
    },
    {
        "q": "Match the MLOps maturity level to its defining characteristic:",
        "type": "match",
        "left": ["Level 0 – Manual", "Level 1 – Automated training", "Level 2 – CI/CD pipeline", "Level 3 – Fully automated governance"],
        "right": ["Everything done in notebooks, no reproducibility", "Training runs on schedule, but deployment is manual", "One-click deploy with automated tests and canary", "Model retraining, monitoring, and rollback fully automated with policy enforcement"]
    },
    {
        "q": "______ is the practice of keeping a small percentage of production traffic going to an old model forever as a fallback safety net.",
        "type": "fill_blank",
        "answers": ["permanent shadow", "immortal champion", "fallback routing"],
        "other_options": ["canary forever", "blue-green lock", "traffic mirroring"]
    },
    {
        "q": "True or False: In batch prediction systems for monthly reporting, the Monitoring stage can be performed manually by a human each month.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: Data drift detection algorithms themselves can suffer from drift and need periodic recalibration.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'hidden cost' to the stage where it usually surprises the team:",
        "type": "match",
        "left": ["Human labeling at scale", "GPU hours for hyperparameter search", "Cold-start inference latency on serverless", "Storing 10 billion prediction records for audit"],
        "right": ["Data Collection", "Model Building", "Deployment", "Monitoring"]
    },
    {
        "q": "Rearrange these email subjects in the order they typically appear during a failing production model incident:",
        "type": "rearrange",
        "words": ["P1 – Prediction service down", "Update: Root cause identified – concept drift", "Rollback completed – service restored", "Post-mortem meeting scheduled", "Retraining pipeline triggered", "New model promoted – incident closed"]
    },
    {
        "q": "______ is when you deliberately inject simulated drift into a staging environment to test your monitoring system’s sensitivity.",
        "type": "fill_blank",
        "answers": ["Chaos engineering", "drift injection testing", "synthetic drift testing"],
        "other_options": ["adversarial training", "stress testing", "load testing"]
    },
    {
        "q": "The only stage where it is acceptable to look at individual customer records without anonymization (with proper authorization) is usually ______.",
        "type": "fill_blank",
        "answers": ["Exploratory Analysis", "Data Collection"],
        "other_options": ["Deployment", "Monitoring", "Model Building"]
    },
    {
        "q": "True or False: A model that was state-of-the-art when deployed can become technically illegal months later if new privacy laws are passed.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: The Deployment stage includes writing the SLA (Service Level Agreement) that defines acceptable latency and error rates.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'what went wrong' post-mortem finding to its root cause stage:",
        "type": "match",
        "left": ["Used future information in features", "Model served unscaled features", "Monitoring alerted on wrong metric", "Training data included test period"],
        "right": ["Data Preparation", "Deployment", "Monitoring", "Data Collection"]
    },
    {
        "q": "The ______ stage is where you first create a 'data quality scorecard' with thresholds for null rates, cardinality, and freshness.",
        "type": "fill_blank",
        "answers": ["Data Collection"],
        "other_options": ["Preparation", "Monitoring", "Deployment"]
    },
    {
        "q": "______ is the term for automatically routing ambiguous or low-confidence predictions to human reviewers in production.",
        "type": "fill_blank",
        "answers": ["human-in-the-loop", "human in the loop"],
        "other_options": ["fallback model", "confidence routing", "escalation layer"]
    },
    {
        "q": "______ is the practice of training separate models for different geographic regions to prevent a single point of failure.",
        "type": "fill_blank",
        "answers": ["model sharding", "geo-sharding"],
        "other_options": ["multi-model", "ensemble", "federated"]
    },
    {
        "q": "True or False: The Exploratory Analysis stage often uncovers that the original target variable is a proxy for what the business actually cares about.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: You should always log the exact software versions (Python, library, OS) used during Model Building for forensic debugging later.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: In some industries, the Monitoring stage must retain every single prediction and its input for 7+ years due to regulations.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these items into the correct order for building a responsible AI governance checklist:",
        "type": "rearrange",
        "words": ["Define responsible AI principles", "Map data provenance & lineage", "Perform bias & fairness audit", "Create model risk tier classification", "Write mitigation & rollback plan", "Obtain final sign-off from ethics board"]
    },
    {
        "q": "Match the specialized role to the lifecycle stage where they typically have the strongest influence:",
        "type": "match",
        "left": ["Data Engineer", "ML Engineer", "Analytics Engineer", "Responsible AI Lead", "Site Reliability Engineer (SRE)"],
        "right": ["Data Collection", "Deployment", "Exploratory Analysis", "Model Building", "Monitoring"]
    },
    {
        "q": "______ is the technique of using a cheap proxy model to filter requests before calling an expensive LLM or vision model.",
        "type": "fill_blank",
        "answers": ["model cascading", "cascade", "router model"],
        "other_options": ["distillation", "quantization", "pruning"]
    },
    {
        "q": "True or False: The Data Preparation stage is where you decide whether to use exact timestamps or relative time features (e.g., days_since_last_login).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: Federated learning completely eliminates the need for the Data Collection stage in privacy-sensitive environments.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the model update strategy to its real-world trigger example:",
        "type": "match",
        "left": ["Periodic retraining", "Event-triggered retraining", "Continuous online learning", "Never retrain (immutable)"],
        "right": ["Every Sunday at 3 AM", "When fraud patterns change after a holiday", "Update weights after every 100 predictions", "Regulatory-approved credit scoring model"]
    },
    {
        "q": "Rearrange these steps for implementing a multi-objective optimization during Model Building:",
        "type": "rearrange",
        "words": ["Define primary business metric", "Add fairness constraint as secondary objective", "Add inference cost/latency constraint", "Use Pareto front to select final model", "Present trade-off curve to stakeholders"]
    },
    {
        "q": "______ is the stage where you calculate the ROI of the entire ML project by comparing lift against engineering costs.",
        "type": "fill_blank",
        "answers": ["Monitoring"],
        "other_options": ["Deployment", "Model Building", "Exploratory Analysis"]
    },
    {
        "q": "True or False: Some companies run synthetic data generation pipelines as part of Data Preparation when real data is legally blocked.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: The Deployment stage can be considered successful only after the first successful rollback has been tested.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'edge case' to the stage where it is most likely discovered too late:",
        "type": "match",
        "left": ["Leap year date parsing bug", "Currency conversion on public holidays", "Daylight saving time shift", "Country-specific address format"],
        "right": ["Data Preparation", "Deployment", "Data Collection", "Monitoring"]
    },
    {
        "q": "______ is the unofficial stage some teams insert between Model Building and Deployment for extended offline stress testing and red-teaming.",
        "type": "fill_blank",
        "answers": ["Model validation", "pre-production validation", "staging validation"],
        "other_options": ["shadow", "canary", "pilot"]
    },
    {
        "q": "The only lifecycle stage where it is acceptable to deliberately over-sample the minority class without documenting it as a business assumption is ______.",
        "type": "fill_blank",
        "answers": ["none", "never"],
        "other_options": ["Data Preparation", "Model Building", "Exploratory Analysis"]
    },
    {
        "q": "______ is the name for running multiple candidate models in parallel in production and dynamically selecting the best prediction per request.",
        "type": "fill_blank",
        "answers": ["model ensemble routing", "dynamic routing", "champion-challenger online"],
        "other_options": ["multi-armed bandit", "cascading", "shadow testing"]
    },
    {
        "q": "In highly seasonal businesses, the Monitoring stage must include ______ seasonality to avoid false drift alerts during holidays.",
        "type": "fill_blank",
        "answers": ["seasonal baselining", "seasonal adjustment"],
        "other_options": ["weekly retraining", "holiday flagging", "manual override"]
    },
    {
        "q": "True or False: You are allowed to peek at the test set performance once to decide whether to continue the project or kill it during Model Building.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "True or False: In some medical applications, the Monitoring stage includes survival analysis on actual patient outcomes years after the prediction.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: Data Collection can be performed entirely by business analysts using no-code ETL tools in modern organizations.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these items in the order they appear in a typical ML project graveyard (failed projects):",
        "type": "rearrange",
        "words": ["Project kicked off with high expectations", "EDA reveals no predictive signal", "Multiple models tried with random tweaks", "Stakeholders lose interest", "Project quietly archived", "Team claims 'data issue'"]
    },
    {
        "q": "Match the exotic deployment environment to its primary constraint:",
        "type": "match",
        "left": ["On-device ML (mobile phone)", "Edge gateway in factory", "Satellite with 200ms latency", "Embedded in medical implant"],
        "right": ["Model size < 5MB, no internet", "Must run 1000 inferences/sec on 12W", "No real-time retraining possible", "10-year battery life, no updates"]
    },
    {
        "q": "______ is the practice of keeping the previous 12 model versions in the registry just in case regulators ask for historical predictions.",
        "type": "fill_blank",
        "answers": ["model versioning compliance", "version hoarding", "regulatory retention"],
        "other_options": ["canary archive", "rollback buffer", "shadow storage"]
    },
    {
        "q": "True or False: During Exploratory Analysis, it is common to discover that the requested ML solution is actually a reporting problem in disguise.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: The Monitoring stage is the only stage where you are allowed to calculate profit per prediction in real currency units.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'forbidden action' to the stage where violating it causes the biggest disaster:",
        "type": "match",
        "left": ["Using production credentials in a notebook", "Hard-coding secrets in model code", "Training on data that includes future labels", "Disabling all monitoring alerts to reduce noise"],
        "right": ["Exploratory Analysis", "Deployment", "Data Preparation", "Monitoring"]
    },
    {
        "q": "Rearrange these increasingly strict data access policies from typical startup to bank:",
        "type": "rearrange",
        "words": ["Everyone has prod DB access", "Only engineers with ticket", "Data served via approved views only", "All queries reviewed by security team", "Zero direct access – only via feature store"]
    },
    {
        "q": "______ is when you train a model to predict whether a human labeler will agree with the current model, then use it to route only high-disagreement cases.",
        "type": "fill_blank",
        "answers": ["disagreement sampling", "active learning router", "confidence-disagreement routing"],
        "other_options": ["human-in-the-loop", "selective labeling", "cold-start filter"]
    },
    {
        "q": "The phenomenon where a perfect model starts failing because users changed behavior in response to its predictions is called ______ loop.",
        "type": "fill_blank",
        "answers": ["performative feedback", "prediction feedback loop", "self-fulfilling prophecy"],
        "other_options": ["concept drift", "adversarial attack", "gaming"]
    },
    {
        "q": "True or False: Some companies deliberately introduce controlled randomness in production predictions to make model extraction attacks harder.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: The Deployment stage is where you finally discover that the model's pickle file is 47 GB and cannot be loaded into memory.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'smell' to the stage that should have caught it:",
        "type": "match",
        "left": ["100% accuracy on test set", "Feature named 'is_fraud_manual_override'", "Prediction latency 8 seconds on CPU", "Missing values filled with 9999"],
        "right": ["Model Building", "Data Collection", "Deployment", "Data Preparation"]
    },
    {
        "q": "______ is the stage where you first calculate how much money the company will lose if the model is down for one hour.",
        "type": "fill_blank",
        "answers": ["Deployment"],
        "other_options": ["Monitoring", "Model Building", "Collection"]
    },
    {
        "q": "______ is the term for a model that was deployed by mistake and ran in production for months before anyone noticed it was unused.",
        "type": "fill_blank",
        "answers": ["zombie model", "ghost model", "orphaned model"],
        "other_options": ["shadow model", "canary model", "baseline model"]
    },
    {
        "q": "In some companies, the final gate before Deployment is a mandatory ______ review where lawyers read the Model Card line by line.",
        "type": "fill_blank",
        "answers": ["legal", "compliance", "risk"],
        "other_options": ["technical", "performance", "data"]
    },
    {
        "q": "True or False: A model that predicts tomorrow’s temperature perfectly using today’s weather station data probably contains a subtle timestamp leakage.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: The Monitoring stage can trigger an automatic payout or refund to customers if the model confidence falls below a business-defined threshold.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: It is impossible to fully automate the Exploratory Analysis stage because domain insight is always required.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these actions when a regulator sends an official request for explainability on a specific past prediction:",
        "type": "rearrange",
        "words": ["Locate the exact model version", "Retrieve logged input payload", "Re-run preprocessing pipeline", "Generate SHAP or LIME explanation", "Redact PII from explanation", "Send audited report to regulator"]
    },
    {
        "q": "Match the 'unusual' monitoring metric to the real company that actually uses it:",
        "type": "match",
        "left": ["% of predictions that were manually overridden by humans", "Average time from prediction to user complaint", "Revenue per served prediction", "Number of journalists who mentioned the model today"],
        "right": ["Trading firm", "Ride-hailing platform", "Ad-tech company", "Autonomous vehicle team"]
    },
    {
        "q": "______ is the practice of training a tiny 'watchdog' model whose only job is to predict whether the main model is about to make a catastrophic error.",
        "type": "fill_blank",
        "answers": ["error detector", "mistake predictor", "watchdog model"],
        "other_options": ["meta-model", "confidence model", "rejector"]
    },
    {
        "q": "True or False: Some banks retrain their credit risk models only once every 5 years because regulators require model stability.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: The Data Collection stage can be skipped entirely when using transfer learning from public pre-trained models.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the failure mode to the lifecycle stage that should have prevented it:",
        "type": "match",
        "left": ["Model predicts negative prices", "Model crashes on inputs containing emojis", "Model leaks training data in explanations", "Model performance drops every Monday morning"],
        "right": ["Model Building", "Data Preparation", "Deployment", "Monitoring"]
    },
    {
        "q": "Rearrange these increasingly sophisticated ways to define 'model success' from junior to senior stakeholder:",
        "type": "rearrange",
        "words": ["It beats the baseline", "It has >90% accuracy", "It improves the business KPI by 3%", "It survives a red-team attack", "It still works after we change the data schema", "We can explain every rejected loan to a regulator"]
    },
    {
        "q": "______ is when the Monitoring dashboard itself becomes the main consumer of the model's predictions just to stay updated.",
        "type": "fill_blank",
        "answers": ["monitoring tail wagging the dog", "dashboard feedback loop", "self-monitoring loop"],
        "other_options": ["infinite loop", "observability overhead", "metric bloat"]
    },
    {
        "q": "The only lifecycle stage where it is acceptable to have a human manually approve every single prediction before it reaches the user is ______.",
        "type": "fill_blank",
        "answers": ["Deployment"],
        "other_options": ["Monitoring", "Model Building", "Preparation"]
    },
    {
        "q": "True or False: In some high-stakes systems, the Deployment stage includes a physical 'break glass' key that instantly reverts to a rule-based fallback.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: A perfectly reproducible pipeline can still fail in production if the random seed behavior changed between library versions.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'model retirement' reason to its most famous real-world example:",
        "type": "match",
        "left": ["Model became too expensive to serve", "Model was gamed by users", "Regulation banned the input features", "Company pivoted business model"],
        "right": ["Google Flu Trends", "Early credit scoring models using ZIP code", "Many COVID-19 prediction dashboards", "Microsoft Tay chatbot"]
    },
    {
        "q": "______ is the stage where you first write the SQL query that will later run every day in production to generate the training dataset.",
        "type": "fill_blank",
        "answers": ["Data Collection"],
        "other_options": ["Preparation", "Model Building", "Monitoring"]
    },
    {
        "q": "______ is the tongue-in-cheek name for a model that is so accurate on historical data that everyone refuses to believe the future has changed.",
        "type": "fill_blank",
        "answers": ["prophet model", "hindsight hero", "perfect hindcaster"],
        "other_options": ["oracle", "time traveler", "overfitted genius"]
    },
    {
        "q": "In some autonomous vehicle teams, the Monitoring stage includes a metric called '______ per 1,000 miles' to quantify human driver interventions.",
        "type": "fill_blank",
        "answers": ["disengagements"],
        "other_options": ["takeovers", "interventions", "overrides"]
    },
    {
        "q": "True or False: It is acceptable to use a different random seed for the final production model than was used during experimentation, as long as performance is similar.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "True or False: During Exploratory Analysis, discovering that 98% of the target events happen in only three countries can kill an otherwise promising global model.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: The Deployment stage is where you discover that your beautiful 500 MB model exceeds the memory limit of the $5/month cloud function.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these increasingly rare events in a mature MLOps pipeline:",
        "type": "rearrange",
        "words": ["Manual retraining", "Hotfix deployment at 3 AM", "Someone opens the Jupyter notebook in production", "Rollback to a model from 18 months ago", "Deleting the entire feature store by accident", "Data scientist SSHes into the prediction server"]
    },
    {
        "q": "Match the 'weird but real' production bug to the stage that should have caught it earlier:",
        "type": "match",
        "left": ["Model returns NaN when user’s name contains an apostrophe", "Predictions flip every day at midnight UTC regardless of timezone", "Model accuracy drops exactly when daylight saving time ends", "Model crashes on inputs containing the word 'null' as string"],
        "right": ["Data Preparation", "Deployment", "Data Collection", "Data Preparation"]
    },
    {
        "q": "______ is the practice of running the exact same training pipeline on historical data every month just to prove nothing has accidentally changed.",
        "type": "fill_blank",
        "answers": ["reproducibility canary", "pipeline regression testing", "golden run validation"],
        "other_options": ["backtesting", "shadow training", "dry run"]
    },
    {
        "q": "True or False: In some insurance companies, the Model Building stage must be completed and frozen before the product is even allowed to be sold.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: The Monitoring stage can legitimately send a PagerDuty alert when the champion model starts losing to a 5-year-old heuristic baseline.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'secretly important' soft skill to the lifecycle stage where its absence hurts the most:",
        "type": "match",
        "left": ["Storytelling with visualizations", "Negotiating data access with legal", "Explaining SHAP values to non-technical executives", "Writing clear rollback instructions at 2 AM"],
        "right": ["Exploratory Analysis", "Data Collection", "Model Building", "Deployment"]
    },
    {
        "q": "Rearrange these quotes from real data scientists in the order they are typically said during a failing project:",
        "type": "rearrange",
        "words": ["'This will be quick — just a simple model'", "'Wait, the label is entered after the event?'", "'Let me just retrain on all data real quick'", "'We need to loop back to requirements'", "'Can we blame the data?'", "'The business moved the goalposts again'"]
    },
    {
        "q": "______ is when you keep a dummy model that always predicts the historical average just so stakeholders can see how much value the real model actually adds.",
        "type": "fill_blank",
        "answers": ["sanity baseline", "straw-man model", "value proof model"],
        "other_options": ["null model", "mean predictor", "random model"]
    },
    {
        "q": "The only stage where deliberately breaking the model on purpose (fault injection) is considered good practice is ______.",
        "type": "fill_blank",
        "answers": ["Deployment"],
        "other_options": ["Monitoring", "Model Building", "Preparation"]
    },
    {
        "q": "True or False: Some companies have a 'model debt' backlog exactly like technical debt, tracking models that need refactoring or retraining.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: A single misplaced decimal point in a reward function during Model Building once accidentally bankrupted a trading firm in under an hour.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'famous last words' to the stage where they were spoken:",
        "type": "match",
        "left": ["'We'll clean the data properly later'", "'It's only 0.1% of the rows'", "'The test set looks fine'", "'No one will ever send JSON with extra fields'"],
        "right": ["Data Collection", "Data Preparation", "Model Building", "Deployment"]
    },
    {
        "q": "______ is the stage where you first realize the business team has been using a completely different definition of the target variable than what is stored in the database.",
        "type": "fill_blank",
        "answers": ["Exploratory Analysis"],
        "other_options": ["Data Collection", "Model Building", "Deployment"]
    },
    {
        "q": "______ is the informal name for the phenomenon where a model quietly starts predicting the ID of the row instead of the actual target.",
        "type": "fill_blank",
        "answers": ["index leakage", "row number cheating", "id memorization"],
        "other_options": ["overfitting", "concept drift", "label leakage"]
    },
    {
        "q": "In some news recommendation systems, the Monitoring stage tracks a metric called ______ — the percentage of users who immediately close the app after seeing a prediction.",
        "type": "fill_blank",
        "answers": ["rage click", "instant bounce", "immediate exit rate"],
        "other_options": ["churn spike", "dislike ratio", "skip rate"]
    },
    {
        "q": "True or False: It is possible to have a fully functioning production model that has never been looked at by a human after initial deployment.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: During Data Collection, sampling only the last 30 days of data is safe if the use case is fraud detection.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "True or False: The phrase 'garbage in, gospel out' perfectly describes what happens when flawless Deployment and Monitoring are built on terrible Data Preparation.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these real Slack messages in the chronological order they appeared during a real production incident:",
        "type": "rearrange",
        "words": ["anyone seeing weird predictions?", "wait, the model is returning yesterday’s scores", "oh no — the feature job ran twice today", "rollback initiated", "incident bridge created", "post-mortem tomorrow 10am"]
    },
    {
        "q": "Match the 'anti-pattern' to the stage it destroys trust in:",
        "type": "match",
        "left": ["Cherry-picking the best test slice to report", "Changing the evaluation metric after seeing results", "Silently retraining every week without versioning", "Turning off drift alerts because they’re too noisy"],
        "right": ["Model Building", "Model Building", "Deployment", "Monitoring"]
    },
    {
        "q": "______ is the practice of keeping a secret 'kill switch' feature that forces the model to output a safe default if a hidden trigger is activated.",
        "type": "fill_blank",
        "answers": ["backdoor override", "panic button", "dead man's switch"],
        "other_options": ["fallback trigger", "emergency brake", "master override"]
    },
    {
        "q": "True or False: Some dating apps use a shadow-banned model that runs normally but its matches are never shown to real users.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: In certain high-frequency trading systems, the entire lifecycle from new data to new deployed model happens in under 100 milliseconds.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'model archaeology' discovery to the stage where it was embarrassingly found:",
        "type": "match",
        "left": ["Hard-coded list of 50 VIP user IDs with special logic", "Training data includes rows from 2099", "Model weights contain a developer’s ex’s name", "The 'random' forest is actually deterministic because seed=42 everywhere"],
        "right": ["Deployment", "Data Collection", "Model Building", "Model Building"]
    },
    {
        "q": "Rearrange these career stages of a data scientist as their relationship with the lifecycle evolves:",
        "type": "rearrange",
        "words": ["Only cares about AUC", "Obsessed with feature engineering", "Starts writing tests", "Begins instrumenting monitoring", "Spends meetings defending old models", "Gives talks titled 'Why we killed our best model'"]
    },
    {
        "q": "______ is when the Monitoring alerts are so well tuned that the on-call engineer receives exactly one page per year — and it’s always a real fire.",
        "type": "fill_blank",
        "answers": ["alert nirvana", "perfect observability", "zero-noise monitoring"],
        "other_options": ["silent mode", "batphone discipline", "pager enlightenment"]
    },
    {
        "q": "The only stage where stakeholders are allowed to say 'just push it to production and we’ll see' without getting fired is ______.",
        "type": "fill_blank",
        "answers": ["never", "none"],
        "other_options": ["Exploratory Analysis", "Model Building", "Deployment"]
    },
    {
        "q": "True or False: There exist production models whose training data legally cannot be stored anywhere — it must be processed on-the-fly and immediately deleted.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: A model that has been running perfectly for 3 years can still be considered experimental if no one ever documented it properly.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'legendary excuse' to the stage it was used:",
        "type": "match",
        "left": ["'The data was dirty'", "'The users are weird this week'", "'My local notebook works'", "'It’s only failing in canary'"],
        "right": ["Data Preparation", "Monitoring", "Deployment", "Deployment"]
    },
    {
        "q": "The ______ stage is where you first create the official 'data contract' specifying exact schema, freshness, and ownership of each table used for training.",
        "type": "fill_blank",
        "answers": ["Data Collection"],
        "other_options": ["Preparation", "Monitoring", "Deployment"]
    },
    {
        "q": "______ is the term for a model that was trained once on 2018 data and has been serving predictions unchanged ever since, still making money.",
        "type": "fill_blank",
        "answers": ["immortal model", "set-and-forget model", "eternal model"],
        "other_options": ["legacy model", "frozen model", "dinosaur model"]
    },
    {
        "q": "In some e-commerce companies, the Monitoring stage tracks a KPI called ______ — the exact dollar amount directly attributed to model predictions in the last 24h.",
        "type": "fill_blank",
        "answers": ["model-attributed revenue", "model lift revenue", "incremental revenue"],
        "other_options": ["prediction value", "model ROI", "lift dollars"]
    },
    {
        "q": "True or False: During Model Building, you are allowed to use GroupKFold instead of simple KFold when your data has natural groupings (e.g., multiple rows per customer).",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: The Exploratory Analysis stage is the best place to involve domain experts who have never seen the raw data before.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: A company can be legally required to delete a deployed model and all its logs within 72 hours of a user request.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these columns in the exact order they should appear in a well-structured feature store entity definition:",
        "type": "rearrange",
        "words": ["entity_key", "feature_name", "value_type", "description", "owner", "creation_timestamp", "last_updated", "ttl_days"]
    },
    {
        "q": "Match the advanced validation technique to its primary purpose:",
        "type": "match",
        "left": ["Adversarial validation", "Leakage detection sweep", "Trojan detection", "Population stability index (PSI)"],
        "right": ["Check train/test distribution similarity", "Find features that perfectly predict train vs test", "Detect backdoored training data", "Measure feature drift in production"]
    },
    {
        "q": "______ is the technique of encrypting model weights so they can only be used on specific hardware (e.g., inside a secure enclave).",
        "type": "fill_blank",
        "answers": ["model binding", "hardware attestation", "enclave sealing"],
        "other_options": ["model watermarking", "model fingerprinting", "secure inference"]
    },
    {
        "q": "True or False: Some companies use differential privacy noise during training but remove it completely for the final deployed model to maximize accuracy.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "True or False: In batch scoring jobs that run monthly, the Deployment stage can be as simple as an Airflow DAG with no live endpoint.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'model governance artifact' to its typical owner:",
        "type": "match",
        "left": ["Model Risk Assessment form", "Data Ethics Review checklist", "Model Performance Report", "Incident Response Runbook"],
        "right": ["Risk & Compliance team", "Ethics board", "ML Engineering", "MLOps/SRE"]
    },
    {
        "q": "Rearrange these steps for a proper model sunsetting ceremony:",
        "type": "rearrange",
        "words": ["Announce deprecation timeline", "Redirect traffic to successor model", "Archive model artifacts", "Delete serving endpoints", "Remove from model registry", "Write retirement retrospective"]
    },
    {
        "q": "______ is the stage where you first benchmark inference latency on the exact hardware that will be used in production.",
        "type": "fill_blank",
        "answers": ["Deployment"],
        "other_options": ["Model Building", "Monitoring", "Preparation"]
    },
    {
        "q": "True or False: The Data Preparation stage can include generating completely synthetic datasets using GANs when real data is unavailable.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: In some defense applications, model weights are classified as munitions and cannot be exported without government approval.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'exotic serving requirement' to the industry that actually has it:",
        "type": "match",
        "left": ["Model must run offline on a submarine for 90 days", "Predictions must be explainable to a judge in court", "Model must fit in 64 KB RAM", "Model must be auditable by external regulator every quarter"],
        "right": ["Military", "Consumer finance", "IoT microcontroller", "Insurance"]
    },
    {
        "q": "______ is the practice of treating the entire ML system as immutable infrastructure — when a new model is ready, you destroy and recreate the entire serving cluster.",
        "type": "fill_blank",
        "answers": ["immutable deployment", "phoenix serving", "full recreation deployment"],
        "other_options": ["blue-green full", "zero-downtime rebuild", "infrastructure as code serving"]
    },
    {
        "q": "The ______ stage is where you first define the exact 'ground truth reconciliation window' for delayed labels (e.g., 30-day chargeback window).",
        "type": "fill_blank",
        "answers": ["Data Collection"],
        "other_options": ["Monitoring", "Model Building", "Deployment"]
    },
    {
        "q": "______ is the technique of training a separate 'calibration model' on top of your main model’s raw scores to ensure probabilities are well-calibrated in production.",
        "type": "fill_blank",
        "answers": ["post-hoc calibration", "Platt scaling", "isotonic regression"],
        "other_options": ["temperature scaling", "label smoothing", "confidence tuning"]
    },
    {
        "q": "______ is the stage where you decide whether to use point-in-time features or the latest available features for training.",
        "type": "fill_blank",
        "answers": ["Data Preparation"],
        "other_options": ["Data Collection", "Model Building", "Monitoring"]
    },
    {
        "q": "True or False: In some pharmaceutical companies, the Model Building stage must be fully documented and locked before any clinical trial begins.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: The Monitoring stage can include automated 'model apology emails' sent to users when a wrong prediction is confirmed.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: Federated learning completely removes the need for a centralized Monitoring dashboard.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these feature store concepts from offline to online in the correct latency order:",
        "type": "rearrange",
        "words": ["Historical feature values for training", "Batch materialization job", "Push to online store", "Real-time feature update via Kafka", "Sub-millisecond lookup at inference"]
    },
    {
        "q": "Match the privacy-preserving technique to its primary lifecycle stage of application:",
        "type": "match",
        "left": ["Differential privacy noise addition", "k-anonymity on training data", "Homomorphic encryption at inference", "Secure multi-party computation for features"],
        "right": ["Model Building", "Data Preparation", "Deployment", "Data Collection"]
    },
    {
        "q": "______ is the practice of running the production model on historical disaster scenarios to prove it would have behaved safely.",
        "type": "fill_blank",
        "answers": ["counterfactual stress testing", "historical red-teaming", "what-if validation"],
        "other_options": ["backtesting", "shadow simulation", "safety replay"]
    },
    {
        "q": "True or False: Some companies use blockchain to create an immutable audit trail of every model version and its training data hash.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: In batch prediction systems, the Deployment stage can be performed by uploading a CSV of predictions to an S3 bucket.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'model explainability mandate' to the regulation that enforces it:",
        "type": "match",
        "left": ["Right to explanation for automated decisions", "Model documentation and impact assessment", "Adverse Action Notices with reasons", "Annual model validation report"],
        "right": ["GDPR Article 22", "EU AI Act", "FCRA / ECOA (US)", "SR 11-7 / MRG (banking)"]
    },
    {
        "q": "Rearrange these steps for implementing a proper 'model promotion pipeline' with multiple environments:",
        "type": "rearrange",
        "words": ["dev → integration tests pass", "staging → shadow deployment", "canary in prod (1%)", "gradual rollout to 100%", "old version automatically retired", "champion updated in registry"]
    },
    {
        "q": "______ is the stage where you first calculate the 'data debt' — the estimated engineering hours needed to fix known data quality issues.",
        "type": "fill_blank",
        "answers": ["Data Collection", "Exploratory Analysis"],
        "other_options": ["Preparation", "Monitoring", "Deployment"]
    },
    {
        "q": "True or False: Some companies use quantum-resistant encryption to protect model weights stored in the registry.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: The Exploratory Analysis stage can legitimately conclude that the best 'model' is actually a SQL query with a few business rules.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'infrastructure-as-code' tool to the lifecycle stage it dominates:",
        "type": "match",
        "left": ["Terraform", "Dagster / dbt", "Kubeflow Pipelines", "Argo Workflows", "Prometheus + Grafana"],
        "right": ["Deployment", "Data Preparation", "Model Building", "Monitoring", "Monitoring"]
    },
    {
        "q": "______ is the practice of treating model predictions as a microservice with its own SLOs, error budgets, and incident response team.",
        "type": "fill_blank",
        "answers": ["prediction service SRE", "model as service", "production-grade inference"],
        "other_options": ["MLOps maturity level 2", "reliable AI", "observable ML"]
    },
    {
        "q": "______ is the stage where you first realize that the target variable is recorded in a different timezone in half of the source systems.",
        "type": "fill_blank",
        "answers": ["Data Collection"],
        "other_options": ["Preparation", "Exploratory Analysis", "Monitoring"]
    },
    {
        "q": "______ is the name for a model that is only allowed to influence 0.01% of decisions because regulators haven’t approved full rollout yet.",
        "type": "fill_blank",
        "answers": ["pilot model", "sandboxed model", "restricted deployment"],
        "other_options": ["canary forever", "shadow permanent", "gated model"]
    },
    {
        "q": "In some logistics companies, the Monitoring stage tracks a metric called ______ — the percentage of predictions that were overridden by a human dispatcher.",
        "type": "fill_blank",
        "answers": ["human override rate", "dispatcher veto rate", "manual correction rate"],
        "other_options": ["disagreement rate", "fallback usage", "rejection rate"]
    },
    {
        "q": "True or False: You can use stratified sampling during Data Collection even if the target variable is not available in the raw stream.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "True or False: In some satellite imagery projects, the entire Data Collection stage happens only once every 16 days when the satellite passes overhead.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: The Deployment stage is allowed to have a different floating-point precision (e.g., float16 vs float32) than Model Building if latency requires it.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these increasingly strict model approval gates that exist in a Tier-1 bank:",
        "type": "rearrange",
        "words": ["Team lead review", "Model validation team sign-off", "Model risk committee vote", "Chief risk officer final approval", "Regulator notified (no objection)", "Board informed"]
    },
    {
        "q": "Match the 'data poisoning defense' to the stage where it is applied:",
        "type": "match",
        "left": ["Spectre defense (anomaly detection on training data)", "Data sanitization pipeline", "Robust training with trimmed loss", "Inference-time input validation"],
        "right": ["Data Collection", "Data Preparation", "Model Building", "Deployment"]
    },
    {
        "q": "______ is the technique of splitting a huge model into smaller 'expert' sub-models and routing each request to only one expert.",
        "type": "fill_blank",
        "answers": ["Mixture of Experts", "MoE routing", "sparse activation"],
        "other_options": ["model sharding", "pipeline parallelism", "tensor slicing"]
    },
    {
        "q": "True or False: Some companies deliberately add fake 'honeypot' records to training data to detect if a competitor scraped their model outputs.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: The Exploratory Analysis stage can be performed on a 1% random sample of petabyte-scale data without losing critical insights.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'model quantization' level to its typical accuracy-latency trade-off:",
        "type": "match",
        "left": ["INT8 quantization", "BF16 training + inference", "4-bit GPTQ", "1-bit binary neural network"],
        "right": ["~0.5% accuracy drop, 4× faster", "No accuracy drop, slightly slower than FP16", "5–10% drop, 8×+ faster & smaller", "Massive drop, mostly research toy"]
    },
    {
        "q": "Rearrange these steps for a proper 'data refresh' when a source table schema changes unexpectedly:",
        "type": "rearrange",
        "words": ["Detect schema drift alert", "Quarantine new data", "Update ingestion contract", "Backfill corrected historical data", "Trigger full retraining", "Promote new model version"]
    },
    {
        "q": "______ is the stage where you first run the 'sanity check' that the most important feature according to the model actually makes business sense.",
        "type": "fill_blank",
        "answers": ["Model Building"],
        "other_options": ["Exploratory Analysis", "Deployment", "Monitoring"]
    },
    {
        "q": "True or False: Some companies have a policy that no model may be deployed unless it has been attacked by an internal red team for at least 40 hours.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: In certain medical devices, the exact same model binary must be used from factory testing through the device’s entire 15-year lifespan.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'observability pillar' to the lifecycle stage that owns it:",
        "type": "match",
        "left": ["Metrics (latency, error rate)", "Logs (input/output payloads)", "Traces (end-to-end request path)", "Model-specific drift dashboards"],
        "right": ["Deployment", "Monitoring", "Deployment", "Monitoring"]
    },
    {
        "q": "______ is the practice of storing only the git commit hash of the training code + the exact dataset snapshot hash as the complete 'model provenance'.",
        "type": "fill_blank",
        "answers": ["minimal reproducible provenance", "git + data hash tagging", "commit-bound modeling"],
        "other_options": ["lightweight lineage", "code-first registry", "provenance minimalism"]
    },
    {
        "q": "______ is the only stage where you are allowed to use Excel as the primary tool without getting strange looks from the team.",
        "type": "fill_blank",
        "answers": ["Exploratory Analysis"],
        "other_options": ["Data Preparation", "Deployment", "Monitoring"]
    },
    {
        "q": "______ is the name for a production model that is deliberately kept one version behind the absolute best performer to reduce risk of sudden failure.",
        "type": "fill_blank",
        "answers": ["conservative champion", "lagging champion", "safe follower"],
        "other_options": ["canary champion", "shadow leader", "risk-averse model"]
    },
    {
        "q": "In some content moderation systems, the Monitoring stage tracks a metric called ______ — the number of appeals successfully overturned per 10,000 decisions.",
        "type": "fill_blank",
        "answers": ["appeal overturn rate", "successful appeal rate", "reversal rate"],
        "other_options": ["complaint success", "user win rate", "judgment error rate"]
    },
    {
        "q": "True or False: A model can be legally deployed in Europe even if it is a black box, as long as you provide meaningful post-hoc explanations for every decision.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "True or False: Some companies use quantum random number generators as the source of randomness for train/test splits to prevent any possible subconscious bias.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: The Data Preparation stage can legitimately include deleting 99.9% of the majority class to create a smaller, balanced dataset for initial prototyping.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these actions when a catastrophic backward-incompatible change lands in a core feature table:",
        "type": "rearrange",
        "words": ["All prediction endpoints start failing", "Emergency rollback of model versions", "Hotfix the feature pipeline", "Create new model versions with corrected logic", "Gradual re-rollout with extra validation", "Write 'never again' runbook entry"]
    },
    {
        "q": "Match the 'model watermarking' purpose to its real-world use case:",
        "type": "match",
        "left": ["Embedding secret trigger sequences", "Hiding ownership signal in weights", "Detecting if outputs were AI-generated", "Proving a leaked model came from your org"],
        "right": ["Backdoor detection research", "Copyright protection", "Content authenticity", "Leak tracing"]
    },
    {
        "q": "______ is the technique of training a 'distillation teacher' model in the cloud and then distilling a tiny student model that runs on edge devices.",
        "type": "fill_blank",
        "answers": ["cloud-to-edge distillation", "teacher-student distillation", "knowledge distillation for edge"],
        "other_options": ["model compression", "quantization distillation", "pruning cascade"]
    },
    {
        "q": "True or False: Some financial regulators require that the exact same laptop used for final model validation must be kept in a locked room for 10 years.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: In certain classified projects, the Model Building stage happens inside an air-gapped SCIF with no internet access whatsoever.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'exotic data source' to the lifecycle stage that makes it painful:",
        "type": "match",
        "left": ["Microphone audio from factory floor", "Satellite SAR imagery at 3m resolution", "EEG signals at 1000 Hz", "Blockchain transaction graph"],
        "right": ["Data Collection", "Data Preparation", "Data Collection", "Data Preparation"]
    },
    {
        "q": "Rearrange these increasingly paranoid security measures for model intellectual property:",
        "type": "rearrange",
        "words": ["Password-protected pickle files", "Encrypted model registry", "Model served only inside TEE", "Weights split across 3 parties with MPC", "Model never leaves secure vault — only encrypted queries allowed"]
    },
    {
        "q": "______ is the stage where you first discover that the most predictive feature is actually the primary key of the table.",
        "type": "fill_blank",
        "answers": ["Exploratory Analysis", "Model Building"],
        "other_options": ["Data Preparation", "Monitoring", "Deployment"]
    },
    {
        "q": "True or False: Some companies run a nightly 'model beauty contest' where every registered model is re-scored on fresh data and the worst ones are automatically archived.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: The Deployment stage can include compiling the model to WebAssembly so it runs directly in users’ browsers with zero server cost.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'failure celebration' ritual to the company that actually does it:",
        "type": "match",
        "left": ["Champagne when a model is killed", "Wake for retired models with eulogy", "Failure wall of shame → fame", "Post-mortem cake with error message frosting"],
        "right": ["Spotify", "Netflix", "Google", "Meta"]
    },
    {
        "q": "______ is the practice of keeping a 'model graveyard' repository where every failed experiment is committed with a one-sentence epitaph.",
        "type": "fill_blank",
        "answers": ["model cemetery", "failed experiments archive", "graveyard of shame"],
        "other_options": ["experiment morgue", "dead models repo", "failure museum"]
    },
    {
        "q": "______ is the stage where you first have to explain to a VP why the model needs 400 GB of RAM just to load.",
        "type": "fill_blank",
        "answers": ["Deployment"],
        "other_options": ["Model Building", "Monitoring", "Data Preparation"]
    },
    {
        "q": "______ is the unofficial term for a model that keeps getting retrained but never actually beats the 3-line SQL heuristic from 2012.",
        "type": "fill_blank",
        "answers": ["eternal underperformer", "perpetual sophomore", "heuristic hugger"],
        "other_options": ["lagging learner", "baseline kisser", "sql shadow"]
    },
    {
        "q": "In some gaming companies, Monitoring tracks ______ — the percentage of players who rage-quit within 5 minutes of receiving a matchmaking prediction.",
        "type": "fill_blank",
        "answers": ["match-induced churn", "rage quit rate", "toxic match fallout"],
        "other_options": ["instant leave rate", "tilt spike", "prediction blame"]
    },
    {
        "q": "True or False: You can deploy a model that was trained on synthetic data only, as long as you never claim it saw real user data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: Some nuclear power plants use ML models whose weights are physically etched into custom silicon and can never be updated.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: The phrase “it works on my machine” is still the #1 cause of Deployment-stage incidents in 2025.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these real calendar invites in the order they appear when a model is about to be killed:",
        "type": "rearrange",
        "words": ["Model retirement planning", "Stakeholder impact review", "Successor model demo", "Final sign-off meeting", "Kill switch activation", "Victory lap / wake"]
    },
    {
        "q": "Match the 'model surgery' technique to its desperation level:",
        "type": "match",
        "left": ["Fine-tuning on 50 new examples", "Manually editing a decision tree node", "Patching a single weight in the pickle file", "Recompiling with --fast-math to squeeze 2ms"],
        "right": ["Normal Tuesday", "Crunch time", "2 AM panic", "Career-ending move"]
    },
    {
        "q": "______ is the technique of running two completely independent ML pipelines and only acting when both agree (used in life-critical systems).",
        "type": "fill_blank",
        "answers": ["dual-path consensus", "tandem inference", "N-version modeling"],
        "other_options": ["redundant prediction", "twin models", "consensus serving"]
    },
    {
        "q": "True or False: Some companies have a 'model bounty program' where employees get paid for finding deployed models that can be replaced with simpler rules.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: In certain space missions, the model is uploaded once before launch and then runs for 12 years with zero possibility of patching.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'data weirdness' to the stage where it finally bit someone:",
        "type": "match",
        "left": ["Customer named Null Null lives in Null Island", "Unix epoch stored as string '1970-01-01' in some tables", "Birth year 0000 for users born in 2000", "Country code 'XX' for 'Prefer not to say'"],
        "right": ["Data Preparation", "Deployment", "Data Collection", "Monitoring"]
    },
    {
        "q": "Rearrange these stages of grief when a beloved model is forcibly retired by compliance:",
        "type": "rearrange",
        "words": ["Denial – 'But it’s only 0.3% worse!'", "Anger – 'Legal doesn’t understand AI'", "Bargaining – 'Can we keep it in shadow mode?'", "Depression – stares at old AUC plots", "Acceptance – writes the retirement PR", "Bonus stage: forks it for personal side project"]
    },
    {
        "q": "______ is the stage where you first hear the sentence “Can’t we just ask ChatGPT?” from a senior director.",
        "type": "fill_blank",
        "answers": ["Exploratory Analysis"],
        "other_options": ["Model Building", "Deployment", "Monitoring"]
    },
    {
        "q": "True or False: There exist production models that are literally a single if-else statement wrapped in a Docker container for versioning.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: Some hedge funds retrain their alpha-generating models using only the previous 23 minutes of market data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'deployment superstition' to the team that actually believes it:",
        "type": "match",
        "left": ["Never deploy on Friday", "Model must be promoted at exactly 11:11", "Name the version after a Norse god for luck", "Always deploy during Mercury retrograde for chaos testing"],
        "right": ["Netflix", "Spotify", "Uber", "A small startup no one admits to"]
    },
    {
        "q": "______ is the practice of keeping a 'model museum' where legendary old models are preserved and can still be queried for nostalgia or legal reasons.",
        "type": "fill_blank",
        "answers": ["model museum", "hall of fame registry", "legacy model archive"],
        "other_options": ["retired champions", "golden oldies", "vintage inference"]
    },
    {
        "q": "______ is the stage where you first discover that the label was entered by a third-party vendor who was paid per click and clicked too fast.",
        "type": "fill_blank",
        "answers": ["Exploratory Analysis"],
        "other_options": ["Data Collection", "Model Building", "Monitoring"]
    },
    {
        "q": "______ is the term for a model that is mathematically perfect on paper but was never actually loaded into production because no one wrote the serving code.",
        "type": "fill_blank",
        "answers": ["theoretical champion", "paper model", "ghost champion"],
        "other_options": ["research artifact", "notebook orphan", "unshipped winner"]
    },
    {
        "q": "In some clinical decision support systems, Monitoring includes a metric called ______ — the number of times a doctor clicked 'strongly disagree' with the model.",
        "type": "fill_blank",
        "answers": ["physician veto rate", "strong disagreement rate", "doctor override severity"],
        "other_options": ["clinical rejection", "MD distrust score", "override intensity"]
    },
    {
        "q": "True or False: A model trained exclusively on data from one hospital can be legally deployed nationwide in the US without any additional validation.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "True or False: Some oil rigs run edge ML models that send predictions once per week via satellite because bandwidth costs $10,000 per GB.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: You are allowed to use Monday’s data to predict Sunday’s outcome if you pretend the week is a circle.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these actual ticket titles from a real MLOps team in the order they were created during one terrible week:",
        "type": "rearrange",
        "words": ["Model serving OOM at 3 AM", "Why is the model returning negative infinity?", "Customer complains predictions are exactly 42", "Turns out the scaler was fitted on test data", "Legal wants all logs from March deleted", "CEO asks why the model is racist this morning"]
    },
    {
        "q": "Match the 'model confession' to the anonymous data scientist who wrote it on an internal meme channel:",
        "type": "match",
        "left": ["I leaked the test set into training and no one noticed for 18 months", "My best model is literally just last month’s value", "I named the features x1, x2, … x973", "I deployed with debug=True in production for 6 weeks"],
        "right": ["Fraud team", "Demand forecasting", "Former academic", "Intern who is no longer an intern"]
    },
    {
        "q": "______ is the technique of having two completely separate teams build the same model from scratch and only deploying if both agree within 0.1%.",
        "type": "fill_blank",
        "answers": ["dual-team validation", "twin-model reconciliation", "independent replication requirement"],
        "other_options": ["redundant modeling", "parallel science", "double-blind ML"]
    },
    {
        "q": "True or False: There exists a production model whose only feature is the row number modulo 7 because that accidentally worked better than everything else.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: Some central banks require that every single prediction made by an approved model is printed and archived on paper.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Match the 'deployment horror story' ending to its actual consequence:",
        "type": "match",
        "left": ["Model predicted 999999999 as price", "Confidence score was 0.999… for every input", "Latent space collapsed to a single point", "Model started returning Python NoneType"],
        "right": ["E-commerce site sold $50K TVs for $99", "Risk system approved every loan", "Recommendation system showed only one item", "API returned 500 errors with 'None'"]
    },
    {
        "q": "Rearrange these stages of a data scientist’s relationship with the business stakeholder:",
        "type": "rearrange",
        "words": ["Hero – 'You’re a wizard!'", "Partner – 'When will it be ready?'", "Vendor – 'Can you add one more feature?'", "Scapegoat – 'Why is accuracy dropping?'", "Oracle – 'The model says no, so no'", "Myth – 'Remember when we had that amazing model…'"]
    },
    {
        "q": "______ is the stage where you first hear “But it worked perfectly in the notebook!” at 3:17 AM.",
        "type": "fill_blank",
        "answers": ["Deployment"],
        "other_options": ["Monitoring", "Model Building", "Exploratory Analysis"]
    },
    {
        "q": "True or False: A company once paid $2.3 million in cloud bills because someone forgot to turn off a shadow deployment after testing.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: Some models are deployed with a hidden 'self-destruct' timestamp that automatically disables them after 5 years for license reasons.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'model name' to the team that refuses to change it:",
        "type": "match",
        "left": ["titanic_v12_final_final2_REAL", "model.pkl", "best_model_ever_do_not_touch", "magic_blackbox_2020"],
        "right": ["Solo data scientist", "Legacy system", "Team that left the company", "Model that prints money"]
    },
    {
        "q": "______ is the practice of running a weekly 'model hunger games' where all active models compete on fresh data and the bottom 10% are automatically retired.",
        "type": "fill_blank",
        "answers": ["survival of the fittest models", "model hunger games", "darwinian registry cleanup"],
        "other_options": ["model purge week", "fitness culling", "natural selection pipeline"]
    },
    {
        "q": "______ is the stage where you first learn that the gold labels were generated by a previous version of the very model you are trying to replace.",
        "type": "fill_blank",
        "answers": ["Data Collection"],
        "other_options": ["Exploratory Analysis", "Model Building", "Monitoring"]
    },
    {
        "q": "______ is the legendary model that has been in production since 2014, no one knows how it works, but removing it would cost $40 M/year.",
        "type": "fill_blank",
        "answers": ["the golden goose", "untouchable oracle", "the sacred cow"],
        "other_options": ["legacy god", "money printer", "forbidden model"]
    },
    {
        "q": "In some high-frequency trading firms, the entire Monitoring stage consists of a single metric: ______ — the Sharpe ratio over the last 100 milliseconds.",
        "type": "fill_blank",
        "answers": ["micro-Sharpe", "instant Sharpe", "100ms Sharpe"],
        "other_options": ["tick Sharpe", "nano-PnL", "edge decay"]
    },
    {
        "q": "True or False: A model can be deployed with a built-in 'sunset date' after which it automatically refuses to make predictions.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: Some telescopes run onboard ML models that decide in real time which stars are worth downlinking because bandwidth is 1 KB/s.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: You can ethically use public Reddit posts for training a commercial toxicity classifier without asking every author.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Rearrange these actual commit messages from a panicked 4 AM retraining sprint:",
        "type": "rearrange",
        "words": ["fix typo in feature name", "revert typo fix", "actually it was right the first time", "add comment explaining why we can't have nice things", "bump version to v666", "sorry"]
    },
    {
        "q": "Match the 'model archaeology find' to what the team did next:",
        "type": "match",
        "left": ["Found a feature called is_weekend that was always False", "Model had a hardcoded path to /Users/jake/", "Training script dropped 98% of data 'for speed'", "Best model was actually the null model"],
        "right": ["Quietly deleted and pretended it never existed", "Changed every laptop username to jake", "Wrote a 40-page apology report", "Celebrated and shipped it"]
    },
    {
        "q": "______ is the technique of running the model inside a sandbox that only allows safe actions (e.g., no network, no file write) in life-critical environments.",
        "type": "fill_blank",
        "answers": ["sandboxed inference", "air-gapped serving", "jailed execution"],
        "other_options": ["container lockdown", "secure enclave only", "restricted runtime"]
    },
    {
        "q": "True or False: A company once accidentally trained their fraud model on the fraud team's test transactions and achieved 99.999% recall.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: Some submarines deploy acoustic models that were trained 8 years ago because bringing new data out is classified.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'deployment superstition outcome' to the team that suffered it:",
        "type": "match",
        "left": ["Deployed on Friday the 13th", "Named model version 13", "Killed the previous model at exactly 13:13", "Model ID contained 666"],
        "right": ["Service ran perfectly for 6 years", "Nothing bad happened but everyone still blames it", "Outage exactly 13 minutes later", "Accountants refused to pay the bill"]
    },
    {
        "q": "Rearrange these increasingly desperate attempts to make a model smaller for mobile deployment:",
        "type": "rearrange",
        "words": ["8-bit quantization", "Prune 90% of weights", "Knowledge distillation to 1% size", "Train directly in TensorFlow Lite", "Rewrite the entire model in lookup tables", "Replace with if-else on device ID"]
    },
    {
        "q": "______ is the stage where a junior data scientist first learns the phrase 'we have to ship something by Friday'.",
        "type": "fill_blank",
        "answers": ["Model Building"],
        "other_options": ["Deployment", "Exploratory Analysis", "Monitoring"]
    },
    {
        "q": "True or False: There exists a production model whose only documentation is a sticky note on a monitor that reads 'DO NOT TOUCH – IT WORKS'.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: Some factories run ML models on PLCs that were programmed in ladder logic and have no floating-point unit.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'legendary data science quote' to the situation it was uttered:",
        "type": "match",
        "left": ["'The model is haunted'", "It's not a bug, it's an emergent feature'", "'We are just sophisticated guessers'", "'This is why we can't have nice things'"],
        "right": ["After the third unexplained accuracy jump", "When defending a 0.3% lift", "During a PhD student's first production incident", "When someone used pd.read_csv without dtype specification"]
    },
    {
        "q": "______ is the practice of running a secret parallel model that no one knows about just to prove the official one is garbage.",
        "type": "fill_blank",
        "answers": ["skunkworks champion", "rogue shadow model", "undercover baseline"],
        "other_options": ["stealth challenger", "black ops model", "ninja experiment"]
    },
    {
        "q": "The ______ stage is where you first define the official 'label maturity window' — how long you must wait before considering a label final.",
        "type": "fill_blank",
        "answers": ["Data Collection"],
        "other_options": ["Exploratory Analysis", "Model Building", "Monitoring"]
    },
    {
        "q": "______ is the practice of storing the exact SQL query + bind parameters that generated every training example for perfect forensic reproducibility.",
        "type": "fill_blank",
        "answers": ["query-level provenance", "SQL fingerprinting", "example-level lineage"],
        "other_options": ["deep lineage", "per-row audit", "source tracing"]
    },
    {
        "q": "______ is the stage where you first run the 'time-travel join' to reconstruct what features were actually available at prediction time in the past.",
        "type": "fill_blank",
        "answers": ["Data Preparation"],
        "other_options": ["Data Collection", "Model Building", "Deployment"]
    },
    {
        "q": "True or False: Some companies use homomorphic encryption to let the model make predictions on encrypted inputs without ever decrypting them.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: The EU AI Act classifies a spam filter as a 'minimal-risk' system that requires zero documentation or monitoring.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: In some energy trading firms, the model is retrained every 5 minutes using only data that arrived in the last 5 minutes.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange these artifacts in the order they should be versioned together to guarantee 100% reproducibility:",
        "type": "rearrange",
        "words": ["Training dataset snapshot", "Preprocessing code", "Random seeds config", "Model hyperparameters", "Training framework version pins", "Final serialized model"]
    },
    {
        "q": "Match the 'advanced drift detection method' to its core idea:",
        "type": "match",
        "left": ["Domain Classifier drift", "MMD (Maximum Mean Discrepancy)", "Learned drift detector (LSTM-based)", "Black-box shift detection"],
        "right": ["Train a classifier to distinguish train vs production", "Kernel-based statistical test", "Sequence model learns normal behavior", "Only looks at model confidence distribution"]
    },
    {
        "q": "______ is the technique of training a 'proxy model' on public data that mimics the behavior of your real model trained on private data.",
        "type": "fill_blank",
        "answers": ["model stealing proxy", "synthetic twin", "public shadow model"],
        "other_options": ["distillation proxy", "privacy mirror", "data-free clone"]
    },
    {
        "q": "True or False: Some companies run a nightly 'model hallucination test' that feeds random noise inputs and alerts if the model sounds too confident.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: The Data Preparation stage can include federated feature engineering where each client computes aggregates locally and only shares results.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'regulatory model requirement' to the industry that actually has it:",
        "type": "match",
        "left": ["Model must be monotonic in every feature", "Model must be interpretable by a human in < 2 minutes", "Model changes require 90-day public comment period", "Model must run on CPU with < 100ms latency on 10-year-old hardware"],
        "right": ["Credit scoring (US)", "Clinical decision support (EU)", "Federal Reserve SR 11-7", "Implantable medical device (FDA)"]
    },
    {
        "q": "Rearrange these steps for a proper 'model rollback' after a bad deployment:",
        "type": "rearrange",
        "words": ["Trigger traffic cutover to previous version", "Verify health checks turn green", "Notify stakeholders incident resolved", "Start root-cause investigation", "Add new automated test to prevent recurrence", "Write post-mortem"]
    },
    {
        "q": "______ is the stage where you first benchmark 'cold cache' latency — how long the first prediction takes after a server restart.",
        "type": "fill_blank",
        "answers": ["Deployment"],
        "other_options": ["Model Building", "Monitoring", "Data Preparation"]
    },
    {
        "q": "True or False: Some companies use private set intersection (PSI) protocols during Data Collection to combine datasets without revealing, revealing raw records.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "True or False: In certain safety-critical systems, the model binary is formally verified with mathematical proofs before deployment.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the 'cost optimization technique' to the lifecycle stage where it saves the most money:",
        "type": "match",
        "left": ["Spot instances for training", "Model distillation to smaller size", "Batch prediction instead of real-time API", "Feature store caching"],
        "right": ["Model Building", "Deployment", "Deployment", "Data Preparation"]
    },
    {
        "q": "______ is the practice of automatically generating a 'model obituary' when a version is retired, listing its peak performance and cause of death.",
        "type": "fill_blank",
        "answers": ["model obituary", "retirement summary", "version eulogy"],
        "other_options": ["death certificate", "final report", "sunset note"]
    },
    {
        "q": "Which data collection method is most likely to introduce sampling bias?",
        "type": "mcq",
        "o": [
            "Convenience sampling",
            "Random sampling",
            "Stratified sampling",
            "Systematic sampling"
        ]
    },
    {
        "q": "What is the primary goal of the data preparation phase?",
        "type": "mcq",
        "o": [
            "To transform raw data into a clean, usable format",
            "To build predictive models",
            "To deploy models to production",
            "To visualize data patterns"
        ]
    },
    {
        "q": "What is the output of the following code?",
        "type": "mcq",
        "c": "import pandas as pd\nprint(pd.isnull([1, None, 3]))",
        "o": [
            "[False, True, False]",
            "[True, False, True]",
            "[1, None, 3]",
            "Error"
        ]
    },
    {
        "q": "The process of handling missing values, outliers, and data normalization occurs during the ______ phase.",
        "type": "fill_blank",
        "answers": ["data preparation"],
        "other_options": ["data collection", "model building", "deployment"]
    },
    {
        "q": "Match the data science lifecycle phases with their primary activities:",
        "type": "match",
        "left": ["Data Collection", "Exploratory Analysis", "Model Building", "Monitoring"],
        "right": ["Gathering raw data from sources", "Identifying patterns and relationships", "Training machine learning algorithms", "Tracking model performance in production"]
    },
    {
        "q": "A/B testing is commonly used during the model monitoring phase.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which visualization is most appropriate for identifying correlations between multiple numerical variables?",
        "type": "mcq",
        "o": [
            "Correlation matrix heatmap",
            "Bar chart",
            "Pie chart",
            "Line plot"
        ]
    },
    {
        "q": "What is the output of the following code?",
        "type": "mcq",
        "c": "from sklearn.model_selection import train_test_split\nprint(train_test_split([1,2,3,4], test_size=0.25)[1].size)",
        "o": [
            "1",
            "2",
            "3",
            "4"
        ]
    },
    {
        "q": "Rearrange the data science lifecycle phases in correct order:",
        "type": "rearrange",
        "words": ["Data Collection", "Data Preparation", "Exploratory Analysis", "Model Building", "Deployment", "Monitoring"]
    },
    {
        "q": "Feature engineering is typically performed during the ______ phase.",
        "type": "fill_blank",
        "answers": ["data preparation"],
        "other_options": ["data collection", "deployment", "monitoring"]
    },
    {
        "q": "Which metric is most important for monitoring model performance in production?",
        "type": "mcq",
        "o": [
            "All of the above",
            "Prediction accuracy",
            "Response time",
            "Data drift detection"
        ]
    },
    {
        "q": "Cross-validation is used during model evaluation to assess how well a model will generalize to unseen data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the primary purpose of exploratory data analysis (EDA)?",
        "type": "mcq",
        "o": [
            "To understand data patterns and generate hypotheses",
            "To build production-ready models",
            "To deploy models to cloud platforms",
            "To collect data from APIs"
        ]
    },
    {
        "q": "Match the deployment strategies with their descriptions:",
        "type": "match",
        "left": ["Canary Deployment", "Blue-Green Deployment", "Shadow Deployment", "A/B Testing"],
        "right": ["Gradual rollout to small user group", "Two identical environments switched instantly", "Model runs parallel to production without affecting users", "Comparing two versions with different user groups"]
    },
    {
        "q": "What is the output of the following code?",
        "type": "mcq",
        "c": "from sklearn.metrics import accuracy_score\nprint(accuracy_score([1,0,1,1], [1,0,0,1]))",
        "o": [
            "0.75",
            "0.5",
            "0.25",
            "1.0"
        ]
    },
    {
        "q": "Model ______ involves retraining models with new data to maintain performance over time.",
        "type": "fill_blank",
        "answers": ["retraining"],
        "other_options": ["deployment", "collection", "exploration"]
    },
    {
        "q": "Which technique is NOT typically used for handling missing data?",
        "type": "mcq",
        "o": [
            "Data duplication",
            "Mean/median imputation",
            "KNN imputation",
            "Dropping missing values"
        ]
    },
    {
        "q": "Rearrange the steps for building a machine learning model:",
        "type": "rearrange",
        "words": ["Feature Selection", "Model Training", "Hyperparameter Tuning", "Model Evaluation"]
    },
    {
        "q": "Continuous integration and continuous deployment (CI/CD) pipelines are essential for modern data science deployment.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which tool is primarily used for monitoring data quality and model drift?",
        "type": "mcq",
        "o": [
            "All of the above",
            "Evidently AI",
            "Great Expectations",
            "MLflow"
        ]
    },
    {
        "q": "In a healthcare project predicting patient readmission, which data collection method would raise the MOST ethical concerns?",
        "type": "mcq",
        "o": [
            "Collecting patient social media data without explicit consent",
            "Using anonymized electronic health records with IRB approval",
            "Conducting patient surveys with informed consent",
            "Analyzing publicly available health statistics"
        ]
    },
    {
        "q": "Rearrange the data preprocessing steps for text data in correct order:",
        "type": "rearrange",
        "words": ["Tokenization", "Stopword Removal", "Stemming", "TF-IDF Vectorization"]
    },
    {
        "q": "During exploratory analysis of customer churn data, you discover that 95% of your dataset comes from users in North America, but your product is global. This represents a significant ______ problem.",
        "type": "fill_blank",
        "answers": ["geographic representation"],
        "other_options": ["data cleaning", "model training", "feature engineering"]
    },
    {
        "q": "Match the model evaluation metrics with their appropriate use cases:",
        "type": "match",
        "left": ["F1-Score", "RMSE", "ROC-AUC", "Mean Absolute Percentage Error"],
        "right": ["Imbalanced classification problems", "Regression model accuracy", "Binary classification performance", "Forecasting error relative to scale"]
    },
    {
        "q": "What is the output of the following code analyzing feature importance?",
        "type": "mcq",
        "c": "import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfeatures = ['age', 'income', 'education']\nimportance = [0.15, 0.60, 0.25]\nprint(features[importance.index(max(importance))])",
        "o": [
            "income",
            "age",
            "education",
            "0.60"
        ]
    },
    {
        "q": "Containerization using Docker is essential for ensuring model reproducibility across different environments.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "You're building a fraud detection system for credit card transactions. Which deployment strategy would minimize business risk while testing the new model?",
        "type": "mcq",
        "o": [
            "Shadow deployment where predictions are logged but not acted upon",
            "Immediate full replacement of the existing system",
            "No testing - deploy directly to all users",
            "Deploy only during non-business hours"
        ]
    },
    {
        "q": "When monitoring a recommendation system, a sudden drop in click-through rate might indicate ______.",
        "type": "fill_blank",
        "answers": ["model drift"],
        "other_options": ["data collection", "feature engineering", "data cleaning"]
    },
    {
        "q": "What is the output of the following data validation code?",
        "type": "mcq",
        "c": "data = {'age': [25, -5, 30], 'income': [50000, 60000, -1000]}\ndf = pd.DataFrame(data)\ninvalid_age = (df['age'] < 0).sum()\ninvalid_income = (df['income'] < 0).sum()\nprint(f\"Invalid records: {invalid_age + invalid_income}\")",
        "o": [
            "Invalid records: 2",
            "Invalid records: 1",
            "Invalid records: 3",
            "Invalid records: 0"
        ]
    },
    {
        "q": "Rearrange the steps for addressing data quality issues:",
        "type": "rearrange",
        "words": ["Identify Anomalies", "Determine Root Cause", "Implement Fix", "Validate Results"]
    },
    {
        "q": "In time-series forecasting, using future information to predict past events constitutes data leakage.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the data science challenges with their appropriate solutions:",
        "type": "match",
        "left": ["Class Imbalance", "High Dimensionality", "Concept Drift", "Cold Start Problem"],
        "right": ["SMOTE or weighted loss functions", "PCA or feature selection", "Periodic model retraining", "Content-based or hybrid recommendations"]
    },
    {
        "q": "A retail company wants to predict inventory demand. Which data source would provide the MOST valuable external signals?",
        "type": "mcq",
        "o": [
            "Local weather patterns and holiday calendars",
            "Employee shift schedules",
            "Office supply inventory levels",
            "HR department meeting schedules"
        ]
    },
    {
        "q": "When preparing image data for a computer vision model, converting all images to the same dimensions and normalizing pixel values is called ______.",
        "type": "fill_blank",
        "answers": ["standardization"],
        "other_options": ["augmentation", "validation", "collection"]
    },
    {
        "q": "What is the output of the following hyperparameter tuning code?",
        "type": "mcq",
        "c": "from sklearn.model_selection import GridSearchCV\nparams = {'max_depth': [3, 5], 'min_samples_leaf': [1, 2]}\nprint(f\"Total combinations: {len(params['max_depth']) * len(params['min_samples_leaf'])}\")",
        "o": [
            "Total combinations: 4",
            "Total combinations: 2",
            "Total combinations: 6",
            "Total combinations: 8"
        ]
    },
    {
        "q": "Feature stores help maintain consistency between features used during training and inference.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "You notice your production model's performance degrades every quarter. The MOST likely cause is:",
        "type": "mcq",
        "o": [
            "Seasonal pattern changes affecting data distributions",
            "Server hardware performance degradation",
            "Development team turnover",
            "Office network connectivity issues"
        ]
    },
    {
        "q": "Rearrange the MLOps workflow stages:",
        "type": "rearrange",
        "words": ["Experiment Tracking", "Model Registry", "Continuous Training", "Pipeline Orchestration"]
    },
    {
        "q": "When analyzing customer segmentation results, the ______ metric helps determine the optimal number of clusters.",
        "type": "fill_blank",
        "answers": ["silhouette score"],
        "other_options": ["accuracy", "precision", "recall"]
    },
    {
        "q": "In a multi-tenant SaaS application, implementing ______ ensures that one customer's data doesn't leak into another customer's predictions.",
        "type": "mcq",
        "o": [
            "Data isolation and tenant-aware feature engineering",
            "Larger batch sizes during inference",
            "Faster API response times",
            "More complex neural network architectures"
        ]
    },
    {
        "q": "The process of ______ involves creating new features from existing ones to improve model performance, such as calculating ratios or interaction terms.",
        "type": "fill_blank",
        "answers": ["feature engineering"],
        "other_options": ["data collection", "model deployment", "performance monitoring"]
    },
    {
        "q": "Rearrange the steps for implementing a data versioning system:",
        "type": "rearrange",
        "words": ["Data Profiling", "Schema Validation", "Version Snapshot", "Change Tracking"]
    },
    {
        "q": "Match the data governance concept with its definition:",
        "type": "match",
        "left": ["Data Lineage", "Data Provenance", "Data Stewardship", "Data Catalog"],
        "right": ["Tracking data origin and transformations", "Historical record of data ownership", "Responsibility for data quality", "Inventory of available data assets"]
    },
    {
        "q": "Using differential privacy techniques in data collection always significantly reduces model accuracy.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "What is the output of the following data drift detection code?",
        "type": "mcq",
        "c": "from scipy import stats\nimport numpy as np\ntraining_data = np.random.normal(0, 1, 1000)\nproduction_data = np.random.normal(0.5, 1, 1000)\np_value = stats.ks_2samp(training_data, production_data).pvalue\nprint('Drift detected:' if p_value < 0.05 else 'No significant drift')",
        "o": [
            "Drift detected:",
            "No significant drift",
            "0.05",
            "Error in calculation"
        ]
    },
    {
        "q": "In federated learning, model training occurs:",
        "type": "mcq",
        "o": [
            "Locally on distributed devices without sharing raw data",
            "Only on centralized cloud servers",
            "Using publicly available datasets only",
            "Without any data preprocessing steps"
        ]
    },
    {
        "q": "The practice of ______ involves systematically testing data pipelines with synthetic data to validate transformation logic.",
        "type": "fill_blank",
        "answers": ["data pipeline testing"],
        "other_options": ["model monitoring", "feature selection", "hyperparameter tuning"]
    },
    {
        "q": "Rearrange the data annotation workflow for supervised learning:",
        "type": "rearrange",
        "words": ["Annotation Guidelines", "Quality Control", "Inter-annotator Agreement", "Label Reconciliation"]
    },
    {
        "q": "Model cards should include information about intended use cases, limitations, and ethical considerations.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which technique is most effective for detecting label drift in classification models?",
        "type": "mcq",
        "o": [
            "Monitoring changes in class distribution over time",
            "Increasing model complexity",
            "Adding more training data",
            "Using larger batch sizes"
        ]
    },
    {
        "q": "Match the data storage format with its optimal use case:",
        "type": "match",
        "left": ["Parquet", "Avro", "Delta Lake", "Feather"],
        "right": ["Analytical queries on large datasets", "Schema evolution in streaming data", "ACID transactions on data lakes", "Fast read/write for intermediate data"]
    },
    {
        "q": "What is the output of the following feature importance permutation code?",
        "type": "mcq",
        "c": "import numpy as np\nbaseline_score = 0.85\npermuted_scores = [0.82, 0.84, 0.83, 0.81]\nimportance = baseline_score - np.mean(permuted_scores)\nprint(f'Feature importance: {importance:.3f}')",
        "o": [
            "Feature importance: 0.025",
            "Feature importance: 0.850",
            "Feature importance: 0.825",
            "Feature importance: 0.000"
        ]
    },
    {
        "q": "In MLOps, ______ enables automatic retraining of models when performance metrics degrade below thresholds.",
        "type": "fill_blank",
        "answers": ["continuous training"],
        "other_options": ["data validation", "feature store", "model registry"]
    },
    {
        "q": "Synthetic data generation can help address data scarcity while preserving privacy.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which approach is most suitable for explaining black-box model predictions to non-technical stakeholders?",
        "type": "mcq",
        "o": [
            "Local interpretable model-agnostic explanations (LIME)",
            "SHAP values with complex dependency plots",
            "Raw feature importance scores",
            "Model architecture diagrams"
        ]
    },
    {
        "q": "Rearrange the steps for conducting a bias audit:",
        "type": "rearrange",
        "words": ["Define Protected Attributes", "Measure Model Fairness", "Identify Disparate Impact", "Implement Mitigation Strategies"]
    },
    {
        "q": "The ______ pattern separates feature computation from model serving to ensure consistency between training and inference.",
        "type": "fill_blank",
        "answers": ["feature store"],
        "other_options": ["model registry", "data catalog", "pipeline orchestration"]
    },
    {
        "q": "Match the data quality dimension with its monitoring approach:",
        "type": "match",
        "left": ["Completeness", "Consistency", "Timeliness", "Accuracy"],
        "right": ["Percentage of non-null values", "Adherence to business rules", "Data freshness metrics", "Comparison against ground truth"]
    },
    {
        "q": "What is the output of the following data validation rule check?",
        "type": "mcq",
        "c": "data = {'age': [25, 130, 30], 'email': ['a@b.com', 'invalid', 'c@d.com']}\ninvalid_age = sum(1 for x in data['age'] if x > 120)\ninvalid_email = sum(1 for x in data['email'] if '@' not in x)\nprint(f'Total violations: {invalid_age + invalid_email}')",
        "o": [
            "Total violations: 2",
            "Total violations: 1",
            "Total violations: 3",
            "Total violations: 0"
        ]
    },
    {
        "q": "Model quantization reduces model size and inference latency without necessarily sacrificing significant accuracy.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Match the advanced monitoring technique with its primary purpose:",
        "type": "match",
        "left": ["Concept Drift Detection", "Data Drift Detection", "Model Decay Monitoring", "Prediction Distribution Analysis"],
        "right": ["Identifies changing relationships between features and target", "Detects changes in input data distribution", "Tracks declining model performance over time", "Analyzes shifts in model output patterns"]
    },
    {
        "q": "In a multi-armed bandit deployment approach, the exploration-exploitation tradeoff is automatically managed during model serving.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Rearrange the workflow for implementing differential privacy:",
        "type": "rearrange",
        "words": ["Define Privacy Budget", "Calculate Sensitivity", "Add Calibrated Noise", "Validate Privacy Guarantees"]
    },
    {
        "q": "The practice of creating ______ datasets involves generating multiple versions of data with different statistical properties to test model robustness.",
        "type": "fill_blank",
        "answers": ["synthetic"],
        "other_options": ["validation", "training", "monitoring"]
    },
    {
        "q": "What is the primary purpose of a shadow model in deployment?",
        "type": "mcq",
        "o": [
            "To run parallel to production model without affecting business decisions",
            "To reduce computational costs during inference",
            "To serve as a backup during system failures",
            "To improve model accuracy through ensemble methods"
        ]
    },
    {
        "q": "When implementing continuous validation, data ______ checks ensure incoming data matches expected schema and value ranges before processing.",
        "type": "fill_blank",
        "answers": ["quality"],
        "other_options": ["quantity", "velocity", "variety"]
    },
    {
        "q": "Match the model interpretability technique with its complexity level:",
        "type": "match",
        "left": ["Feature Importance", "Partial Dependence Plots", "SHAP Values", "Counterfactual Explanations"],
        "right": ["Simple global interpretation", "Medium complexity global", "Complex local explanations", "Advanced what-if analysis"]
    },
    {
        "q": "Rearrange the steps for building a model monitoring dashboard:",
        "type": "rearrange",
        "words": ["Define Key Metrics", "Set Alert Thresholds", "Implement Data Collection", "Create Visualization Layout"]
    },
    {
        "q": "What is the output of the following causal impact analysis simulation?",
        "type": "mcq",
        "c": "import numpy as np\nbaseline_conversion = 0.15\ntreatment_conversion = 0.18\nlift = (treatment_conversion - baseline_conversion) / baseline_conversion\nprint(f'Lift: {lift:.1%}')",
        "o": [
            "Lift: 20.0%",
            "Lift: 3.0%",
            "Lift: 18.0%",
            "Lift: 15.0%"
        ]
    },
    {
        "q": "Using knowledge distillation, a smaller student model can achieve similar performance to a larger teacher model while being more deployment-friendly.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In edge computing deployments, model ______ techniques reduce size and computational requirements for resource-constrained devices.",
        "type": "fill_blank",
        "answers": ["optimization"],
        "other_options": ["training", "monitoring", "validation"]
    },
    {
        "q": "Which strategy is most effective for handling categorical variables with high cardinality?",
        "type": "mcq",
        "o": [
            "Target encoding with regularization",
            "One-hot encoding all categories",
            "Label encoding without preprocessing",
            "Dropping high-cardinality features"
        ]
    },
    {
        "q": "Match the advanced feature engineering technique with its application:",
        "type": "match",
        "left": ["Polynomial Features", "Interaction Terms", "Binning", "Embedding Layers"],
        "right": ["Captures non-linear relationships", "Models feature interdependencies", "Handles continuous variable non-linearity", "Learns categorical representations"]
    },
    {
        "q": "Rearrange the model serving optimization techniques by deployment complexity:",
        "type": "rearrange",
        "words": ["Model Quantization", "Hardware Acceleration", "Model Pruning", "Knowledge Distillation"]
    },
    {
        "q": "What is the primary benefit of using ensemble methods in production systems?",
        "type": "mcq",
        "o": [
            "Improved robustness and reduced variance",
            "Faster inference times",
            "Simpler model interpretation",
            "Reduced storage requirements"
        ]
    },
    {
        "q": "Automated feature selection using recursive elimination always improves model performance by reducing overfitting.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "The process of ______ involves systematically testing model performance across different demographic segments to ensure fairness.",
        "type": "fill_blank",
        "answers": ["bias auditing"],
        "other_options": ["model training", "data collection", "feature engineering"]
    },
    {
        "q": "What is the output of the following anomaly detection threshold calculation?",
        "type": "mcq",
        "c": "import numpy as np\nscores = [0.1, 0.2, 0.15, 0.25, 2.5, 0.18]\nthreshold = np.percentile(scores, 95)\nprint(f'Anomaly threshold: {threshold:.2f}')",
        "o": [
            "Anomaly threshold: 1.48",
            "Anomaly threshold: 0.25",
            "Anomaly threshold: 2.50",
            "Anomaly threshold: 0.95"
        ]
    },
    {
        "q": "Match the data pipeline architecture with its characteristic:",
        "type": "match",
        "left": ["Lambda Architecture", "Kappa Architecture", "Microservices Architecture", "Event-Driven Architecture"],
        "right": ["Batch and stream processing layers", "Stream processing only", "Loosely coupled specialized services", "Message-based asynchronous communication"]
    },
    {
        "q": "In production ML systems, ______ versioning tracks changes to data, code, and models to ensure reproducibility.",
        "type": "fill_blank",
        "answers": ["artifact"],
        "other_options": ["software", "data", "model"]
    },
    {
        "q": "Match the specialized data collection scenario with its primary ethical consideration:",
        "type": "match",
        "left": ["IoT Sensor Networks", "Social Media Scraping", "Medical Imaging", "Financial Transaction Monitoring"],
        "right": ["Informed consent for continuous monitoring", "Terms of service compliance and user privacy", "HIPAA compliance and patient confidentiality", "PCI DSS compliance and fraud detection boundaries"]
    },
    {
        "q": "Rearrange the steps for implementing federated learning:",
        "type": "rearrange",
        "words": ["Initialize Global Model", "Distribute to Clients", "Local Training", "Aggregate Updates", "Model Validation"]
    },
    {
        "q": "Quantum machine learning algorithms require completely different data preparation pipelines than classical approaches.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "In graph neural networks, the process of ______ involves sampling neighboring nodes to handle large-scale graphs efficiently.",
        "type": "fill_blank",
        "answers": ["neighborhood sampling"],
        "other_options": ["feature engineering", "data collection", "model deployment"]
    },
    {
        "q": "What is the primary challenge when deploying models for real-time audio processing?",
        "type": "mcq",
        "o": [
            "Latency constraints and streaming data handling",
            "Lack of available audio datasets",
            "Insufficient model accuracy metrics",
            "Overfitting on training data"
        ]
    },
    {
        "q": "Match the edge computing deployment scenario with its optimal strategy:",
        "type": "match",
        "left": ["Autonomous Vehicles", "Smart Agriculture Sensors", "Industrial IoT", "Healthcare Wearables"],
        "right": ["Model partitioning between edge and cloud", "Lightweight models with periodic updates", "Real-time anomaly detection with cloud logging", "Privacy-preserving local processing"]
    },
    {
        "q": "The technique of ______ augmentation generates realistic training examples by applying domain-specific transformations to existing data.",
        "type": "fill_blank",
        "answers": ["data"],
        "other_options": ["model", "feature", "algorithm"]
    },
    {
        "q": "What is the output of the following time-series cross-validation code?",
        "type": "mcq",
        "c": "from sklearn.model_selection import TimeSeriesSplit\nimport numpy as np\ntscv = TimeSeriesSplit(n_splits=3)\ndata = np.array([1, 2, 3, 4, 5, 6])\nsplits = list(tscv.split(data))\nprint(f'First test size: {len(splits[0][1])}')",
        "o": [
            "First test size: 1",
            "First test size: 2",
            "First test size: 3",
            "First test size: 6"
        ]
    },
    {
        "q": "Rearrange the workflow for building an explainable AI system:",
        "type": "rearrange",
        "words": ["Define Explanation Requirements", "Select Interpretability Methods", "Integrate with Model Pipeline", "Validate Explanation Quality", "Deploy with Explanation API"]
    },
    {
        "q": "Synthetic data generation using GANs can perfectly replicate all statistical properties of real-world data without any limitations.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "In reinforcement learning deployment, the ______ problem balances trying new actions versus exploiting known good actions.",
        "type": "fill_blank",
        "answers": ["exploration-exploitation"],
        "other_options": ["overfitting-underfitting", "bias-variance", "training-inference"]
    },
    {
        "q": "Which approach is most suitable for monitoring model performance in unsupervised learning scenarios?",
        "type": "mcq",
        "o": [
            "Tracking cluster stability and silhouette scores over time",
            "Monitoring accuracy and precision metrics",
            "Measuring F1-score degradation",
            "Validating against labeled test sets"
        ]
    },
    {
        "q": "Match the specialized data type with its unique preprocessing requirement:",
        "type": "match",
        "left": ["3D Point Clouds", "Genomic Sequences", "Satellite Imagery", "Time-Series Sensors"],
        "right": ["Voxelization or point net architectures", "K-mer counting and sequence alignment", "Atmospheric correction and radiometric calibration", "Missing value imputation and trend removal"]
    },
    {
        "q": "The process of ______ learning transfers knowledge from related tasks to improve performance on data-scarce target tasks.",
        "type": "fill_blank",
        "answers": ["transfer"],
        "other_options": ["supervised", "unsupervised", "reinforcement"]
    },
    {
        "q": "What is the primary consideration when deploying models for high-frequency trading systems?",
        "type": "mcq",
        "o": [
            "Microsecond-level latency requirements",
            "Model interpretability for regulators",
            "Training data quantity",
            "Feature engineering complexity"
        ]
    },
    {
        "q": "Rearrange the steps for implementing a model governance framework:",
        "type": "rearrange",
        "words": ["Define Model Inventory", "Establish Approval Workflows", "Implement Version Control", "Set Performance Benchmarks", "Create Audit Trails"]
    },
    {
        "q": "Federated learning completely eliminates all privacy concerns associated with distributed model training.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "In computer vision deployment, ______ optimization reduces model size while maintaining accuracy for mobile applications.",
        "type": "fill_blank",
        "answers": ["model"],
        "other_options": ["data", "feature", "algorithm"]
    },
    {
        "q": "Which technique is most effective for handling concept drift in streaming data applications?",
        "type": "mcq",
        "o": [
            "Online learning with adaptive model updates",
            "Increasing model complexity",
            "Adding more historical training data",
            "Using larger batch sizes"
        ]
    },
    {
        "q": "In production ML systems, implementing ______ circuits allows models to gracefully degrade when certain features become unavailable.",
        "type": "fill_blank",
        "answers": ["circuit breakers"],
        "other_options": ["monitoring alerts", "retraining pipelines", "feature stores"]
    },
    {
        "q": "Rearrange the steps for conducting a counterfactual fairness analysis:",
        "type": "rearrange",
        "words": ["Define Protected Attributes", "Generate Counterfactual Data", "Measure Prediction Differences", "Implement Fairness Constraints"]
    },
    {
        "q": "Match the emerging data paradigm with its data preparation challenge:",
        "type": "match",
        "left": ["Neuromorphic Computing", "Digital Twins", "Metaverse Applications", "Swarm Robotics"],
        "right": ["Event-based data streaming", "Real-time sensor fusion", "3D spatial data alignment", "Distributed coordination signals"]
    },
    {
        "q": "Homomorphic encryption allows model training on encrypted data without decryption, but significantly increases computational overhead.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "What is the primary purpose of implementing dark launches in ML deployment?",
        "type": "mcq",
        "o": [
            "To test new models with live traffic without exposing users to changes",
            "To reduce model serving costs during low-traffic periods",
            "To improve model interpretability for stakeholders",
            "To accelerate feature engineering pipelines"
        ]
    },
    {
        "q": "The practice of ______ learning uses unlabeled data to improve model robustness by creating self-supervised pretext tasks.",
        "type": "fill_blank",
        "answers": ["self-supervised"],
        "other_options": ["supervised", "semi-supervised", "transfer"]
    },
    {
        "q": "What is the output of the following causal inference sensitivity analysis?",
        "type": "mcq",
        "c": "import numpy as np\nbaseline_effect = 0.15\nunmeasured_confounding = 0.05\nadjusted_effect = baseline_effect - unmeasured_confounding\nprint(f'Robust effect: {adjusted_effect:.2f}')",
        "o": [
            "Robust effect: 0.10",
            "Robust effect: 0.15",
            "Robust effect: 0.20",
            "Robust effect: 0.05"
        ]
    },
    {
        "q": "In multi-modal learning, the ______ fusion approach processes each data type separately before combining representations.",
        "type": "fill_blank",
        "answers": ["late"],
        "other_options": ["early", "hybrid", "cross-modal"]
    },
    {
        "q": "Rearrange the workflow for building a responsible AI monitoring system:",
        "type": "rearrange",
        "words": ["Define Fairness Metrics", "Monitor Demographic Parity", "Track Model Impact", "Implement Bias Mitigation", "Document Decisions"]
    },
    {
        "q": "Model soups created by averaging weights from multiple training runs always improve performance without any trade-offs.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which technique is most effective for detecting adversarial attacks on computer vision models in production?",
        "type": "mcq",
        "o": [
            "Monitoring prediction confidence distributions and outlier detection",
            "Increasing model complexity and parameters",
            "Adding more training data from the same distribution",
            "Using simpler linear models instead of neural networks"
        ]
    },
    {
        "q": "Match the specialized deployment architecture with its use case:",
        "type": "match",
        "left": ["Model Mesh", "BentoML", "KFServing", "Triton Inference Server"],
        "right": ["Multi-model serving management", "Unified model packaging format", "Kubernetes-native model serving", "High-performance inference optimization"]
    },
    {
        "q": "The process of ______ distillation transfers knowledge from ensemble models to single models while maintaining performance.",
        "type": "fill_blank",
        "answers": ["knowledge"],
        "other_options": ["model", "data", "feature"]
    },
    {
        "q": "What is the primary consideration when deploying models for clinical decision support systems?",
        "type": "mcq",
        "o": [
            "Regulatory compliance and interpretability for medical professionals",
            "Maximizing prediction speed above all other factors",
            "Minimizing model size for storage efficiency",
            "Using the most complex available architecture"
        ]
    },
    {
        "q": "In continual learning systems, ______ replay stores representative examples from previous tasks to prevent catastrophic forgetting.",
        "type": "fill_blank",
        "answers": ["experience"],
        "other_options": ["data", "model", "feature"]
    },
    {
        "q": "Rearrange the steps for implementing a data-centric AI approach:",
        "type": "rearrange",
        "words": ["Analyze Data Quality", "Systematically Improve Data", "Iterate on Data Collection", "Monitor Data Drift", "Update Data Strategies"]
    },
    {
        "q": "Differential privacy guarantees become stronger when applied multiple times to the same dataset.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which approach best addresses the challenge of dataset shift in production ML systems?",
        "type": "mcq",
        "o": [
            "Implementing robust monitoring and adaptive retraining strategies",
            "Using larger and more complex model architectures",
            "Increasing the frequency of model deployment",
            "Reducing feature engineering complexity"
        ]
    },
    {
        "q": "Match the advanced model debugging technique with its application:",
        "type": "match",
        "left": ["Influence Functions", "Integrated Gradients", "Anchors", "Prototype Analysis"],
        "right": ["Identifying influential training examples", "Feature attribution for deep networks", "High-precision rule-based explanations", "Understanding model internal representations"]
    },
    {
        "q": "The ______ learning paradigm uses human feedback to align model behavior with human values and preferences.",
        "type": "fill_blank",
        "answers": ["reinforcement learning from human feedback"],
        "other_options": ["supervised", "unsupervised", "self-supervised"]
    },
    {
        "q": "Match the data-centric AI technique with its primary focus:",
        "type": "match",
        "left": ["Data Cartography", "Data Shapley", "Dataset Condensation", "Active Learning"],
        "right": ["Mapping dataset examples by difficulty and confidence", "Valuing individual data points' contributions", "Creating smaller representative datasets", "Intelligently selecting data for labeling"]
    },
    {
        "q": "Rearrange the steps for implementing a causality-aware ML pipeline:",
        "type": "rearrange",
        "words": ["Define Causal Graph", "Identify Confounders", "Select Causal Estimator", "Validate Causal Assumptions", "Measure Treatment Effects"]
    },
    {
        "q": "Foundation models require the same data preprocessing steps as traditional machine learning models.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "In production ML systems, ______ testing validates model behavior under extreme but plausible input conditions.",
        "type": "fill_blank",
        "answers": ["stress"],
        "other_options": ["unit", "integration", "smoke"]
    },
    {
        "q": "What is the primary innovation of retrieval-augmented generation (RAG) in deployment?",
        "type": "mcq",
        "o": [
            "Combining parametric knowledge with real-time information retrieval",
            "Reducing model size through aggressive quantization",
            "Eliminating the need for any training data",
            "Making models completely deterministic"
        ]
    },
    {
        "q": "The process of ______ alignment ensures AI systems pursue intended goals without unintended side effects.",
        "type": "fill_blank",
        "answers": ["value"],
        "other_options": ["model", "data", "feature"]
    },
    {
        "q": "What is the output of the following conformal prediction calibration?",
        "type": "mcq",
        "c": "import numpy as np\nscores = [0.1, 0.3, 0.05, 0.4, 0.25]\nquantile = np.quantile(scores, 0.9)\nprint(f'90% prediction threshold: {quantile:.2f}')",
        "o": [
            "90% prediction threshold: 0.37",
            "90% prediction threshold: 0.40",
            "90% prediction threshold: 0.90",
            "90% prediction threshold: 0.25"
        ]
    },
    {
        "q": "Match the emerging data type with its unique lifecycle challenge:",
        "type": "match",
        "left": ["Biometric Data", "Blockchain Transactions", "Brain-Computer Interfaces", "Quantum Sensor Output"],
        "right": ["Privacy preservation and regulatory compliance", "Immutable audit trails and scalability", "Real-time processing and ethical considerations", "Quantum-classical data conversion and noise filtering"]
    },
    {
        "q": "Rearrange the workflow for building a trustworthy AI system:",
        "type": "rearrange",
        "words": ["Establish Trust Principles", "Implement Transparency", "Enable Contestability", "Document Limitations", "Provide Redress Mechanisms"]
    },
    {
        "q": "Sparse expert models (mixture of experts) always reduce inference latency compared to dense models of similar parameter count.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which technique addresses the simulation-to-reality gap in robotic learning systems?",
        "type": "mcq",
        "o": [
            "Domain randomization and progressive neural networks",
            "Increasing simulation fidelity exclusively",
            "Using larger batch sizes during training",
            "Eliminating all real-world testing"
        ]
    },
    {
        "q": "In multi-agent reinforcement learning, the ______ problem occurs when agents must learn coordinated behaviors.",
        "type": "fill_blank",
        "answers": ["credit assignment"],
        "other_options": ["exploration", "overfitting", "deployment"]
    },
    {
        "q": "What is the primary benefit of using neural architecture search (NAS) in model development?",
        "type": "mcq",
        "o": [
            "Automating the discovery of optimal model architectures",
            "Eliminating the need for any hyperparameter tuning",
            "Guaranteeing perfect model interpretability",
            "Removing all data preprocessing requirements"
        ]
    },
    {
        "q": "The practice of ______ testing validates model robustness against distribution shifts not seen in training.",
        "type": "fill_blank",
        "answers": ["out-of-distribution"],
        "other_options": ["unit", "integration", "regression"]
    },
    {
        "q": "Match the AI safety technique with its mechanism:",
        "type": "match",
        "left": ["Constitutional AI", "Recursive Reward Modeling", "Debate", "Amortized Bayesian Inference"],
        "right": ["Training models to follow principles and critique responses", "Iteratively refining reward models from human preferences", "Using adversarial debates to surface best arguments", "Approximating Bayesian updates for uncertainty quantification"]
    },
    {
        "q": "Rearrange the steps for implementing a privacy-preserving ML system:",
        "type": "rearrange",
        "words": ["Conduct Privacy Risk Assessment", "Select Privacy Technology", "Implement Data Minimization", "Audit Privacy Guarantees", "Establish Breach Protocols"]
    },
    {
        "q": "Liquid neural networks with continuous-time hidden states are primarily designed for processing static, tabular data.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which approach best addresses the challenge of compositional generalization in language models?",
        "type": "mcq",
        "o": [
            "Systematic prompting and structured training data",
            "Increasing model parameters without bound",
            "Reducing vocabulary size significantly",
            "Eliminating all syntactic parsing"
        ]
    },
    {
        "q": "In scientific ML, ______ learning incorporates physical laws and constraints into model architectures.",
        "type": "fill_blank",
        "answers": ["physics-informed"],
        "other_options": ["supervised", "unsupervised", "self-supervised"]
    },
    {
        "q": "What is the primary innovation of diffusion models in generative AI?",
        "type": "mcq",
        "o": [
            "Iterative denoising process from random noise to structured data",
            "Eliminating all training data requirements",
            "Generating outputs in a single forward pass",
            "Using only reinforcement learning for training"
        ]
    },
    {
        "q": "Match the neuromorphic computing concept with its data science implication:",
        "type": "match",
        "left": ["Spiking Neural Networks", "Memristor Crossbars", "Event-Based Vision", "Neuromorphic Sensors"],
        "right": ["Time-encoded sparse data processing", "In-memory computation reducing data movement", "Processing only pixel changes not full frames", "Edge sensing with minimal power consumption"]
    },
    {
        "q": "Rearrange the workflow for implementing a digital twin system:",
        "type": "rearrange",
        "words": ["Create Physical Asset Model", "Establish Real-Time Data Feeds", "Build Predictive Simulation", "Implement Control Loops", "Validate Against Physical System"]
    },
    {
        "q": "Causal forest algorithms can automatically discover causal relationships without any prior domain knowledge.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "In geometric deep learning, ______ networks process data with inherent symmetry and structure like graphs and manifolds.",
        "type": "fill_blank",
        "answers": ["equivariant"],
        "other_options": ["convolutional", "recurrent", "transformers"]
    },
    {
        "q": "What is the primary challenge when deploying models for real-time scientific computing applications?",
        "type": "mcq",
        "o": [
            "Numerical stability and precision preservation across scales",
            "Lack of available mathematical libraries",
            "Insufficient model interpretability requirements",
            "Over-reliance on cloud computing resources"
        ]
    },
    {
        "q": "The ______ learning paradigm enables models to reason about their own uncertainty and knowledge boundaries.",
        "type": "fill_blank",
        "answers": ["meta"],
        "other_options": ["supervised", "unsupervised", "semi-supervised"]
    },
    {
        "q": "What is the output of the following symbolic regression evaluation?",
        "type": "mcq",
        "c": "import numpy as np\nfrom sklearn.metrics import mean_squared_error\nactual = np.array([2, 4, 6, 8])\ndiscovered_formula = 2 * np.array([1, 2, 3, 4])\nprint(f'MSE: {mean_squared_error(actual, discovered_formula):.1f}')",
        "o": [
            "MSE: 0.0",
            "MSE: 1.0",
            "MSE: 2.0",
            "MSE: 4.0"
        ]
    },
    {
        "q": "Match the quantum machine learning concept with its classical counterpart:",
        "type": "match",
        "left": ["Quantum Feature Maps", "Variational Quantum Circuits", "Quantum Kernel Estimation", "Quantum Boltzmann Machines"],
        "right": ["Non-linear feature transformation", "Parameterized neural networks", "Kernel methods in SVMs", "Generative energy-based models"]
    },
    {
        "q": "In federated analytics, the ______ technique allows statistical computation across distributed data without raw data sharing.",
        "type": "fill_blank",
        "answers": ["secure aggregation"],
        "other_options": ["data parallelism", "model averaging", "gradient descent"]
    },
    {
        "q": "Rearrange the steps for building an AI assurance case:",
        "type": "rearrange",
        "words": ["Define System Boundaries", "Identify Potential Hazards", "Specify Safety Requirements", "Provide Evidence Arguments", "Document Residual Risks"]
    },
    {
        "q": "Transformers can effectively process only sequential data and cannot handle graph-structured information.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which approach addresses the challenge of catastrophic interference in continual learning systems?",
        "type": "mcq",
        "o": [
            "Elastic weight consolidation and progressive networks",
            "Increasing model capacity indefinitely",
            "Training on all possible data combinations simultaneously",
            "Eliminating model parameter updates entirely"
        ]
    },
    {
        "q": "The process of ______ programming combines probabilistic modeling with program synthesis for interpretable AI.",
        "type": "fill_blank",
        "answers": ["probabilistic"],
        "other_options": ["functional", "object-oriented", "logic"]
    },
    {
        "q": "What is the primary innovation of neural ordinary differential equations (Neural ODEs)?",
        "type": "mcq",
        "o": [
            "Continuous-depth models with adaptive computation",
            "Eliminating all numerical instability in training",
            "Guaranteed convex optimization landscapes",
            "Discrete-time processing with fixed intervals"
        ]
    },
    {
        "q": "Match the AI governance framework with its focus area:",
        "type": "match",
        "left": ["NIST AI RMF", "EU AI Act", "OECD AI Principles", "IEEE Ethically Aligned Design"],
        "right": ["Risk management for AI systems", "Regulatory compliance and categorization", "International policy guidelines", "Technical standards for ethical AI"]
    },
    {
        "q": "In reservoir computing, the ______ reservoir provides fixed, non-linear transformation of input signals.",
        "type": "fill_blank",
        "answers": ["echo state"],
        "other_options": ["quantum", "optical", "fluid"]
    },
    {
        "q": "Rearrange the workflow for implementing automated machine learning (AutoML):",
        "type": "rearrange",
        "words": ["Define Search Space", "Select Optimization Algorithm", "Evaluate Candidate Pipelines", "Perform Hyperparameter Tuning", "Validate Final Model"]
    },
    {
        "q": "Graph neural networks require completely different deployment infrastructure than traditional deep learning models.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which technique enables training of large language models with limited computational resources?",
        "type": "mcq",
        "o": [
            "Parameter-efficient fine-tuning and model parallelism",
            "Training on smaller vocabulary sizes exclusively",
            "Eliminating all attention mechanisms",
            "Using only CPU-based computation"
        ]
    },
    {
        "q": "In multi-task learning, the ______ approach shares representations while maintaining task-specific heads.",
        "type": "fill_blank",
        "answers": ["hard parameter sharing"],
        "other_options": ["soft attention", "model stacking", "data augmentation"]
    },
    {
        "q": "Rearrange the steps for implementing a data mesh architecture:",
        "type": "rearrange",
        "words": ["Define Data Domains", "Establish Data Products", "Implement Federated Governance", "Create Self-Serve Platform"]
    },
    {
        "q": "Match the edge AI constraint with its mitigation strategy:",
        "type": "match",
        "left": ["Memory Limitations", "Power Constraints", "Network Latency", "Compute Limitations"],
        "right": ["Model quantization and pruning", "Energy-aware inference scheduling", "On-device processing", "Hardware accelerators and model distillation"]
    },
    {
        "q": "TinyML deployments require the same monitoring infrastructure as cloud-based ML systems.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "In AI supply chain management, ______ tracing tracks model components back to their original data and code sources.",
        "type": "fill_blank",
        "answers": ["provenance"],
        "other_options": ["version", "quality", "performance"]
    },
    {
        "q": "What is the primary innovation of capsule networks in computer vision?",
        "type": "mcq",
        "o": [
            "Preserving hierarchical pose relationships between features",
            "Eliminating all convolutional operations",
            "Using larger kernel sizes exclusively",
            "Removing all pooling layers"
        ]
    },
    {
        "q": "The ______ learning approach enables models to learn from very few examples by leveraging prior knowledge.",
        "type": "fill_blank",
        "answers": ["few-shot"],
        "other_options": ["zero-shot", "transfer", "self-supervised"]
    },
    {
        "q": "What is the output of the following uncertainty quantification code?",
        "type": "mcq",
        "c": "import numpy as np\npredictions = [0.7, 0.8, 0.75, 0.85, 0.9]\nuncertainty = np.std(predictions)\nprint(f'Prediction uncertainty: {uncertainty:.3f}')",
        "o": [
            "Prediction uncertainty: 0.075",
            "Prediction uncertainty: 0.800",
            "Prediction uncertainty: 0.200",
            "Prediction uncertainty: 0.000"
        ]
    },
    {
        "q": "Match the AI ethics principle with its technical implementation:",
        "type": "match",
        "left": ["Justice", "Beneficence", "Non-maleficence", "Explicability"],
        "right": ["Fairness constraints and bias testing", "Social good objective functions", "Adversarial robustness testing", "Model interpretability and documentation"]
    },
    {
        "q": "In computational sustainability, ______ learning optimizes resource allocation for environmental benefits.",
        "type": "fill_blank",
        "answers": ["reinforcement"],
        "other_options": ["supervised", "unsupervised", "semi-supervised"]
    },
    {
        "q": "Rearrange the workflow for building an AI incident response system:",
        "type": "rearrange",
        "words": ["Detect Anomaly", "Activate Response Team", "Contain Impact", "Analyze Root Cause", "Implement Corrective Actions"]
    },
    {
        "q": "Hyperdimensional computing uses vectors with thousands of dimensions to represent symbolic information.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which technique addresses the challenge of neural network verification?",
        "type": "mcq",
        "o": [
            "Formal methods and satisfiability modulo theories",
            "Increasing model complexity indefinitely",
            "Using only linear activation functions",
            "Eliminating all hidden layers"
        ]
    },
    {
        "q": "The process of ______ learning enables AI systems to learn human preferences through comparison feedback.",
        "type": "fill_blank",
        "answers": ["preference"],
        "other_options": ["imitation", "curriculum", "multi-task"]
    },
    {
        "q": "What is the primary benefit of using spiking neural networks for edge AI?",
        "type": "mcq",
        "o": [
            "Event-driven processing with ultra-low power consumption",
            "Guaranteed real-time performance on all hardware",
            "Eliminating all training data requirements",
            "Perfect biological accuracy in all applications"
        ]
    },
    {
        "q": "Match the AI safety research area with its focus:",
        "type": "match",
        "left": ["Specification Gaming", "Robustness", "Monitorability", "Containment"],
        "right": ["Preventing reward hacking behaviors", "Handling distribution shifts and adversaries", "Observability of internal model states", "Limiting system capabilities and access"]
    },
    {
        "q": "In AI for science, ______ networks learn the fundamental equations governing physical systems from data.",
        "type": "fill_blank",
        "answers": ["neural ordinary differential equation"],
        "other_options": ["convolutional", "recurrent", "transformers"]
    },
    {
        "q": "Rearrange the steps for implementing differential privacy in ML:",
        "type": "rearrange",
        "words": ["Set Privacy Budget", "Calculate Query Sensitivity", "Add Laplace Noise", "Verify Privacy Guarantees"]
    },
    {
        "q": "All reinforcement learning algorithms require carefully designed reward functions to avoid unintended behaviors.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which approach enables training of world models for planning and imagination in AI systems?",
        "type": "mcq",
        "o": [
            "Predictive state representations and latent dynamics",
            "Increasing model parameter count exclusively",
            "Eliminating all memory mechanisms",
            "Using only supervised learning objectives"
        ]
    },
    {
        "q": "In multi-modal learning, ______ alignment ensures different data modalities represent the same semantic concepts.",
        "type": "fill_blank",
        "answers": ["cross-modal"],
        "other_options": ["self-supervised", "contrastive", "supervised"]
    },
    {
        "q": "Match the AI regulation framework with its geographic jurisdiction:",
        "type": "match",
        "left": ["GDPR", "CCPA", "PIPL", "LGPD"],
        "right": ["European Union", "California, USA", "China", "Brazil"]
    },
    {
        "q": "Rearrange the steps for implementing a model card for a high-risk AI system:",
        "type": "rearrange",
        "words": ["Document Intended Use", "List Limitations", "Detail Performance Characteristics", "Specify Ethical Considerations", "Provide Contact Information"]
    },
    {
        "q": "Synthetic data can fully replace real-world data for training regulatory-compliant medical AI systems.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "In AI governance, the ______ principle requires that automated decisions can be explained to affected individuals.",
        "type": "fill_blank",
        "answers": ["right to explanation"],
        "other_options": ["data minimization", "purpose limitation", "storage limitation"]
    },
    {
        "q": "What is the primary purpose of implementing AI system impact assessments?",
        "type": "mcq",
        "o": [
            "To identify and mitigate potential risks before deployment",
            "To increase model complexity and parameters",
            "To reduce training data requirements",
            "To eliminate all human oversight requirements"
        ]
    },
    {
        "q": "The ______ learning approach trains models to be robust against distribution shifts by exposing them to diverse domains during training.",
        "type": "fill_blank",
        "answers": ["domain generalization"],
        "other_options": ["transfer learning", "meta-learning", "multi-task learning"]
    },
    {
        "q": "What is the output of the following fairness metric calculation?",
        "type": "mcq",
        "c": "import numpy as np\ngroup_a_scores = [0.8, 0.7, 0.9]\ngroup_b_scores = [0.6, 0.5, 0.7]\nfairness_gap = np.mean(group_a_scores) - np.mean(group_b_scores)\nprint(f'Fairness disparity: {fairness_gap:.2f}')",
        "o": [
            "Fairness disparity: 0.20",
            "Fairness disparity: 0.10",
            "Fairness disparity: 0.30",
            "Fairness disparity: 0.00"
        ]
    },
    {
        "q": "Match the AI system failure mode with its detection method:",
        "type": "match",
        "left": ["Cascade Failures", "Feedback Loops", "Representation Harm", "Allocation Harm"],
        "right": ["Circuit breaker patterns and dependency monitoring", "Drift detection and temporal analysis", "Disaggregated performance metrics", "Opportunity distribution tracking"]
    },
    {
        "q": "In AI safety, ______ learning trains models to avoid catastrophic behaviors through constrained optimization.",
        "type": "fill_blank",
        "answers": ["safe reinforcement"],
        "other_options": ["supervised", "unsupervised", "self-supervised"]
    },
    {
        "q": "Rearrange the AI system development lifecycle for high-stakes applications:",
        "type": "rearrange",
        "words": ["Requirements Analysis", "Risk Assessment", "Design with Safeguards", "Rigorous Testing", "Controlled Deployment", "Continuous Monitoring"]
    },
    {
        "q": "All AI systems used in credit scoring must be fully explainable under the Equal Credit Opportunity Act.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which technique helps prevent reward hacking in reinforcement learning systems?",
        "type": "mcq",
        "o": [
            "Reward shaping and multi-objective optimization",
            "Increasing the reward magnitude indefinitely",
            "Using only sparse reward signals",
            "Eliminating all negative rewards"
        ]
    },
    {
        "q": "The process of ______ validation tests AI systems against rare but critical edge cases.",
        "type": "fill_blank",
        "answers": ["adversarial"],
        "other_options": ["unit", "integration", "regression"]
    },
    {
        "q": "What is the primary benefit of using conformal prediction for uncertainty quantification?",
        "type": "mcq",
        "o": [
            "Provides statistically valid prediction intervals",
            "Eliminates all prediction errors",
            "Reduces model training time",
            "Increases model parameter efficiency"
        ]
    },
    {
        "q": "Match the AI transparency technique with its audience:",
        "type": "match",
        "left": ["Technical Documentation", "Model Cards", "Fact Sheets", "Plain Language Summaries"],
        "right": ["AI engineers and researchers", "Business stakeholders and regulators", "System auditors and compliance officers", "End users and affected individuals"]
    },
    {
        "q": "In AI incident management, ______ procedures ensure rapid response to system failures.",
        "type": "fill_blank",
        "answers": ["rollback"],
        "other_options": ["training", "monitoring", "deployment"]
    },
    {
        "q": "Rearrange the data minimization principles for privacy-preserving AI:",
        "type": "rearrange",
        "words": ["Collect Only Necessary Data", "Limit Processing Purpose", "Implement Access Controls", "Establish Retention Periods", "Ensure Secure Deletion"]
    },
    {
        "q": "Federated learning completely eliminates all privacy risks in distributed model training.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which approach helps ensure AI systems remain aligned with human values as they scale?",
        "type": "mcq",
        "o": [
            "Iterated amplification and recursive reward modeling",
            "Increasing model size without oversight",
            "Eliminating all human feedback loops",
            "Using only static training datasets"
        ]
    },
    {
        "q": "In AI ethics, ______ impact assessments evaluate how systems affect different demographic groups.",
        "type": "fill_blank",
        "answers": ["disparate"],
        "other_options": ["economic", "environmental", "social"]
    },
    {
        "q": "Match the AI system architecture with its primary resilience characteristic:",
        "type": "match",
        "left": ["Circuit Breaker Pattern", "Bulkhead Pattern", "Chaos Engineering", "Graceful Degradation"],
        "right": ["Prevents cascade failures by isolating components", "Contains failures within partitioned segments", "Proactively tests system stability under stress", "Maintains partial functionality during partial failures"]
    },
    {
        "q": "Rearrange the steps for implementing a responsible AI disclosure protocol:",
        "type": "rearrange",
        "words": ["Identify AI Interaction", "Provide Clear Notice", "Explain Capabilities", "Disclose Limitations", "Offer Human Alternative"]
    },
    {
        "q": "All AI systems making automated legal decisions require the same level of interpretability across all jurisdictions.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "In AI system design, ______ testing evaluates how models perform under coordinated multi-failure scenarios.",
        "type": "fill_blank",
        "answers": ["resilience"],
        "other_options": ["unit", "integration", "performance"]
    },
    {
        "q": "What is the primary purpose of implementing AI system versioning with cryptographic hashing?",
        "type": "mcq",
        "o": [
            "Ensuring audit trail integrity and reproducibility",
            "Reducing model storage requirements",
            "Increasing inference speed",
            "Simplifying model architecture"
        ]
    },
    {
        "q": "The ______ learning paradigm enables models to adapt to new tasks without forgetting previous ones through dynamic architecture expansion.",
        "type": "fill_blank",
        "answers": ["lifelong"],
        "other_options": ["transfer", "meta", "multi-task"]
    },
    {
        "q": "What is the output of the following model staleness detection code?",
        "type": "mcq",
        "c": "import pandas as pd\nfrom datetime import datetime\nlast_training = datetime(2024, 1, 1)\ncurrent_date = datetime(2024, 7, 1)\ndays_stale = (current_date - last_training).days\nthreshold = 90\nprint('Model stale:' if days_stale > threshold else 'Model current')",
        "o": [
            "Model stale:",
            "Model current",
            "90",
            "181"
        ]
    },
    {
        "q": "Match the AI system recovery strategy with its implementation approach:",
        "type": "match",
        "left": ["Blue-Green Deployment", "Canary Analysis", "Traffic Shadowing", "Feature Flags"],
        "right": ["Maintains two identical production environments", "Gradually routes traffic to new version", "Mirrors production traffic to test systems", "Enables runtime configuration changes without deployment"]
    },
    {
        "q": "In AI supply chain security, ______ verification ensures third-party model components haven't been tampered with.",
        "type": "fill_blank",
        "answers": ["cryptographic"],
        "other_options": ["performance", "functional", "compatibility"]
    },
    {
        "q": "Rearrange the AI system decommissioning protocol:",
        "type": "rearrange",
        "words": ["Archive Model Artifacts", "Remove from Production", "Update System Documentation", "Conduct Post-Mortem Analysis", "Execute Data Disposal"]
    },
    {
        "q": "AI systems used in critical infrastructure must have manual override capabilities by law in most jurisdictions.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which technique helps prevent model inversion attacks on deployed AI systems?",
        "type": "mcq",
        "o": [
            "Differential privacy and output perturbation",
            "Increasing model complexity",
            "Using larger training datasets",
            "Eliminating all model outputs"
        ]
    },
    {
        "q": "The process of ______ validation ensures AI systems meet regulatory requirements before deployment.",
        "type": "fill_blank",
        "answers": ["compliance"],
        "other_options": ["unit", "integration", "performance"]
    },
    {
        "q": "What is the primary benefit of implementing AI system circuit breakers?",
        "type": "mcq",
        "o": [
            "Prevents catastrophic cascade failures during system degradation",
            "Increases model accuracy on all tasks",
            "Reduces training time for new models",
            "Eliminates the need for monitoring"
        ]
    },
    {
        "q": "Match the AI incident severity level with its response protocol:",
        "type": "match",
        "left": ["Severity 1: Critical", "Severity 2: High", "Severity 3: Medium", "Severity 4: Low"],
        "right": ["Immediate full-team engagement and continuous response", "Designated response team with expedited resolution", "Scheduled investigation with business hours response", "Documented for future analysis and trend tracking"]
    },
    {
        "q": "In AI system reliability engineering, ______ testing simulates peak load conditions to validate performance boundaries.",
        "type": "fill_blank",
        "answers": ["stress"],
        "other_options": ["unit", "integration", "smoke"]
    },
    {
        "q": "Rearrange the AI model retirement checklist:",
        "type": "rearrange",
        "words": ["Verify Replacement Readiness", "Communicate Retirement Timeline", "Export Final Model State", "Update Routing Rules", "Archive Documentation"]
    },
    {
        "q": "All AI system failures can be detected through automated monitoring alone.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which approach helps maintain AI system reliability during data pipeline failures?",
        "type": "mcq",
        "o": [
            "Implementing fallback strategies and default behaviors",
            "Increasing model complexity to compensate",
            "Stopping all system operations immediately",
            "Ignoring missing data and proceeding normally"
        ]
    },
    {
        "q": "In AI system design, ______ patterns separate critical functionality to prevent single points of failure.",
        "type": "fill_blank",
        "answers": ["bulkhead"],
        "other_options": ["circuit breaker", "retry", "timeout"]
    },
    {
        "q": "Match the AI system scaling pattern with its primary use case:",
        "type": "match",
        "left": ["Sharding", "CQRS", "Event Sourcing", "Caching Layers"],
        "right": ["Distributing data across multiple databases", "Separating read and write operations", "Maintaining immutable audit log of changes", "Reducing latency for frequent queries"]
    },
    {
        "q": "Rearrange the steps for implementing AI system canary deployment:",
        "type": "rearrange",
        "words": ["Deploy to Isolated Environment", "Route Small Traffic Percentage", "Monitor Key Metrics", "Analyze Performance Data", "Gradually Increase Traffic"]
    },
    {
        "q": "AI systems using transfer learning from pre-trained models inherit all biases from the original training data.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In microservices AI architecture, the ______ pattern coordinates complex operations across multiple specialized services.",
        "type": "fill_blank",
        "answers": ["orchestration"],
        "other_options": ["aggregator", "proxy", "chained"]
    },
    {
        "q": "What is the primary advantage of using service mesh architecture for distributed AI systems?",
        "type": "mcq",
        "o": [
            "Unified observability and security across services",
            "Reduced computational requirements",
            "Elimination of all network latency",
            "Automatic model training and deployment"
        ]
    },
    {
        "q": "The ______ testing approach validates AI system behavior under simulated hardware failures and network partitions.",
        "type": "fill_blank",
        "answers": ["chaos engineering"],
        "other_options": ["unit", "integration", "regression"]
    },
    {
        "q": "What is the output of the following circuit breaker state calculation?",
        "type": "mcq",
        "c": "failure_count = 8\nsuccess_count = 2\nfailure_threshold = 0.7\ncurrent_rate = failure_count / (failure_count + success_count)\nprint('Circuit open:' if current_rate > failure_threshold else 'Circuit closed')",
        "o": [
            "Circuit open:",
            "Circuit closed",
            "0.8",
            "0.7"
        ]
    },
    {
        "q": "Match the AI system anti-pattern with its mitigation strategy:",
        "type": "match",
        "left": ["Big Ball of Mud", "God Class", "Spaghetti Architecture", "Vendor Lock-in"],
        "right": ["Modular design and clear boundaries", "Single responsibility principle", "Explicit interfaces and dependency inversion", "Multi-cloud strategy and abstraction layers"]
    },
    {
        "q": "In distributed AI training, ______ parallelism splits model layers across multiple devices for very large models.",
        "type": "fill_blank",
        "answers": ["pipeline"],
        "other_options": ["data", "tensor", "model"]
    },
    {
        "q": "Rearrange the steps for implementing zero-downtime AI system updates:",
        "type": "rearrange",
        "words": ["Prepare New Version", "Health Check Validation", "Gradual Traffic Migration", "Session Draining", "Old Version Cleanup"]
    },
    {
        "q": "All AI system scaling challenges can be solved by adding more computing resources.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which architectural pattern helps prevent cascading failures in microservices-based AI systems?",
        "type": "mcq",
        "o": [
            "Bulkhead isolation and circuit breakers",
            "Increasing all timeout values indefinitely",
            "Removing all service dependencies",
            "Using only synchronous communication"
        ]
    },
    {
        "q": "The ______ database pattern separates operational and analytical workloads in AI systems.",
        "type": "fill_blank",
        "answers": ["command query responsibility segregation"],
        "other_options": ["event sourcing", "materialized view", "read replica"]
    },
    {
        "q": "What is the primary benefit of implementing API versioning in AI service interfaces?",
        "type": "mcq",
        "o": [
            "Maintains backward compatibility during evolution",
            "Reduces model inference latency",
            "Eliminates all API documentation needs",
            "Increases model training speed"
        ]
    },
    {
        "q": "Match the distributed system challenge with its AI-specific impact:",
        "type": "match",
        "left": ["Network Partitions", "Clock Skew", "Partial Failures", "Concurrent Updates"],
        "right": ["Model consistency and prediction divergence", "Event ordering in streaming pipelines", "Degraded but not fully failed service", "Training data conflicts and merge issues"]
    },
    {
        "q": "In AI system design, ______ consistency ensures all nodes eventually converge to the same state.",
        "type": "fill_blank",
        "answers": ["eventual"],
        "other_options": ["strong", "causal", "sequential"]
    },
    {
        "q": "Rearrange the steps for implementing distributed tracing in AI microservices:",
        "type": "rearrange",
        "words": ["Instrument Services", "Generate Trace IDs", "Propagate Context", "Collect Spans", "Analyze Dependencies"]
    },
    {
        "q": "Serverless architectures eliminate all operational overhead for AI model serving.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which approach helps manage technical debt in long-lived AI systems?",
        "type": "mcq",
        "o": [
            "Regular refactoring and architectural reviews",
            "Ignoring deprecated components",
            "Adding more features without cleanup",
            "Eliminating all documentation"
        ]
    },
    {
        "q": "In cloud-native AI systems, ______ patterns enable automatic scaling based on prediction request load.",
        "type": "fill_blank",
        "answers": ["autoscaling"],
        "other_options": ["load balancing", "caching", "sharding"]
    },
    {
        "q": "Match the AI system design smell with its refactoring approach:",
        "type": "match",
        "left": ["Feature Envy", "Shotgun Surgery", "Parallel Inheritance", "Message Chains"],
        "right": ["Move method to data's class", "Consolidate changes into single component", "Merge hierarchy trees", "Introduce intermediary to hide chain"]
    },
    {
        "q": "Rearrange the steps for conducting an AI system architecture review:",
        "type": "rearrange",
        "words": ["Document Current State", "Identify Pain Points", "Evaluate Alternatives", "Propose Improvements", "Create Migration Plan"]
    },
    {
        "q": "AI systems using eventual consistency can guarantee immediate read-after-write consistency across all nodes.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "In domain-driven design for AI systems, ______ contexts define clear boundaries between different business domains.",
        "type": "fill_blank",
        "answers": ["bounded"],
        "other_options": ["ubiquitous", "core", "supporting"]
    },
    {
        "q": "What is the primary purpose of implementing the Outbox Pattern in event-driven AI systems?",
        "type": "mcq",
        "o": [
            "Ensure reliable event publishing without dual writes",
            "Increase model inference speed",
            "Reduce database storage requirements",
            "Eliminate all network latency"
        ]
    },
    {
        "q": "The ______ testing strategy validates AI system behavior by comparing outputs against known reference implementations.",
        "type": "fill_blank",
        "answers": ["snapshot"],
        "other_options": ["unit", "integration", "regression"]
    },
    {
        "q": "What is the output of the following saga pattern compensation handler simulation?",
        "type": "mcq",
        "c": "steps_completed = ['data_validation', 'feature_engineering', 'model_prediction']\nfailed_step = 'result_storage'\ncompensation_actions = []\nfor step in reversed(steps_completed):\n    compensation_actions.append(f'undo_{step}')\nprint(f'Compensation sequence: {compensation_actions[-1]}')",
        "o": [
            "Compensation sequence: undo_data_validation",
            "Compensation sequence: undo_feature_engineering",
            "Compensation sequence: undo_model_prediction",
            "Compensation sequence: undo_result_storage"
        ]
    },
    {
        "q": "Match the architectural decision record (ADR) component with its purpose:",
        "type": "match",
        "left": ["Context", "Decision", "Consequences", "Status"],
        "right": ["Describes the forces and constraints", "Records the chosen approach", "Documents trade-offs and impacts", "Tracks current state of decision"]
    },
    {
        "q": "In AI system evolution, ______ migration allows both old and new systems to operate simultaneously during transition.",
        "type": "fill_blank",
        "answers": ["parallel"],
        "other_options": ["big bang", "phased", "direct cutover"]
    },
    {
        "q": "Rearrange the steps for implementing feature toggles in AI systems:",
        "type": "rearrange",
        "words": ["Define Toggle Points", "Implement Toggle Logic", "Configure Toggle States", "Monitor Toggle Usage", "Cleanup Expired Toggles"]
    },
    {
        "q": "All AI system performance bottlenecks can be identified through CPU and memory monitoring alone.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which pattern helps manage complex AI workflow dependencies without tight coupling?",
        "type": "mcq",
        "o": [
            "Workflow engine with explicit state management",
            "Monolithic procedural code",
            "Direct service-to-service calls only",
            "Shared global state variables"
        ]
    },
    {
        "q": "The ______ principle suggests AI system components should be open for extension but closed for modification.",
        "type": "fill_blank",
        "answers": ["open-closed"],
        "other_options": ["single responsibility", "interface segregation", "dependency inversion"]
    },
    {
        "q": "What is the primary benefit of using consumer-driven contracts in AI microservices?",
        "type": "mcq",
        "o": [
            "Prevents breaking changes for service consumers",
            "Increases model training accuracy",
            "Reduces network bandwidth usage",
            "Eliminates all testing requirements"
        ]
    },
    {
        "q": "Match the system quality attribute with its AI-specific consideration:",
        "type": "match",
        "left": ["Modifiability", "Testability", "Deployability", "Monitorability"],
        "right": ["Model version management and A/B testing", "Prediction reproducibility and mock data", "Containerization and model packaging", "Prediction logging and drift detection"]
    },
    {
        "q": "In AI system documentation, ______ diagrams show how data flows between system components.",
        "type": "fill_blank",
        "answers": ["data flow"],
        "other_options": ["sequence", "state", "component"]
    },
    {
        "q": "Rearrange the technical debt management lifecycle:",
        "type": "rearrange",
        "words": ["Identify Debt", "Prioritize Items", "Schedule Repayment", "Implement Fixes", "Verify Resolution"]
    },
    {
        "q": "Event sourcing requires storing only the current state of AI model predictions, not historical events.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which approach helps maintain AI system quality during rapid team scaling?",
        "type": "mcq",
        "o": [
            "Establish coding standards and automated quality gates",
            "Remove all code review processes",
            "Allow complete architectural freedom",
            "Eliminate documentation requirements"
        ]
    },
    {
        "q": "In AI system reliability engineering, ______ analysis identifies single points of failure before they occur.",
        "type": "fill_blank",
        "answers": ["failure mode and effects"],
        "other_options": ["root cause", "performance", "cost-benefit"]
    },
    {
        "q": "Match the AI system integration pattern with its data synchronization challenge:",
        "type": "match",
        "left": ["Change Data Capture", "Dual Writes", "Event Sourcing", "API Composition"],
        "right": ["Handling database log parsing and ordering", "Maintaining consistency across distributed systems", "Rebuilding state from event history", "Joining data from multiple services in real-time"]
    },
    {
        "q": "Rearrange the steps for implementing a dark launch of new AI features:",
        "type": "rearrange",
        "words": ["Deploy to Production", "Enable for Internal Users", "Collect Performance Data", "Analyze Impact", "Gradual User Rollout"]
    },
    {
        "q": "AI systems using CQRS must always use separate databases for read and write operations.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "In AI system reliability, the ______ window defines the maximum acceptable time for system recovery after failure.",
        "type": "fill_blank",
        "answers": ["recovery time objective"],
        "other_options": ["service level objective", "error budget", "mean time between failures"]
    },
    {
        "q": "What is the primary advantage of using the Sidecar pattern in AI microservices?",
        "type": "mcq",
        "o": [
            "Separation of cross-cutting concerns from business logic",
            "Reduced computational requirements for model inference",
            "Elimination of all network communication",
            "Automatic model retraining without human intervention"
        ]
    },
    {
        "q": "The ______ testing approach validates AI system behavior by executing complete user journeys across multiple services.",
        "type": "fill_blank",
        "answers": ["end-to-end"],
        "other_options": ["unit", "integration", "smoke"]
    },
    {
        "q": "What is the output of the following circuit breaker state transition simulation?",
        "type": "mcq",
        "c": "state = 'CLOSED'\nfailures = 0\nthreshold = 5\nfor i in range(6):\n    failures += 1\n    if failures >= threshold and state == 'CLOSED':\n        state = 'OPEN'\nprint(f'Final state: {state}')",
        "o": [
            "Final state: OPEN",
            "Final state: CLOSED",
            "Final state: HALF_OPEN",
            "Final state: UNKNOWN"
        ]
    },
    {
        "q": "Match the AI system decomposition strategy with its primary benefit:",
        "type": "match",
        "left": ["Business Capability", "Domain-Driven Design", "Technical Layer", "Geographic Region"],
        "right": ["Aligns with organizational structure", "Reflects business language and concepts", "Separates concerns by technology stack", "Optimizes for local regulations and latency"]
    },
    {
        "q": "In AI system security, the ______ principle ensures services operate with minimal required permissions.",
        "type": "fill_blank",
        "answers": ["least privilege"],
        "other_options": ["defense in depth", "separation of duties", "fail securely"]
    },
    {
        "q": "Rearrange the steps for implementing a feature flag governance process:",
        "type": "rearrange",
        "words": ["Define Approval Workflow", "Establish Naming Conventions", "Implement Access Controls", "Monitor Flag Usage", "Enforce Cleanup Policies"]
    },
    {
        "q": "All AI system scalability issues can be resolved by adding more caching layers.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which pattern helps manage AI service dependencies in distributed tracing?",
        "type": "mcq",
        "o": [
            "Context propagation with correlation IDs",
            "Shared database for all services",
            "Synchronous request-reply only",
            "Hardcoded service endpoints"
        ]
    },
    {
        "q": "The ______ metric measures the percentage of successful AI system requests over time.",
        "type": "fill_blank",
        "answers": ["availability"],
        "other_options": ["latency", "throughput", "error rate"]
    },
    {
        "q": "What is the primary purpose of implementing health checks in AI microservices?",
        "type": "mcq",
        "o": [
            "Enable automated failure detection and recovery",
            "Increase model prediction accuracy",
            "Reduce training data requirements",
            "Eliminate all manual monitoring"
        ]
    },
    {
        "q": "Match the AI system deployment strategy with its risk profile:",
        "type": "match",
        "left": ["Blue-Green", "Canary", "Shadow", "A/B Testing"],
        "right": ["Low risk with instant rollback capability", "Medium risk with gradual exposure", "Zero risk to users during testing", "Business risk from user experience variations"]
    },
    {
        "q": "In AI system design, ______ consistency allows temporary inconsistencies for better performance.",
        "type": "fill_blank",
        "answers": ["weak"],
        "other_options": ["strong", "eventual", "causal"]
    },
    {
        "q": "Rearrange the steps for conducting AI system capacity planning:",
        "type": "rearrange",
        "words": ["Analyze Current Usage", "Forecast Growth", "Identify Bottlenecks", "Plan Scaling Strategy", "Implement Monitoring"]
    },
    {
        "q": "Microservices architecture eliminates all dependencies between AI system components.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which approach helps prevent AI service registry bottlenecks in large deployments?",
        "type": "mcq",
        "o": [
            "Client-side caching with periodic refresh",
            "Single centralized registry instance",
            "Hardcoded service locations",
            "No service discovery mechanism"
        ]
    },
    {
        "q": "In cloud-native AI systems, ______ patterns enable automatic recovery from instance failures.",
        "type": "fill_blank",
        "answers": ["self-healing"],
        "other_options": ["load balancing", "auto-scaling", "circuit breaker"]
    },
    {
        "q": "Match the AI system quality attribute trade-off with its mitigation strategy:",
        "type": "match",
        "left": ["Latency vs. Accuracy", "Cost vs. Performance", "Flexibility vs. Complexity", "Security vs. Usability"],
        "right": ["Cascading models with early exits", "Spot instances with graceful degradation", "Plugin architecture with clear interfaces", "Role-based access with sensible defaults"]
    },
    {
        "q": "Rearrange the steps for implementing AI system chaos engineering experiments:",
        "type": "rearrange",
        "words": ["Define Steady State", "Formulate Hypothesis", "Inject Controlled Failure", "Measure Impact", "Analyze Results"]
    },
    {
        "q": "AI systems using homomorphic encryption can perform computations on encrypted data without decryption, but this eliminates all performance overhead.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "In AI system design, ______ testing validates that components work together in a production-like environment.",
        "type": "fill_blank",
        "answers": ["integration"],
        "other_options": ["unit", "smoke", "regression"]
    },
    {
        "q": "What is the primary benefit of using the Strangler Fig pattern for AI system migration?",
        "type": "mcq",
        "o": [
            "Gradual replacement of legacy systems with minimal risk",
            "Instantaneous switch to new architecture",
            "Elimination of all testing requirements",
            "Automatic data migration without validation"
        ]
    },
    {
        "q": "The ______ principle in AI system design suggests favoring composition over inheritance for code reuse.",
        "type": "fill_blank",
        "answers": ["composition over inheritance"],
        "other_options": ["separation of concerns", "dependency inversion", "interface segregation"]
    },
    {
        "q": "What is the output of the following retry mechanism simulation?",
        "type": "mcq",
        "c": "import time\nmax_retries = 3\nbackoff_factor = 2\nfor attempt in range(max_retries):\n    if attempt == 1:\n        print(f'Attempt {attempt + 1}: Success')\n        break\n    wait_time = backoff_factor ** attempt\nelse:\n    print('All attempts failed')",
        "o": [
            "Attempt 2: Success",
            "Attempt 1: Success",
            "Attempt 3: Success",
            "All attempts failed"
        ]
    },
    {
        "q": "Match the AI system architectural style with its primary characteristic:",
        "type": "match",
        "left": ["Microkernel", "Space-Based", "Event-Driven", "Service-Oriented"],
        "right": ["Core system with pluggable components", "In-memory data grid for high scalability", "Asynchronous communication via events", "Business functionality as reusable services"]
    },
    {
        "q": "In AI system security, ______ testing attempts to exploit vulnerabilities in running systems.",
        "type": "fill_blank",
        "answers": ["penetration"],
        "other_options": ["unit", "integration", "performance"]
    },
    {
        "q": "Rearrange the steps for implementing AI system disaster recovery:",
        "type": "rearrange",
        "words": ["Identify Critical Systems", "Define Recovery Objectives", "Implement Backup Strategy", "Test Recovery Procedures", "Document Recovery Plan"]
    },
    {
        "q": "All AI system performance optimization should focus exclusively on reducing inference latency.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which pattern helps manage AI service configuration across multiple environments?",
        "type": "mcq",
        "o": [
            "Externalized configuration with environment-specific files",
            "Hardcoded configuration in source code",
            "Manual configuration per deployment",
            "No configuration management"
        ]
    },
    {
        "q": "The ______ metric tracks the average time between AI system failures.",
        "type": "fill_blank",
        "answers": ["mean time between failures"],
        "other_options": ["mean time to repair", "availability", "error rate"]
    },
    {
        "q": "What is the primary purpose of implementing bulkheads in AI system architecture?",
        "type": "mcq",
        "o": [
            "Isolate failures to prevent system-wide outages",
            "Increase model training speed",
            "Reduce data storage requirements",
            "Eliminate all network communication"
        ]
    },
    {
        "q": "Match the AI system deployment anti-pattern with its consequence:",
        "type": "match",
        "left": ["Big Bang Deployment", "Configuration Drift", "Snowflake Servers", "Database as Integration Point"],
        "right": ["High risk of catastrophic failure", "Inconsistent behavior across environments", "Manual maintenance and poor reproducibility", "Tight coupling and scalability bottlenecks"]
    },
    {
        "q": "In AI system monitoring, ______ dashboards provide high-level overviews of system health.",
        "type": "fill_blank",
        "answers": ["executive"],
        "other_options": ["technical", "operational", "business"]
    },
    {
        "q": "Rearrange the steps for AI system technical debt assessment:",
        "type": "rearrange",
        "words": ["Inventory System Components", "Identify Debt Items", "Estimate Remediation Cost", "Prioritize Based on Impact", "Create Repayment Schedule"]
    },
    {
        "q": "Serverless computing eliminates all operational responsibilities for AI model serving.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which approach helps ensure AI system documentation remains current?",
        "type": "mcq",
        "o": [
            "Treat documentation as code with version control",
            "Document only initial system design",
            "Rely on oral tradition and tribal knowledge",
            "Eliminate all documentation requirements"
        ]
    },
    {
        "q": "In distributed AI systems, ______ algorithms ensure data consistency across multiple nodes.",
        "type": "fill_blank",
        "answers": ["consensus"],
        "other_options": ["synchronization", "replication", "partitioning"]
    },
    {
        "q": "Match the AI system data governance pattern with its implementation challenge:",
        "type": "match",
        "left": ["Data Lineage Tracking", "Data Quality Monitoring", "Data Access Governance", "Data Retention Policy"],
        "right": ["Capturing transformations across distributed pipelines", "Establishing metrics for unstructured data quality", "Balancing security with data scientist accessibility", "Managing legal vs analytical requirements for data lifespan"]
    },
    {
        "q": "Rearrange the steps for implementing AI system digital sovereignty compliance:",
        "type": "rearrange",
        "words": ["Map Data Jurisdictions", "Implement Data Residency", "Encrypt Cross-Border Transfers", "Audit Compliance Controls", "Document Legal Basis"]
    },
    {
        "q": "AI systems processing biometric data for identification always require explicit opt-in consent under GDPR.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In AI ethics, ______ impact assessments evaluate how systems might disproportionately affect vulnerable populations.",
        "type": "fill_blank",
        "answers": ["disparate"],
        "other_options": ["environmental", "economic", "social"]
    },
    {
        "q": "What is the primary purpose of implementing data subject access requests (DSAR) in AI systems?",
        "type": "mcq",
        "o": [
            "To provide individuals with transparency and control over their personal data",
            "To increase model training efficiency",
            "To reduce data storage costs",
            "To eliminate all data processing documentation"
        ]
    },
    {
        "q": "The ______ principle requires AI systems to minimize data collection to only what is necessary for specified purposes.",
        "type": "fill_blank",
        "answers": ["data minimization"],
        "other_options": ["purpose limitation", "storage limitation", "accuracy"]
    },
    {
        "q": "What is the output of the following data anonymization k-anonymity check?",
        "type": "mcq",
        "c": "import pandas as pd\ndata = {'age': [25, 25, 45, 45], 'zipcode': [10001, 10001, 10002, 10002]}\ndf = pd.DataFrame(data)\nk_value = df.groupby(['age', 'zipcode']).size().min()\nprint(f'k-anonymity: {k_value}')",
        "o": [
            "k-anonymity: 2",
            "k-anonymity: 4",
            "k-anonymity: 1",
            "k-anonymity: 0"
        ]
    },
    {
        "q": "Match the AI fairness metric with its mathematical definition:",
        "type": "match",
        "left": ["Demographic Parity", "Equal Opportunity", "Predictive Parity", "Individual Fairness"],
        "right": ["P(Ŷ=1|A=a) = P(Ŷ=1|A=b)", "P(Ŷ=1|Y=1,A=a) = P(Ŷ=1|Y=1,A=b)", "P(Y=1|Ŷ=1,A=a) = P(Y=1|Ŷ=1,A=b)", "Similar individuals receive similar predictions"]
    },
    {
        "q": "In AI transparency, ______ statements explain automated decisions in terms understandable to data subjects.",
        "type": "fill_blank",
        "answers": ["meaningful"],
        "other_options": ["technical", "legal", "business"]
    },
    {
        "q": "Rearrange the steps for conducting an AI system algorithmic impact assessment:",
        "type": "rearrange",
        "words": ["Scope Assessment", "Identify Stakeholders", "Evaluate Potential Harms", "Document Mitigations", "Establish Monitoring"]
    },
    {
        "q": "All AI systems used in hiring decisions must provide candidates with the right to meaningful human review.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which technique helps prevent model inversion attacks on AI systems?",
        "type": "mcq",
        "o": [
            "Differential privacy and output perturbation",
            "Increasing model complexity",
            "Using larger training datasets",
            "Eliminating model confidence scores"
        ]
    },
    {
        "q": "The ______ principle requires AI systems to implement appropriate security measures for personal data.",
        "type": "fill_blank",
        "answers": ["integrity and confidentiality"],
        "other_options": ["purpose limitation", "data minimization", "storage limitation"]
    },
    {
        "q": "What is the primary benefit of implementing privacy by design in AI systems?",
        "type": "mcq",
        "o": [
            "Proactive privacy protection rather than reactive compliance",
            "Reduced computational requirements",
            "Elimination of all data governance processes",
            "Automatic model performance improvement"
        ]
    },
    {
        "q": "Match the AI ethics framework with its core principle:",
        "type": "match",
        "left": ["FAT* Framework", "EU Ethics Guidelines", "Montreal Declaration", "Asilomar Principles"],
        "right": ["Fairness, Accountability, Transparency", "Human agency and oversight", "Democratic participation and diversity", "Beneficial intelligence and shared prosperity"]
    },
    {
        "q": "In AI governance, ______ committees provide oversight for high-risk AI system deployments.",
        "type": "fill_blank",
        "answers": ["ethics"],
        "other_options": ["technical", "compliance", "operational"]
    },
    {
        "q": "Rearrange the steps for implementing AI system right to explanation compliance:",
        "type": "rearrange",
        "words": ["Identify Explainable Components", "Develop Explanation Methods", "Integrate with User Interfaces", "Test Explanation Quality", "Document Explanation Process"]
    },
    {
        "q": "AI systems using fully homomorphic encryption can perform unlimited computations on encrypted data without performance degradation.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which approach helps ensure AI systems respect cultural differences in global deployments?",
        "type": "mcq",
        "o": [
            "Cultural context analysis and localized validation",
            "Using identical models across all regions",
            "Ignoring cultural factors as irrelevant to AI",
            "Eliminating all human oversight mechanisms"
        ]
    },
    {
        "q": "In responsible AI, ______ testing evaluates how systems perform across different demographic segments.",
        "type": "fill_blank",
        "answers": ["disaggregated"],
        "other_options": ["unit", "integration", "performance"]
    },
    {
        "q": "Match the AI system failure mode with its detection strategy:",
        "type": "match",
        "left": ["Cascade Failure", "Silent Corruption", "Resource Exhaustion", "Deadlock"],
        "right": ["Circuit breakers and dependency monitoring", "Checksums and data validation", "Resource quotas and monitoring", "Timeout detection and process monitoring"]
    },
    {
        "q": "Rearrange the steps for implementing AI system chaos engineering:",
        "type": "rearrange",
        "words": ["Define Steady State", "Hypothesize Impact", "Inject Failure", "Verify Impact", "Learn and Improve"]
    },
    {
        "q": "AI systems using quantum machine learning algorithms require completely different data preprocessing than classical algorithms.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "In neuromorphic computing, ______ coding represents information through the timing of electrical spikes.",
        "type": "fill_blank",
        "answers": ["temporal"],
        "other_options": ["spatial", "frequency", "amplitude"]
    },
    {
        "q": "What is the primary challenge when deploying AI systems for real-time control of physical systems?",
        "type": "mcq",
        "o": [
            "Guaranteeing safety-critical response times and reliability",
            "Achieving perfect model accuracy on training data",
            "Minimizing model size for storage efficiency",
            "Eliminating all false positive predictions"
        ]
    },
    {
        "q": "The ______ learning paradigm enables AI systems to learn from human preferences through comparison feedback.",
        "type": "fill_blank",
        "answers": ["preference"],
        "other_options": ["imitation", "reinforcement", "supervised"]
    },
    {
        "q": "What is the output of the following hyperdimensional computing similarity calculation?",
        "type": "mcq",
        "c": "import numpy as np\nvector_a = np.array([1, -1, 1, -1])\nvector_b = np.array([1, 1, -1, -1])\nsimilarity = np.dot(vector_a, vector_b) / len(vector_a)\nprint(f'Similarity: {similarity}')",
        "o": [
            "Similarity: 0.0",
            "Similarity: 1.0",
            "Similarity: -1.0",
            "Similarity: 0.5"
        ]
    },
    {
        "q": "Match the emerging AI hardware with its computational advantage:",
        "type": "match",
        "left": ["Memristor Arrays", "Photonic Processors", "Quantum Annealers", "Neuromorphic Chips"],
        "right": ["In-memory computation reducing data movement", "Ultra-fast matrix operations using light", "Optimization problem solving through quantum effects", "Energy-efficient event-based processing"]
    },
    {
        "q": "In AI safety research, ______ learning trains models to avoid catastrophic behaviors through constrained optimization.",
        "type": "fill_blank",
        "answers": ["safe reinforcement"],
        "other_options": ["supervised", "unsupervised", "self-supervised"]
    },
    {
        "q": "Rearrange the steps for implementing AI system digital forensics:",
        "type": "rearrange",
        "words": ["Preserve Evidence", "Analyze Logs", "Reconstruct Events", "Identify Root Cause", "Document Findings"]
    },
    {
        "q": "All AI system adversarial attacks can be prevented through input sanitization alone.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which technique helps detect model stealing attacks on deployed AI systems?",
        "type": "mcq",
        "o": [
            "Watermarking and API rate limiting",
            "Increasing model complexity",
            "Using smaller training datasets",
            "Eliminating model confidence scores"
        ]
    },
    {
        "q": "The ______ testing methodology validates AI system behavior under extreme but plausible conditions.",
        "type": "fill_blank",
        "answers": ["stress"],
        "other_options": ["unit", "integration", "smoke"]
    },
    {
        "q": "What is the primary benefit of using spiking neural networks for edge AI applications?",
        "type": "mcq",
        "o": [
            "Ultra-low power consumption through event-based processing",
            "Guaranteed real-time performance on all hardware",
            "Elimination of all training data requirements",
            "Perfect biological accuracy in all applications"
        ]
    },
    {
        "q": "Match the AI system verification technique with its application:",
        "type": "match",
        "left": ["Formal Verification", "Fuzz Testing", "Model Checking", "Property-Based Testing"],
        "right": ["Mathematical proof of system properties", "Random input generation to find edge cases", "Exhaustive state space exploration", "Testing general properties against generated examples"]
    },
    {
        "q": "In AI explainability, ______ methods provide local explanations for individual predictions.",
        "type": "fill_blank",
        "answers": ["post-hoc"],
        "other_options": ["ante-hoc", "intrinsic", "global"]
    },
    {
        "q": "Rearrange the steps for implementing AI system cyber insurance requirements:",
        "type": "rearrange",
        "words": ["Assess Risk Exposure", "Implement Security Controls", "Document Security Measures", "Conduct Security Audits", "Maintain Compliance"]
    },
    {
        "q": "Federated learning completely eliminates all privacy risks in distributed model training.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which approach helps ensure AI systems remain robust against data poisoning attacks?",
        "type": "mcq",
        "o": [
            "Data provenance tracking and anomaly detection",
            "Increasing model complexity indefinitely",
            "Using only synthetic training data",
            "Eliminating all data validation steps"
        ]
    },
    {
        "q": "In AI system security, ______ testing evaluates resistance to model extraction attacks.",
        "type": "fill_blank",
        "answers": ["model stealing"],
        "other_options": ["penetration", "vulnerability", "security"]
    },
    {
        "q": "Match the AI system architectural decision with its trade-off consideration:",
        "type": "match",
        "left": ["Monolithic vs Microservices", "Synchronous vs Asynchronous", "Centralized vs Federated Learning", "Batch vs Real-time Processing"],
        "right": ["Development simplicity vs deployment complexity", "Immediate consistency vs eventual scalability", "Data privacy vs model performance", "Processing efficiency vs decision latency"]
    },
    {
        "q": "Rearrange the steps for implementing AI system technical due diligence:",
        "type": "rearrange",
        "words": ["Review Architecture", "Assess Data Quality", "Evaluate Model Performance", "Check Compliance", "Document Risks"]
    },
    {
        "q": "AI systems using transfer learning always inherit the bias characteristics of their pre-trained models.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "In AI system design, ______ testing validates that individual components function correctly in isolation.",
        "type": "fill_blank",
        "answers": ["unit"],
        "other_options": ["integration", "system", "acceptance"]
    },
    {
        "q": "What is the primary purpose of implementing the Ambassador pattern in AI microservices?",
        "type": "mcq",
        "o": [
            "Offload common client connectivity tasks from services",
            "Increase model training accuracy",
            "Reduce data storage requirements",
            "Eliminate all network communication"
        ]
    },
    {
        "q": "The ______ principle suggests AI system components should depend on abstractions rather than concrete implementations.",
        "type": "fill_blank",
        "answers": ["dependency inversion"],
        "other_options": ["single responsibility", "open-closed", "interface segregation"]
    },
    {
        "q": "What is the output of the following service discovery health check simulation?",
        "type": "mcq",
        "c": "services = {'ml-service': True, 'data-service': False, 'auth-service': True}\nhealthy_count = sum(1 for status in services.values() if status)\nprint(f'Healthy services: {healthy_count}')",
        "o": [
            "Healthy services: 2",
            "Healthy services: 3",
            "Healthy services: 1",
            "Healthy services: 0"
        ]
    },
    {
        "q": "Match the AI system scalability pattern with its implementation complexity:",
        "type": "match",
        "left": ["Horizontal Scaling", "Vertical Scaling", "Database Sharding", "Caching Strategy"],
        "right": ["Medium complexity, distributed systems knowledge", "Low complexity, hardware upgrade", "High complexity, data migration required", "Low-medium complexity, cache invalidation challenges"]
    },
    {
        "q": "In AI system reliability, ______ testing validates that new changes don't break existing functionality.",
        "type": "fill_blank",
        "answers": ["regression"],
        "other_options": ["unit", "integration", "performance"]
    },
    {
        "q": "Rearrange the steps for implementing AI system continuous integration:",
        "type": "rearrange",
        "words": ["Code Commit", "Automated Build", "Test Execution", "Quality Gates", "Artifact Creation"]
    },
    {
        "q": "All AI system performance issues can be identified through CPU monitoring alone.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which pattern helps manage AI service configuration across multiple environments?",
        "type": "mcq",
        "o": [
            "Externalized configuration with environment-specific overrides",
            "Hardcoded configuration in source code",
            "Manual configuration per deployment",
            "No configuration management"
        ]
    },
    {
        "q": "The ______ metric measures the percentage of time AI systems are operational and accessible.",
        "type": "fill_blank",
        "answers": ["availability"],
        "other_options": ["latency", "throughput", "error rate"]
    },
    {
        "q": "What is the primary purpose of implementing health checks in AI microservices?",
        "type": "mcq",
        "o": [
            "Enable automated failure detection and recovery",
            "Increase model prediction accuracy",
            "Reduce training data requirements",
            "Eliminate all manual monitoring"
        ]
    },
    {
        "q": "Match the AI system deployment strategy with its risk profile:",
        "type": "match",
        "left": ["Blue-Green", "Canary", "Shadow", "A/B Testing"],
        "right": ["Low risk with instant rollback capability", "Medium risk with gradual exposure", "Zero risk to users during testing", "Business risk from user experience variations"]
    },
    {
        "q": "In AI system design, ______ consistency allows temporary inconsistencies for better performance.",
        "type": "fill_blank",
        "answers": ["weak"],
        "other_options": ["strong", "eventual", "causal"]
    },
    {
        "q": "Rearrange the steps for conducting AI system capacity planning:",
        "type": "rearrange",
        "words": ["Analyze Current Usage", "Forecast Growth", "Identify Bottlenecks", "Plan Scaling Strategy", "Implement Monitoring"]
    },
    {
        "q": "Microservices architecture eliminates all dependencies between AI system components.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which approach helps prevent AI service registry bottlenecks in large deployments?",
        "type": "mcq",
        "o": [
            "Client-side caching with periodic refresh",
            "Single centralized registry instance",
            "Hardcoded service locations",
            "No service discovery mechanism"
        ]
    },
    {
        "q": "In cloud-native AI systems, ______ patterns enable automatic recovery from instance failures.",
        "type": "fill_blank",
        "answers": ["self-healing"],
        "other_options": ["load balancing", "auto-scaling", "circuit breaker"]
    },
    {
        "q": "Match the AI system data lineage challenge with its technical solution:",
        "type": "match",
        "left": ["Cross-System Provenance", "Version Drift Detection", "Pipeline Dependency Mapping", "Regulatory Compliance Tracking"],
        "right": ["Distributed tracing with correlation IDs", "Data checksums and hash verification", "Directed acyclic graph visualization", "Immutable audit logs with timestamps"]
    },
    {
        "q": "Rearrange the steps for implementing AI system data sovereignty compliance:",
        "type": "rearrange",
        "words": ["Identify Data Jurisdictions", "Implement Geographic Routing", "Encrypt Cross-Border Transfers", "Validate Data Residency", "Document Legal Frameworks"]
    },
    {
        "q": "AI systems processing genetic data for research purposes are exempt from GDPR requirements.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "In federated learning, ______ aggregation combines model updates without exposing raw training data.",
        "type": "fill_blank",
        "answers": ["secure"],
        "other_options": ["weighted", "distributed", "hierarchical"]
    },
    {
        "q": "What is the primary challenge when implementing differential privacy in production AI systems?",
        "type": "mcq",
        "o": [
            "Balancing privacy guarantees with model utility",
            "Increasing computational requirements exponentially",
            "Eliminating all data preprocessing steps",
            "Requiring complete data centralization"
        ]
    },
    {
        "q": "The ______ learning approach enables AI systems to improve through human feedback on model behavior.",
        "type": "fill_blank",
        "answers": ["reinforcement learning from human feedback"],
        "other_options": ["supervised", "unsupervised", "self-supervised"]
    },
    {
        "q": "What is the output of the following fairness through unawareness check?",
        "type": "mcq",
        "c": "features = ['age', 'income', 'education', 'gender', 'credit_score']\nprotected_attributes = ['gender', 'age']\nremaining_features = [f for f in features if f not in protected_attributes]\nprint(f'Features after exclusion: {len(remaining_features)}')",
        "o": [
            "Features after exclusion: 3",
            "Features after exclusion: 5",
            "Features after exclusion: 2",
            "Features after exclusion: 0"
        ]
    },
    {
        "q": "Match the AI ethics compliance framework with its jurisdiction:",
        "type": "match",
        "left": ["AI Act", "Algorithmic Accountability Act", "Personal Information Protection Law", "General Data Protection Regulation"],
        "right": ["European Union", "United States", "China", "European Union"]
    },
    {
        "q": "In AI transparency, ______ explanations provide counterfactual scenarios showing how inputs affect outputs.",
        "type": "fill_blank",
        "answers": ["contrastive"],
        "other_options": ["causal", "teleological", "mechanistic"]
    },
    {
        "q": "Rearrange the steps for conducting an AI system bias audit:",
        "type": "rearrange",
        "words": ["Define Protected Groups", "Select Fairness Metrics", "Measure Model Outcomes", "Identify Disparities", "Implement Mitigations"]
    },
    {
        "q": "All AI systems used in credit decisions must provide specific reasons for denial under ECOA regulations.",
        "type": "true_false",
        "correct": "True"
    },
    {
        "q": "Which technique helps prevent membership inference attacks on AI models?",
        "type": "mcq",
        "o": [
            "Differential privacy and model regularization",
            "Increasing model complexity indefinitely",
            "Using smaller training datasets",
            "Eliminating model confidence scores"
        ]
    },
    {
        "q": "The ______ principle requires AI systems to automatically delete personal data when no longer needed.",
        "type": "fill_blank",
        "answers": ["storage limitation"],
        "other_options": ["purpose limitation", "data minimization", "accuracy"]
    },
    {
        "q": "What is the primary benefit of implementing privacy by design in AI system development?",
        "type": "mcq",
        "o": [
            "Proactive privacy protection integrated into system architecture",
            "Reduced computational requirements for model training",
            "Elimination of all data governance documentation",
            "Automatic improvement of model accuracy metrics"
        ]
    },
    {
        "q": "Match the AI governance body with its primary focus:",
        "type": "match",
        "left": ["NIST AI Risk Management Framework", "EU High-Level Expert Group", "Partnership on AI", "IEEE Global Initiative"],
        "right": ["Technical standards and risk assessment", "Policy recommendations and ethics guidelines", "Multi-stakeholder collaboration on AI benefits", "Ethical design standards and certification"]
    },
    {
        "q": "In AI accountability, ______ frameworks assign responsibility for system outcomes across the organization.",
        "type": "fill_blank",
        "answers": ["governance"],
        "other_options": ["technical", "legal", "operational"]
    },
    {
        "q": "Rearrange the steps for implementing AI system right to rectification:",
        "type": "rearrange",
        "words": ["Receive Correction Request", "Verify Data Accuracy", "Update Data Sources", "Notify Downstream Systems", "Confirm Correction Completion"]
    },
    {
        "q": "Homomorphic encryption allows unlimited computations on encrypted data without any performance overhead.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which approach helps ensure AI systems respect indigenous data sovereignty principles?",
        "type": "mcq",
        "o": [
            "Community consent and benefit-sharing agreements",
            "Using identical models across all populations",
            "Ignoring cultural context in model development",
            "Eliminating all local data collection"
        ]
    },
    {
        "q": "In responsible AI development, ______ assessments evaluate environmental impact of model training and inference.",
        "type": "fill_blank",
        "answers": ["sustainability"],
        "other_options": ["fairness", "performance", "security"]
    },
    {
        "q": "Match the AI system energy efficiency metric with its calculation method:",
        "type": "match",
        "left": ["FLOPS per Watt", "Carbon Intensity", "Energy Proportionality", "Inference Efficiency"],
        "right": ["Compute operations per energy unit", "CO2 emissions per computation hour", "Power usage relative to system utilization", "Predictions per kilowatt-hour"]
    },
    {
        "q": "Rearrange the steps for implementing AI system carbon footprint tracking:",
        "type": "rearrange",
        "words": ["Measure Energy Consumption", "Calculate Carbon Equivalent", "Track Across Lifecycle", "Set Reduction Targets", "Implement Efficiency Measures"]
    },
    {
        "q": "AI model quantization always reduces accuracy proportionally to the reduction in model size.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "In sustainable AI, ______ computing optimizes model architecture and training to minimize environmental impact.",
        "type": "fill_blank",
        "answers": ["green"],
        "other_options": ["edge", "quantum", "neuromorphic"]
    },
    {
        "q": "What is the primary environmental benefit of using sparse neural networks?",
        "type": "mcq",
        "o": [
            "Reduced computational requirements through activation sparsity",
            "Elimination of all training data needs",
            "Automatic model compression without quality loss",
            "Zero energy consumption during inference"
        ]
    },
    {
        "q": "The ______ learning paradigm trains smaller student models to mimic larger teacher models while reducing computational demands.",
        "type": "fill_blank",
        "answers": ["knowledge distillation"],
        "other_options": ["transfer", "meta", "self-supervised"]
    },
    {
        "q": "What is the output of the following model sparsity calculation?",
        "type": "mcq",
        "c": "import numpy as np\nweights = np.array([0, 0.5, 0, 0, 0.3, 0, 0, 0.2])\nsparsity = np.mean(weights == 0)\nprint(f'Sparsity: {sparsity:.1%}')",
        "o": [
            "Sparsity: 62.5%",
            "Sparsity: 37.5%",
            "Sparsity: 100.0%",
            "Sparsity: 0.0%"
        ]
    },
    {
        "q": "Match the AI sustainability technique with its primary mechanism:",
        "type": "match",
        "left": ["Pruning", "Quantization", "Neural Architecture Search", "Early Exit Networks"],
        "right": ["Removing unimportant weights and connections", "Reducing numerical precision of parameters", "Automating efficient architecture discovery", "Stopping inference when confidence threshold met"]
    },
    {
        "q": "In AI lifecycle assessment, ______ accounting tracks environmental impact from data collection to model retirement.",
        "type": "fill_blank",
        "answers": ["carbon"],
        "other_options": ["energy", "water", "resource"]
    },
    {
        "q": "Rearrange the steps for implementing energy-aware AI model training:",
        "type": "rearrange",
        "words": ["Profile Energy Usage", "Optimize Hyperparameters", "Use Efficient Architectures", "Schedule Low-Carbon Training", "Monitor Efficiency Metrics"]
    },
    {
        "q": "All AI model compression techniques maintain exactly the same accuracy as the original model.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which approach most effectively reduces AI system carbon footprint during inference?",
        "type": "mcq",
        "o": [
            "Model quantization and efficient serving infrastructure",
            "Increasing model complexity indefinitely",
            "Using only CPU-based inference",
            "Eliminating all model validation"
        ]
    },
    {
        "q": "The ______ metric measures AI system efficiency as computations per energy unit.",
        "type": "fill_blank",
        "answers": ["computational efficiency"],
        "other_options": ["accuracy", "throughput", "latency"]
    },
    {
        "q": "What is the primary advantage of using mixture-of-experts architectures for large models?",
        "type": "mcq",
        "o": [
            "Activating only relevant model parts for each input",
            "Eliminating all training data requirements",
            "Guaranteeing perfect model interpretability",
            "Automatically generating training data"
        ]
    },
    {
        "q": "Match the AI hardware accelerator with its energy efficiency characteristic:",
        "type": "match",
        "left": ["TPUs", "FPGAs", "Neuromorphic Chips", "Graphcore IPUs"],
        "right": ["Optimized for matrix operations with high throughput", "Reconfigurable for specific model architectures", "Event-driven processing with minimal static power", "Massive parallel processing for graph computations"]
    },
    {
        "q": "In sustainable AI development, ______ scheduling runs training jobs during renewable energy availability.",
        "type": "fill_blank",
        "answers": ["carbon-aware"],
        "other_options": ["time-based", "priority", "round-robin"]
    },
    {
        "q": "Rearrange the AI model efficiency optimization hierarchy:",
        "type": "rearrange",
        "words": ["Algorithm Selection", "Architecture Design", "Model Compression", "Hardware Optimization", "System Tuning"]
    },
    {
        "q": "Federated learning always reduces overall energy consumption compared to centralized training.",
        "type": "true_false",
        "correct": "False"
    },
    {
        "q": "Which technique helps reduce AI system water consumption in data centers?",
        "type": "mcq",
        "o": [
            "Liquid cooling optimization and free air cooling",
            "Increasing server density without cooling upgrades",
            "Running models at higher temperatures",
            "Eliminating all cooling systems"
        ]
    },
    {
        "q": "In green AI, ______ benchmarks compare model performance against computational and environmental costs.",
        "type": "fill_blank",
        "answers": ["efficiency"],
        "other_options": ["accuracy", "speed", "reliability"]
    }
]